{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules and activate auto-reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "import datetime\n",
    "from helper import *\n",
    "\n",
    "from model import *\n",
    "from data_loader import *\n",
    "from preprocessing import *\n",
    "from train import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(gpu_use=1, which_gpu=0)\n"
     ]
    }
   ],
   "source": [
    "# gpu_option\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu_use', type=int, action=\"store\", help='GPU enable')\n",
    "parser.add_argument('--which_gpu', type=int, action=\"store\", help='GPU enable')\n",
    "args = parser.parse_args(['--gpu_use=1', '--which_gpu=0'])\n",
    "print(args)\n",
    "\n",
    "# options\n",
    "melBins = 128\n",
    "hop = 512\n",
    "frames = int(2.99*22050.0/hop)#int(29.9*22050.0/hop)\n",
    "batch_size = 5\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "\n",
    "# A location where labels and features are located\n",
    "label_path = os.path.abspath('../..') + '/gtzan/'\n",
    "mel_path = os.path.abspath('../..') + '/gtzan_mel/'\n",
    "save_path = os.path.abspath('../..') + '/models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reggae', 'classical', 'country', 'jazz', 'metal', 'pop', 'disco', 'hiphop', 'rock', 'blues']\n",
      "(4430, 128, 128)\n",
      "(4430, 128, 128) (4430,) (1970, 128, 128) (1970,) (2900, 128, 128) (2900,)\n"
     ]
    }
   ],
   "source": [
    "# load normalized mel-spectrograms and labels\n",
    "x_train,y_train,x_valid,y_valid,x_test,y_test = load_data(label_path, mel_path, melBins, frames)\n",
    "print(x_train.shape,y_train.shape,x_valid.shape,y_valid.shape,x_test.shape,y_test.shape)\n",
    "\n",
    "# data loader    \n",
    "train_data = gtzandata(x_train,y_train)\n",
    "valid_data = gtzandata(x_valid,y_valid)\n",
    "test_data = gtzandata(x_test,y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_1DCNN_short(\n",
      "  (conv0): Sequential(\n",
      "    (0): Conv1d(128, 32, kernel_size=(9,), stride=(1,))\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(32, 32, kernel_size=(8,), stride=(1,))\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv1d(32, 64, kernel_size=(4,), stride=(3,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc0): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (drop0): Dropout(p=0.6)\n",
      "  (fc1): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train.py:47: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  print (\"Epoch [%d/%d], Iter [%d/%d] loss : %.4f\" % (epoch+1, num_epochs, i+1, len(train_loader), loss.data[0]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iter [100/886] loss : 2.7734\n",
      "Epoch [1/50], Iter [200/886] loss : 1.7016\n",
      "Epoch [1/50], Iter [300/886] loss : 1.7993\n",
      "Epoch [1/50], Iter [400/886] loss : 2.1595\n",
      "Epoch [1/50], Iter [500/886] loss : 1.1408\n",
      "Epoch [1/50], Iter [600/886] loss : 1.0961\n",
      "Epoch [1/50], Iter [700/886] loss : 1.9066\n",
      "Epoch [1/50], Iter [800/886] loss : 2.8723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train.py:80: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  eval_loss += loss.data[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 1.9127 \n",
      "\n",
      "Learning rate : 0.01\n",
      "Epoch [2/50], Iter [100/886] loss : 1.5213\n",
      "Epoch [2/50], Iter [200/886] loss : 1.5025\n",
      "Epoch [2/50], Iter [300/886] loss : 1.9639\n",
      "Epoch [2/50], Iter [400/886] loss : 1.5316\n",
      "Epoch [2/50], Iter [500/886] loss : 1.3749\n",
      "Epoch [2/50], Iter [600/886] loss : 1.1512\n",
      "Epoch [2/50], Iter [700/886] loss : 1.1449\n",
      "Epoch [2/50], Iter [800/886] loss : 1.3693\n",
      "Average loss: 1.6920 \n",
      "\n",
      "Learning rate : 0.01\n",
      "Epoch [3/50], Iter [100/886] loss : 0.9156\n",
      "Epoch [3/50], Iter [200/886] loss : 1.1084\n",
      "Epoch [3/50], Iter [300/886] loss : 1.9823\n",
      "Epoch [3/50], Iter [400/886] loss : 2.0029\n",
      "Epoch [3/50], Iter [500/886] loss : 2.4443\n",
      "Epoch [3/50], Iter [600/886] loss : 1.5652\n",
      "Epoch [3/50], Iter [700/886] loss : 1.6236\n",
      "Epoch [3/50], Iter [800/886] loss : 1.4426\n",
      "Average loss: 1.7030 \n",
      "\n",
      "Learning rate : 0.01\n",
      "Epoch [4/50], Iter [100/886] loss : 1.3995\n",
      "Epoch [4/50], Iter [200/886] loss : 3.2457\n",
      "Epoch [4/50], Iter [300/886] loss : 1.2988\n",
      "Epoch [4/50], Iter [400/886] loss : 1.7857\n",
      "Epoch [4/50], Iter [500/886] loss : 2.2195\n",
      "Epoch [4/50], Iter [600/886] loss : 1.3312\n",
      "Epoch [4/50], Iter [700/886] loss : 1.1188\n",
      "Epoch [4/50], Iter [800/886] loss : 1.3050\n",
      "Average loss: 1.6550 \n",
      "\n",
      "Learning rate : 0.01\n",
      "Epoch [5/50], Iter [100/886] loss : 0.9725\n",
      "Epoch [5/50], Iter [200/886] loss : 1.2691\n",
      "Epoch [5/50], Iter [300/886] loss : 0.8112\n",
      "Epoch [5/50], Iter [400/886] loss : 0.6844\n",
      "Epoch [5/50], Iter [500/886] loss : 1.0686\n",
      "Epoch [5/50], Iter [600/886] loss : 1.8797\n",
      "Epoch [5/50], Iter [700/886] loss : 1.7120\n",
      "Epoch [5/50], Iter [800/886] loss : 2.7331\n",
      "Average loss: 1.7330 \n",
      "\n",
      "Learning rate : 0.01\n",
      "Epoch [6/50], Iter [100/886] loss : 1.3264\n",
      "Epoch [6/50], Iter [200/886] loss : 2.3571\n",
      "Epoch [6/50], Iter [300/886] loss : 2.4144\n",
      "Epoch [6/50], Iter [400/886] loss : 1.9008\n",
      "Epoch [6/50], Iter [500/886] loss : 1.7324\n",
      "Epoch [6/50], Iter [600/886] loss : 1.6655\n",
      "Epoch [6/50], Iter [700/886] loss : 1.7086\n",
      "Epoch [6/50], Iter [800/886] loss : 1.9009\n",
      "Average loss: 1.8718 \n",
      "\n",
      "Learning rate : 0.01\n",
      "Epoch [7/50], Iter [100/886] loss : 1.6848\n",
      "Epoch [7/50], Iter [200/886] loss : 1.0836\n",
      "Epoch [7/50], Iter [300/886] loss : 2.0601\n",
      "Epoch [7/50], Iter [400/886] loss : 1.3302\n",
      "Epoch [7/50], Iter [500/886] loss : 2.2098\n",
      "Epoch [7/50], Iter [600/886] loss : 0.9144\n",
      "Epoch [7/50], Iter [700/886] loss : 1.6995\n",
      "Epoch [7/50], Iter [800/886] loss : 1.9394\n",
      "Average loss: 1.6804 \n",
      "\n",
      "Learning rate : 0.01\n",
      "Epoch [8/50], Iter [100/886] loss : 0.7122\n",
      "Epoch [8/50], Iter [200/886] loss : 2.1509\n",
      "Epoch [8/50], Iter [300/886] loss : 0.5974\n",
      "Epoch [8/50], Iter [400/886] loss : 1.7401\n",
      "Epoch [8/50], Iter [500/886] loss : 1.8844\n",
      "Epoch [8/50], Iter [600/886] loss : 1.6527\n",
      "Epoch [8/50], Iter [700/886] loss : 1.8937\n",
      "Epoch [8/50], Iter [800/886] loss : 3.3455\n",
      "Average loss: 1.7165 \n",
      "\n",
      "Epoch     7: reducing learning rate of group 0 to 2.0000e-03.\n",
      "Learning rate : 0.002\n",
      "Epoch [9/50], Iter [100/886] loss : 1.3819\n",
      "Epoch [9/50], Iter [200/886] loss : 1.8660\n",
      "Epoch [9/50], Iter [300/886] loss : 0.5344\n",
      "Epoch [9/50], Iter [400/886] loss : 1.1451\n",
      "Epoch [9/50], Iter [500/886] loss : 1.2654\n",
      "Epoch [9/50], Iter [600/886] loss : 0.7462\n",
      "Epoch [9/50], Iter [700/886] loss : 0.8302\n",
      "Epoch [9/50], Iter [800/886] loss : 1.4438\n",
      "Average loss: 1.6920 \n",
      "\n",
      "Learning rate : 0.002\n",
      "Epoch [10/50], Iter [100/886] loss : 1.6430\n",
      "Epoch [10/50], Iter [200/886] loss : 0.6362\n",
      "Epoch [10/50], Iter [300/886] loss : 0.3672\n",
      "Epoch [10/50], Iter [400/886] loss : 0.4866\n",
      "Epoch [10/50], Iter [500/886] loss : 0.5299\n",
      "Epoch [10/50], Iter [600/886] loss : 0.5602\n",
      "Epoch [10/50], Iter [700/886] loss : 1.1424\n",
      "Epoch [10/50], Iter [800/886] loss : 0.6437\n",
      "Average loss: 1.7181 \n",
      "\n",
      "Learning rate : 0.002\n",
      "Epoch [11/50], Iter [100/886] loss : 1.4355\n",
      "Epoch [11/50], Iter [200/886] loss : 1.9177\n",
      "Epoch [11/50], Iter [300/886] loss : 0.9747\n",
      "Epoch [11/50], Iter [400/886] loss : 1.0164\n",
      "Epoch [11/50], Iter [500/886] loss : 1.1655\n",
      "Epoch [11/50], Iter [600/886] loss : 0.2429\n",
      "Epoch [11/50], Iter [700/886] loss : 0.2510\n",
      "Epoch [11/50], Iter [800/886] loss : 1.6700\n",
      "Average loss: 1.7190 \n",
      "\n",
      "Learning rate : 0.002\n",
      "Epoch [12/50], Iter [100/886] loss : 0.2936\n",
      "Epoch [12/50], Iter [200/886] loss : 0.9996\n",
      "Epoch [12/50], Iter [300/886] loss : 1.2653\n",
      "Epoch [12/50], Iter [400/886] loss : 1.2913\n",
      "Epoch [12/50], Iter [500/886] loss : 1.8097\n",
      "Epoch [12/50], Iter [600/886] loss : 0.2442\n",
      "Epoch [12/50], Iter [700/886] loss : 1.1877\n",
      "Epoch [12/50], Iter [800/886] loss : 0.6883\n",
      "Average loss: 1.8382 \n",
      "\n",
      "Epoch    11: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Learning rate : 0.0004\n",
      "Epoch [13/50], Iter [100/886] loss : 1.0554\n",
      "Epoch [13/50], Iter [200/886] loss : 0.9054\n",
      "Epoch [13/50], Iter [300/886] loss : 0.7163\n",
      "Epoch [13/50], Iter [400/886] loss : 0.4384\n",
      "Epoch [13/50], Iter [500/886] loss : 1.6161\n",
      "Epoch [13/50], Iter [600/886] loss : 0.4177\n",
      "Epoch [13/50], Iter [700/886] loss : 0.5602\n",
      "Epoch [13/50], Iter [800/886] loss : 0.2543\n",
      "Average loss: 1.7861 \n",
      "\n",
      "Learning rate : 0.0004\n",
      "Epoch [14/50], Iter [100/886] loss : 0.8305\n",
      "Epoch [14/50], Iter [200/886] loss : 0.6848\n",
      "Epoch [14/50], Iter [300/886] loss : 1.0240\n",
      "Epoch [14/50], Iter [400/886] loss : 0.8213\n",
      "Epoch [14/50], Iter [500/886] loss : 1.9591\n",
      "Epoch [14/50], Iter [600/886] loss : 0.9840\n",
      "Epoch [14/50], Iter [700/886] loss : 1.2773\n",
      "Epoch [14/50], Iter [800/886] loss : 0.4298\n",
      "Average loss: 1.6965 \n",
      "\n",
      "Learning rate : 0.0004\n",
      "Epoch [15/50], Iter [100/886] loss : 1.2452\n",
      "Epoch [15/50], Iter [200/886] loss : 0.5206\n",
      "Epoch [15/50], Iter [300/886] loss : 0.7852\n",
      "Epoch [15/50], Iter [400/886] loss : 1.2779\n",
      "Epoch [15/50], Iter [500/886] loss : 1.0789\n",
      "Epoch [15/50], Iter [600/886] loss : 0.9392\n",
      "Epoch [15/50], Iter [700/886] loss : 0.3755\n",
      "Epoch [15/50], Iter [800/886] loss : 0.3974\n",
      "Average loss: 1.8127 \n",
      "\n",
      "Learning rate : 0.0004\n",
      "Epoch [16/50], Iter [100/886] loss : 1.2006\n",
      "Epoch [16/50], Iter [200/886] loss : 1.4010\n",
      "Epoch [16/50], Iter [300/886] loss : 0.8220\n",
      "Epoch [16/50], Iter [400/886] loss : 1.0695\n",
      "Epoch [16/50], Iter [500/886] loss : 0.9891\n",
      "Epoch [16/50], Iter [600/886] loss : 0.6780\n",
      "Epoch [16/50], Iter [700/886] loss : 1.1963\n",
      "Epoch [16/50], Iter [800/886] loss : 0.4039\n",
      "Average loss: 1.8636 \n",
      "\n",
      "Epoch    15: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Learning rate : 8e-05\n",
      "Epoch [17/50], Iter [100/886] loss : 0.2900\n",
      "Epoch [17/50], Iter [200/886] loss : 0.6465\n",
      "Epoch [17/50], Iter [300/886] loss : 0.1538\n",
      "Epoch [17/50], Iter [400/886] loss : 0.5109\n",
      "Epoch [17/50], Iter [500/886] loss : 0.1645\n",
      "Epoch [17/50], Iter [600/886] loss : 0.4825\n",
      "Epoch [17/50], Iter [700/886] loss : 0.7887\n",
      "Epoch [17/50], Iter [800/886] loss : 0.2851\n",
      "Average loss: 1.7272 \n",
      "\n",
      "Learning rate : 8e-05\n",
      "Epoch [18/50], Iter [100/886] loss : 0.1588\n",
      "Epoch [18/50], Iter [200/886] loss : 0.3877\n",
      "Epoch [18/50], Iter [300/886] loss : 0.2604\n",
      "Epoch [18/50], Iter [400/886] loss : 0.6255\n",
      "Epoch [18/50], Iter [500/886] loss : 1.1547\n",
      "Epoch [18/50], Iter [600/886] loss : 1.2148\n",
      "Epoch [18/50], Iter [700/886] loss : 0.5899\n",
      "Epoch [18/50], Iter [800/886] loss : 1.6793\n",
      "Average loss: 1.7780 \n",
      "\n",
      "Learning rate : 8e-05\n",
      "Epoch [19/50], Iter [100/886] loss : 0.2389\n",
      "Epoch [19/50], Iter [200/886] loss : 0.3486\n",
      "Epoch [19/50], Iter [300/886] loss : 0.3148\n",
      "Epoch [19/50], Iter [400/886] loss : 1.0175\n",
      "Epoch [19/50], Iter [500/886] loss : 2.1218\n",
      "Epoch [19/50], Iter [600/886] loss : 0.9713\n",
      "Epoch [19/50], Iter [700/886] loss : 1.0262\n",
      "Epoch [19/50], Iter [800/886] loss : 0.9499\n",
      "Average loss: 1.6982 \n",
      "\n",
      "Learning rate : 8e-05\n",
      "Epoch [20/50], Iter [100/886] loss : 1.2100\n",
      "Epoch [20/50], Iter [200/886] loss : 0.6726\n",
      "Epoch [20/50], Iter [300/886] loss : 0.5523\n",
      "Epoch [20/50], Iter [400/886] loss : 0.6193\n",
      "Epoch [20/50], Iter [500/886] loss : 0.9900\n",
      "Epoch [20/50], Iter [600/886] loss : 0.6781\n",
      "Epoch [20/50], Iter [700/886] loss : 1.0626\n",
      "Epoch [20/50], Iter [800/886] loss : 0.5917\n",
      "Average loss: 1.6835 \n",
      "\n",
      "Epoch    19: reducing learning rate of group 0 to 1.6000e-05.\n",
      "Learning rate : 1.6e-05\n",
      "Epoch [21/50], Iter [100/886] loss : 0.6781\n",
      "Epoch [21/50], Iter [200/886] loss : 1.6616\n",
      "Epoch [21/50], Iter [300/886] loss : 2.7825\n",
      "Epoch [21/50], Iter [400/886] loss : 0.8690\n",
      "Epoch [21/50], Iter [500/886] loss : 0.4062\n",
      "Epoch [21/50], Iter [600/886] loss : 0.8547\n",
      "Epoch [21/50], Iter [700/886] loss : 0.7061\n",
      "Epoch [21/50], Iter [800/886] loss : 0.2330\n",
      "Average loss: 1.7809 \n",
      "\n",
      "Learning rate : 1.6e-05\n",
      "Epoch [22/50], Iter [100/886] loss : 0.8410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/50], Iter [200/886] loss : 0.9230\n",
      "Epoch [22/50], Iter [300/886] loss : 1.0638\n",
      "Epoch [22/50], Iter [400/886] loss : 1.6484\n",
      "Epoch [22/50], Iter [500/886] loss : 0.2865\n",
      "Epoch [22/50], Iter [600/886] loss : 1.3280\n",
      "Epoch [22/50], Iter [700/886] loss : 0.2631\n",
      "Epoch [22/50], Iter [800/886] loss : 1.0385\n",
      "Average loss: 1.6951 \n",
      "\n",
      "Learning rate : 1.6e-05\n",
      "Epoch [23/50], Iter [100/886] loss : 1.9720\n",
      "Epoch [23/50], Iter [200/886] loss : 1.6100\n",
      "Epoch [23/50], Iter [300/886] loss : 0.4658\n",
      "Epoch [23/50], Iter [400/886] loss : 1.4318\n",
      "Epoch [23/50], Iter [500/886] loss : 0.9625\n",
      "Epoch [23/50], Iter [600/886] loss : 1.5017\n",
      "Epoch [23/50], Iter [700/886] loss : 0.7668\n",
      "Epoch [23/50], Iter [800/886] loss : 0.1566\n",
      "Average loss: 1.7122 \n",
      "\n",
      "Learning rate : 1.6e-05\n",
      "Epoch [24/50], Iter [100/886] loss : 0.4552\n",
      "Epoch [24/50], Iter [200/886] loss : 0.6646\n",
      "Epoch [24/50], Iter [300/886] loss : 0.2107\n",
      "Epoch [24/50], Iter [400/886] loss : 0.5224\n",
      "Epoch [24/50], Iter [500/886] loss : 1.0465\n",
      "Epoch [24/50], Iter [600/886] loss : 0.3506\n",
      "Epoch [24/50], Iter [700/886] loss : 1.7912\n",
      "Epoch [24/50], Iter [800/886] loss : 1.0869\n",
      "Average loss: 1.8894 \n",
      "\n",
      "Epoch    23: reducing learning rate of group 0 to 3.2000e-06.\n",
      "Learning rate : 3.2e-06\n",
      "Early stopping\n",
      "\n",
      "\n",
      "--- 93.3212788105 seconds spent ---\n",
      "Average loss: 1.5621 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "# use GPU\n",
    "if args.gpu_use == 1:\n",
    "    model = model_1DCNN_short(dropout=0.6).cuda(args.which_gpu)\n",
    "elif args.gpu_use == 0:\n",
    "    model = model_1DCNN_short()\n",
    "print(model)\n",
    "\n",
    "# loss function \n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# run\n",
    "start_time = time.time()\n",
    "fit(model,train_loader,valid_loader,criterion,num_epochs,args,learning_rate)\n",
    "print(\"--- %s seconds spent ---\" % (time.time() - start_time))\n",
    "\n",
    "# evaluation\n",
    "avg_loss, output_all, label_all = eval(model,test_loader,criterion,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.py:100: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = nn.functional.log_softmax(out)\n",
      "train.py:47: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  print (\"Epoch [%d/%d], Iter [%d/%d] loss : %.4f\" % (epoch+1, num_epochs, i+1, len(train_loader), loss.data[0]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iter [100/886] loss : 2.7624\n",
      "Epoch [1/50], Iter [200/886] loss : 2.9409\n",
      "Epoch [1/50], Iter [300/886] loss : 2.1809\n",
      "Epoch [1/50], Iter [400/886] loss : 2.3254\n",
      "Epoch [1/50], Iter [500/886] loss : 2.7448\n",
      "Epoch [1/50], Iter [600/886] loss : 2.4060\n",
      "Epoch [1/50], Iter [700/886] loss : 3.2147\n",
      "Epoch [1/50], Iter [800/886] loss : 2.5121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train.py:118: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  eval_loss += loss.data[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 2.5887 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [2/50], Iter [100/886] loss : 2.2785\n",
      "Epoch [2/50], Iter [200/886] loss : 3.2187\n",
      "Epoch [2/50], Iter [300/886] loss : 2.1823\n",
      "Epoch [2/50], Iter [400/886] loss : 2.5495\n",
      "Epoch [2/50], Iter [500/886] loss : 2.6785\n",
      "Epoch [2/50], Iter [600/886] loss : 2.4385\n",
      "Epoch [2/50], Iter [700/886] loss : 3.8360\n",
      "Epoch [2/50], Iter [800/886] loss : 2.7985\n",
      "Average loss: 2.5489 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [3/50], Iter [100/886] loss : 2.5433\n",
      "Epoch [3/50], Iter [200/886] loss : 3.0526\n",
      "Epoch [3/50], Iter [300/886] loss : 2.7731\n",
      "Epoch [3/50], Iter [400/886] loss : 2.1526\n",
      "Epoch [3/50], Iter [500/886] loss : 2.8224\n",
      "Epoch [3/50], Iter [600/886] loss : 3.4405\n",
      "Epoch [3/50], Iter [700/886] loss : 2.4700\n",
      "Epoch [3/50], Iter [800/886] loss : 2.6344\n",
      "Average loss: 2.7226 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [4/50], Iter [100/886] loss : 2.7412\n",
      "Epoch [4/50], Iter [200/886] loss : 2.7022\n",
      "Epoch [4/50], Iter [300/886] loss : 2.4969\n",
      "Epoch [4/50], Iter [400/886] loss : 2.5406\n",
      "Epoch [4/50], Iter [500/886] loss : 3.1974\n",
      "Epoch [4/50], Iter [600/886] loss : 3.2781\n",
      "Epoch [4/50], Iter [700/886] loss : 3.7834\n",
      "Epoch [4/50], Iter [800/886] loss : 2.6587\n",
      "Average loss: 2.5023 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [5/50], Iter [100/886] loss : 2.9339\n",
      "Epoch [5/50], Iter [200/886] loss : 3.1456\n",
      "Epoch [5/50], Iter [300/886] loss : 3.1155\n",
      "Epoch [5/50], Iter [400/886] loss : 2.8191\n",
      "Epoch [5/50], Iter [500/886] loss : 3.2788\n",
      "Epoch [5/50], Iter [600/886] loss : 2.2716\n",
      "Epoch [5/50], Iter [700/886] loss : 2.2493\n",
      "Epoch [5/50], Iter [800/886] loss : 3.2079\n",
      "Average loss: 2.5188 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [6/50], Iter [100/886] loss : 2.4932\n",
      "Epoch [6/50], Iter [200/886] loss : 2.5435\n",
      "Epoch [6/50], Iter [300/886] loss : 2.6033\n",
      "Epoch [6/50], Iter [400/886] loss : 3.1551\n",
      "Epoch [6/50], Iter [500/886] loss : 2.4090\n",
      "Epoch [6/50], Iter [600/886] loss : 2.5214\n",
      "Epoch [6/50], Iter [700/886] loss : 2.2676\n",
      "Epoch [6/50], Iter [800/886] loss : 4.1025\n",
      "Average loss: 2.8016 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [7/50], Iter [100/886] loss : 3.1924\n",
      "Epoch [7/50], Iter [200/886] loss : 2.6849\n",
      "Epoch [7/50], Iter [300/886] loss : 3.2582\n",
      "Epoch [7/50], Iter [400/886] loss : 2.6883\n",
      "Epoch [7/50], Iter [500/886] loss : 3.0917\n",
      "Epoch [7/50], Iter [600/886] loss : 2.8624\n",
      "Epoch [7/50], Iter [700/886] loss : 3.5814\n",
      "Epoch [7/50], Iter [800/886] loss : 2.5441\n",
      "Average loss: 2.4930 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [8/50], Iter [100/886] loss : 3.0865\n",
      "Epoch [8/50], Iter [200/886] loss : 2.5654\n",
      "Epoch [8/50], Iter [300/886] loss : 2.7144\n",
      "Epoch [8/50], Iter [400/886] loss : 3.1153\n",
      "Epoch [8/50], Iter [500/886] loss : 3.1932\n",
      "Epoch [8/50], Iter [600/886] loss : 2.4980\n",
      "Epoch [8/50], Iter [700/886] loss : 3.3720\n",
      "Epoch [8/50], Iter [800/886] loss : 2.6449\n",
      "Average loss: 2.6345 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [9/50], Iter [100/886] loss : 2.6584\n",
      "Epoch [9/50], Iter [200/886] loss : 2.1505\n",
      "Epoch [9/50], Iter [300/886] loss : 2.6974\n",
      "Epoch [9/50], Iter [400/886] loss : 2.5187\n",
      "Epoch [9/50], Iter [500/886] loss : 2.5465\n",
      "Epoch [9/50], Iter [600/886] loss : 3.3043\n",
      "Epoch [9/50], Iter [700/886] loss : 2.6824\n",
      "Epoch [9/50], Iter [800/886] loss : 3.1568\n",
      "Average loss: 2.7668 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [10/50], Iter [100/886] loss : 9.9317\n",
      "Epoch [10/50], Iter [200/886] loss : 6.6982\n",
      "Epoch [10/50], Iter [300/886] loss : 3.1048\n",
      "Epoch [10/50], Iter [400/886] loss : 3.0842\n",
      "Epoch [10/50], Iter [500/886] loss : 2.2679\n",
      "Epoch [10/50], Iter [600/886] loss : 2.4883\n",
      "Epoch [10/50], Iter [700/886] loss : 2.5095\n",
      "Epoch [10/50], Iter [800/886] loss : 2.3941\n",
      "Average loss: 2.5837 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [11/50], Iter [100/886] loss : 2.9335\n",
      "Epoch [11/50], Iter [200/886] loss : 2.3696\n",
      "Epoch [11/50], Iter [300/886] loss : 2.5971\n",
      "Epoch [11/50], Iter [400/886] loss : 2.2793\n",
      "Epoch [11/50], Iter [500/886] loss : 2.3105\n",
      "Epoch [11/50], Iter [600/886] loss : 3.4629\n",
      "Epoch [11/50], Iter [700/886] loss : 2.1053\n",
      "Epoch [11/50], Iter [800/886] loss : 3.5268\n",
      "Average loss: 2.6259 \n",
      "\n",
      "Epoch    10: reducing learning rate of group 0 to 2.0000e-02.\n",
      "Learning rate : 0.02\n",
      "Epoch [12/50], Iter [100/886] loss : 2.3440\n",
      "Epoch [12/50], Iter [200/886] loss : 2.5682\n",
      "Epoch [12/50], Iter [300/886] loss : 2.6153\n",
      "Epoch [12/50], Iter [400/886] loss : 2.3138\n",
      "Epoch [12/50], Iter [500/886] loss : 2.3417\n",
      "Epoch [12/50], Iter [600/886] loss : 2.3150\n",
      "Epoch [12/50], Iter [700/886] loss : 2.2634\n",
      "Epoch [12/50], Iter [800/886] loss : 2.4378\n",
      "Average loss: 2.3186 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [13/50], Iter [100/886] loss : 2.3064\n",
      "Epoch [13/50], Iter [200/886] loss : 2.3484\n",
      "Epoch [13/50], Iter [300/886] loss : 2.3188\n",
      "Epoch [13/50], Iter [400/886] loss : 2.3421\n",
      "Epoch [13/50], Iter [500/886] loss : 2.3864\n",
      "Epoch [13/50], Iter [600/886] loss : 2.3713\n",
      "Epoch [13/50], Iter [700/886] loss : 2.2492\n",
      "Epoch [13/50], Iter [800/886] loss : 2.3153\n",
      "Average loss: 2.3147 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [14/50], Iter [100/886] loss : 2.3353\n",
      "Epoch [14/50], Iter [200/886] loss : 2.3966\n",
      "Epoch [14/50], Iter [300/886] loss : 2.2516\n",
      "Epoch [14/50], Iter [400/886] loss : 2.2532\n",
      "Epoch [14/50], Iter [500/886] loss : 2.3272\n",
      "Epoch [14/50], Iter [600/886] loss : 2.4814\n",
      "Epoch [14/50], Iter [700/886] loss : 2.0395\n",
      "Epoch [14/50], Iter [800/886] loss : 2.4263\n",
      "Average loss: 2.3300 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [15/50], Iter [100/886] loss : 2.3665\n",
      "Epoch [15/50], Iter [200/886] loss : 2.2222\n",
      "Epoch [15/50], Iter [300/886] loss : 2.3218\n",
      "Epoch [15/50], Iter [400/886] loss : 2.3350\n",
      "Epoch [15/50], Iter [500/886] loss : 2.5667\n",
      "Epoch [15/50], Iter [600/886] loss : 2.4199\n",
      "Epoch [15/50], Iter [700/886] loss : 2.4274\n",
      "Epoch [15/50], Iter [800/886] loss : 2.2365\n",
      "Average loss: 2.3352 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [16/50], Iter [100/886] loss : 2.1858\n",
      "Epoch [16/50], Iter [200/886] loss : 2.5954\n",
      "Epoch [16/50], Iter [300/886] loss : 2.5747\n",
      "Epoch [16/50], Iter [400/886] loss : 2.4127\n",
      "Epoch [16/50], Iter [500/886] loss : 2.3616\n",
      "Epoch [16/50], Iter [600/886] loss : 2.1684\n",
      "Epoch [16/50], Iter [700/886] loss : 2.3618\n",
      "Epoch [16/50], Iter [800/886] loss : 2.4561\n",
      "Average loss: 2.3417 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [17/50], Iter [100/886] loss : 2.3676\n",
      "Epoch [17/50], Iter [200/886] loss : 2.3219\n",
      "Epoch [17/50], Iter [300/886] loss : 2.2132\n",
      "Epoch [17/50], Iter [400/886] loss : 2.2099\n",
      "Epoch [17/50], Iter [500/886] loss : 2.3171\n",
      "Epoch [17/50], Iter [600/886] loss : 2.5411\n",
      "Epoch [17/50], Iter [700/886] loss : 2.2852\n",
      "Epoch [17/50], Iter [800/886] loss : 2.3566\n",
      "Average loss: 2.3537 \n",
      "\n",
      "Epoch    16: reducing learning rate of group 0 to 4.0000e-03.\n",
      "Learning rate : 0.004\n",
      "Epoch [18/50], Iter [100/886] loss : 2.3085\n",
      "Epoch [18/50], Iter [200/886] loss : 2.2696\n",
      "Epoch [18/50], Iter [300/886] loss : 2.3705\n",
      "Epoch [18/50], Iter [400/886] loss : 2.3565\n",
      "Epoch [18/50], Iter [500/886] loss : 2.3836\n",
      "Epoch [18/50], Iter [600/886] loss : 2.2792\n",
      "Epoch [18/50], Iter [700/886] loss : 2.3680\n",
      "Epoch [18/50], Iter [800/886] loss : 2.3013\n",
      "Average loss: 2.3015 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [19/50], Iter [100/886] loss : 2.3841\n",
      "Epoch [19/50], Iter [200/886] loss : 2.2664\n",
      "Epoch [19/50], Iter [300/886] loss : 2.3370\n",
      "Epoch [19/50], Iter [400/886] loss : 2.3600\n",
      "Epoch [19/50], Iter [500/886] loss : 2.3472\n",
      "Epoch [19/50], Iter [600/886] loss : 2.2898\n",
      "Epoch [19/50], Iter [700/886] loss : 2.3220\n",
      "Epoch [19/50], Iter [800/886] loss : 2.3014\n",
      "Average loss: 2.2980 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [20/50], Iter [100/886] loss : 2.2735\n",
      "Epoch [20/50], Iter [200/886] loss : 2.3588\n",
      "Epoch [20/50], Iter [300/886] loss : 2.4127\n",
      "Epoch [20/50], Iter [400/886] loss : 2.3496\n",
      "Epoch [20/50], Iter [500/886] loss : 2.2599\n",
      "Epoch [20/50], Iter [600/886] loss : 2.3698\n",
      "Epoch [20/50], Iter [700/886] loss : 2.3799\n",
      "Epoch [20/50], Iter [800/886] loss : 2.2464\n",
      "Average loss: 2.3033 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [21/50], Iter [100/886] loss : 2.3557\n",
      "Epoch [21/50], Iter [200/886] loss : 2.3394\n",
      "Epoch [21/50], Iter [300/886] loss : 2.3158\n",
      "Epoch [21/50], Iter [400/886] loss : 2.3319\n",
      "Epoch [21/50], Iter [500/886] loss : 2.2772\n",
      "Epoch [21/50], Iter [600/886] loss : 2.2972\n",
      "Epoch [21/50], Iter [700/886] loss : 2.3448\n",
      "Epoch [21/50], Iter [800/886] loss : 2.3368\n",
      "Average loss: 2.2998 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [22/50], Iter [100/886] loss : 2.2785\n",
      "Epoch [22/50], Iter [200/886] loss : 2.3026\n",
      "Epoch [22/50], Iter [300/886] loss : 2.3901\n",
      "Epoch [22/50], Iter [400/886] loss : 2.2370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/50], Iter [500/886] loss : 2.3004\n",
      "Epoch [22/50], Iter [600/886] loss : 2.2177\n",
      "Epoch [22/50], Iter [700/886] loss : 2.2803\n",
      "Epoch [22/50], Iter [800/886] loss : 2.3186\n",
      "Average loss: 2.3027 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [23/50], Iter [100/886] loss : 2.3153\n",
      "Epoch [23/50], Iter [200/886] loss : 2.3138\n",
      "Epoch [23/50], Iter [300/886] loss : 2.3987\n",
      "Epoch [23/50], Iter [400/886] loss : 2.3173\n",
      "Epoch [23/50], Iter [500/886] loss : 2.3577\n",
      "Epoch [23/50], Iter [600/886] loss : 2.3877\n",
      "Epoch [23/50], Iter [700/886] loss : 2.2985\n",
      "Epoch [23/50], Iter [800/886] loss : 2.3414\n",
      "Average loss: 2.3080 \n",
      "\n",
      "Epoch    22: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Learning rate : 0.0008\n",
      "Epoch [24/50], Iter [100/886] loss : 2.2583\n",
      "Epoch [24/50], Iter [200/886] loss : 2.2271\n",
      "Epoch [24/50], Iter [300/886] loss : 2.2929\n",
      "Epoch [24/50], Iter [400/886] loss : 2.2751\n",
      "Epoch [24/50], Iter [500/886] loss : 2.2714\n",
      "Epoch [24/50], Iter [600/886] loss : 2.3437\n",
      "Epoch [24/50], Iter [700/886] loss : 2.3342\n",
      "Epoch [24/50], Iter [800/886] loss : 2.3086\n",
      "Average loss: 2.3026 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [25/50], Iter [100/886] loss : 2.3166\n",
      "Epoch [25/50], Iter [200/886] loss : 2.3270\n",
      "Epoch [25/50], Iter [300/886] loss : 2.3406\n",
      "Epoch [25/50], Iter [400/886] loss : 2.2761\n",
      "Epoch [25/50], Iter [500/886] loss : 2.3016\n",
      "Epoch [25/50], Iter [600/886] loss : 2.3257\n",
      "Epoch [25/50], Iter [700/886] loss : 2.2773\n",
      "Epoch [25/50], Iter [800/886] loss : 2.2984\n",
      "Average loss: 2.3024 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [26/50], Iter [100/886] loss : 2.3182\n",
      "Epoch [26/50], Iter [200/886] loss : 2.3292\n",
      "Epoch [26/50], Iter [300/886] loss : 2.2840\n",
      "Epoch [26/50], Iter [400/886] loss : 2.3076\n",
      "Epoch [26/50], Iter [500/886] loss : 2.3096\n",
      "Epoch [26/50], Iter [600/886] loss : 2.3097\n",
      "Epoch [26/50], Iter [700/886] loss : 2.3045\n",
      "Epoch [26/50], Iter [800/886] loss : 2.2671\n",
      "Average loss: 2.3012 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [27/50], Iter [100/886] loss : 2.2871\n",
      "Epoch [27/50], Iter [200/886] loss : 2.3110\n",
      "Epoch [27/50], Iter [300/886] loss : 2.3451\n",
      "Epoch [27/50], Iter [400/886] loss : 2.2862\n",
      "Epoch [27/50], Iter [500/886] loss : 2.2711\n",
      "Epoch [27/50], Iter [600/886] loss : 2.2846\n",
      "Epoch [27/50], Iter [700/886] loss : 2.3486\n",
      "Epoch [27/50], Iter [800/886] loss : 2.3455\n",
      "Average loss: 2.3032 \n",
      "\n",
      "Epoch    26: reducing learning rate of group 0 to 1.6000e-04.\n",
      "Learning rate : 0.00016\n",
      "Epoch [28/50], Iter [100/886] loss : 2.3163\n",
      "Epoch [28/50], Iter [200/886] loss : 2.2934\n",
      "Epoch [28/50], Iter [300/886] loss : 2.2855\n",
      "Epoch [28/50], Iter [400/886] loss : 2.2781\n",
      "Epoch [28/50], Iter [500/886] loss : 2.2972\n",
      "Epoch [28/50], Iter [600/886] loss : 2.2668\n",
      "Epoch [28/50], Iter [700/886] loss : 2.3012\n",
      "Epoch [28/50], Iter [800/886] loss : 2.3124\n",
      "Average loss: 2.3024 \n",
      "\n",
      "Learning rate : 0.00016\n",
      "Epoch [29/50], Iter [100/886] loss : 2.3156\n",
      "Epoch [29/50], Iter [200/886] loss : 2.2595\n",
      "Epoch [29/50], Iter [300/886] loss : 2.2766\n",
      "Epoch [29/50], Iter [400/886] loss : 2.3138\n",
      "Epoch [29/50], Iter [500/886] loss : 2.2925\n",
      "Epoch [29/50], Iter [600/886] loss : 2.3017\n",
      "Epoch [29/50], Iter [700/886] loss : 2.3073\n",
      "Epoch [29/50], Iter [800/886] loss : 2.3572\n",
      "Average loss: 2.3013 \n",
      "\n",
      "Learning rate : 0.00016\n",
      "Epoch [30/50], Iter [100/886] loss : 2.2916\n",
      "Epoch [30/50], Iter [200/886] loss : 2.3368\n",
      "Epoch [30/50], Iter [300/886] loss : 2.3030\n",
      "Epoch [30/50], Iter [400/886] loss : 2.2925\n",
      "Epoch [30/50], Iter [500/886] loss : 2.2773\n",
      "Epoch [30/50], Iter [600/886] loss : 2.2419\n",
      "Epoch [30/50], Iter [700/886] loss : 2.3097\n",
      "Epoch [30/50], Iter [800/886] loss : 2.2975\n",
      "Average loss: 2.3010 \n",
      "\n",
      "Learning rate : 0.00016\n",
      "Epoch [31/50], Iter [100/886] loss : 2.3165\n",
      "Epoch [31/50], Iter [200/886] loss : 2.3243\n",
      "Epoch [31/50], Iter [300/886] loss : 2.3199\n",
      "Epoch [31/50], Iter [400/886] loss : 2.3281\n",
      "Epoch [31/50], Iter [500/886] loss : 2.2819\n",
      "Epoch [31/50], Iter [600/886] loss : 2.2925\n",
      "Epoch [31/50], Iter [700/886] loss : 2.3527\n",
      "Epoch [31/50], Iter [800/886] loss : 2.3369\n",
      "Average loss: 2.3013 \n",
      "\n",
      "Epoch    30: reducing learning rate of group 0 to 3.2000e-05.\n",
      "Learning rate : 3.2e-05\n",
      "Epoch [32/50], Iter [100/886] loss : 2.3108\n",
      "Epoch [32/50], Iter [200/886] loss : 2.2550\n",
      "Epoch [32/50], Iter [300/886] loss : 2.2880\n",
      "Epoch [32/50], Iter [400/886] loss : 2.3267\n",
      "Epoch [32/50], Iter [500/886] loss : 2.3031\n",
      "Epoch [32/50], Iter [600/886] loss : 2.3106\n",
      "Epoch [32/50], Iter [700/886] loss : 2.3104\n",
      "Epoch [32/50], Iter [800/886] loss : 2.2938\n",
      "Average loss: 2.3011 \n",
      "\n",
      "Learning rate : 3.2e-05\n",
      "Epoch [33/50], Iter [100/886] loss : 2.3367\n",
      "Epoch [33/50], Iter [200/886] loss : 2.3437\n",
      "Epoch [33/50], Iter [300/886] loss : 2.3334\n",
      "Epoch [33/50], Iter [400/886] loss : 2.3072\n",
      "Epoch [33/50], Iter [500/886] loss : 2.3322\n",
      "Epoch [33/50], Iter [600/886] loss : 2.2863\n",
      "Epoch [33/50], Iter [700/886] loss : 2.3253\n",
      "Epoch [33/50], Iter [800/886] loss : 2.3527\n",
      "Average loss: 2.3011 \n",
      "\n",
      "Learning rate : 3.2e-05\n",
      "Epoch [34/50], Iter [100/886] loss : 2.2967\n",
      "Epoch [34/50], Iter [200/886] loss : 2.3111\n",
      "Epoch [34/50], Iter [300/886] loss : 2.3011\n",
      "Epoch [34/50], Iter [400/886] loss : 2.2752\n",
      "Epoch [34/50], Iter [500/886] loss : 2.3384\n",
      "Epoch [34/50], Iter [600/886] loss : 2.2543\n",
      "Epoch [34/50], Iter [700/886] loss : 2.2513\n",
      "Epoch [34/50], Iter [800/886] loss : 2.3027\n",
      "Average loss: 2.3010 \n",
      "\n",
      "Learning rate : 3.2e-05\n",
      "Epoch [35/50], Iter [100/886] loss : 2.2914\n",
      "Epoch [35/50], Iter [200/886] loss : 2.2855\n",
      "Epoch [35/50], Iter [300/886] loss : 2.3214\n",
      "Epoch [35/50], Iter [400/886] loss : 2.2901\n",
      "Epoch [35/50], Iter [500/886] loss : 2.2994\n",
      "Epoch [35/50], Iter [600/886] loss : 2.2969\n",
      "Epoch [35/50], Iter [700/886] loss : 2.2910\n",
      "Epoch [35/50], Iter [800/886] loss : 2.3310\n",
      "Average loss: 2.3009 \n",
      "\n",
      "Epoch    34: reducing learning rate of group 0 to 6.4000e-06.\n",
      "Learning rate : 6.4e-06\n",
      "Early stopping\n",
      "\n",
      "\n",
      "WD:   0.0001  DO:   0.5  LR:   0.1  -> VAL_loss: tensor(2.3009, device='cuda:0')\n",
      "Epoch [1/50], Iter [100/886] loss : 2.3071\n",
      "Epoch [1/50], Iter [200/886] loss : 2.2047\n",
      "Epoch [1/50], Iter [300/886] loss : 3.0919\n",
      "Epoch [1/50], Iter [400/886] loss : 3.3855\n",
      "Epoch [1/50], Iter [500/886] loss : 2.9095\n",
      "Epoch [1/50], Iter [600/886] loss : 3.0548\n",
      "Epoch [1/50], Iter [700/886] loss : 2.7568\n",
      "Epoch [1/50], Iter [800/886] loss : 2.3012\n",
      "Average loss: 2.5705 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [2/50], Iter [100/886] loss : 3.1896\n",
      "Epoch [2/50], Iter [200/886] loss : 2.8884\n",
      "Epoch [2/50], Iter [300/886] loss : 2.5763\n",
      "Epoch [2/50], Iter [400/886] loss : 3.3474\n",
      "Epoch [2/50], Iter [500/886] loss : 3.2019\n",
      "Epoch [2/50], Iter [600/886] loss : 2.4715\n",
      "Epoch [2/50], Iter [700/886] loss : 2.5581\n",
      "Epoch [2/50], Iter [800/886] loss : 3.1058\n",
      "Average loss: 2.8018 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [3/50], Iter [100/886] loss : 3.5035\n",
      "Epoch [3/50], Iter [200/886] loss : 2.9776\n",
      "Epoch [3/50], Iter [300/886] loss : 2.5101\n",
      "Epoch [3/50], Iter [400/886] loss : 2.3874\n",
      "Epoch [3/50], Iter [500/886] loss : 2.5401\n",
      "Epoch [3/50], Iter [600/886] loss : 2.7480\n",
      "Epoch [3/50], Iter [700/886] loss : 1.7727\n",
      "Epoch [3/50], Iter [800/886] loss : 2.0342\n",
      "Average loss: 2.6386 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [4/50], Iter [100/886] loss : 2.8089\n",
      "Epoch [4/50], Iter [200/886] loss : 3.5492\n",
      "Epoch [4/50], Iter [300/886] loss : 2.5312\n",
      "Epoch [4/50], Iter [400/886] loss : 3.3058\n",
      "Epoch [4/50], Iter [500/886] loss : 3.3137\n",
      "Epoch [4/50], Iter [600/886] loss : 3.0758\n",
      "Epoch [4/50], Iter [700/886] loss : 2.7325\n",
      "Epoch [4/50], Iter [800/886] loss : 2.2221\n",
      "Average loss: 2.6427 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [5/50], Iter [100/886] loss : 2.5952\n",
      "Epoch [5/50], Iter [200/886] loss : 3.0203\n",
      "Epoch [5/50], Iter [300/886] loss : 2.5778\n",
      "Epoch [5/50], Iter [400/886] loss : 2.8523\n",
      "Epoch [5/50], Iter [500/886] loss : 3.1647\n",
      "Epoch [5/50], Iter [600/886] loss : 4.1750\n",
      "Epoch [5/50], Iter [700/886] loss : 2.7677\n",
      "Epoch [5/50], Iter [800/886] loss : 2.5035\n",
      "Average loss: 2.5334 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [6/50], Iter [100/886] loss : 2.9318\n",
      "Epoch [6/50], Iter [200/886] loss : 3.1786\n",
      "Epoch [6/50], Iter [300/886] loss : 2.2717\n",
      "Epoch [6/50], Iter [400/886] loss : 3.3337\n",
      "Epoch [6/50], Iter [500/886] loss : 3.7519\n",
      "Epoch [6/50], Iter [600/886] loss : 3.2269\n",
      "Epoch [6/50], Iter [700/886] loss : 2.2202\n",
      "Epoch [6/50], Iter [800/886] loss : 2.3561\n",
      "Average loss: 2.4552 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [7/50], Iter [100/886] loss : 3.0715\n",
      "Epoch [7/50], Iter [200/886] loss : 2.3463\n",
      "Epoch [7/50], Iter [300/886] loss : 2.2303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Iter [400/886] loss : 3.1318\n",
      "Epoch [7/50], Iter [500/886] loss : 3.1466\n",
      "Epoch [7/50], Iter [600/886] loss : 3.5407\n",
      "Epoch [7/50], Iter [700/886] loss : 3.1090\n",
      "Epoch [7/50], Iter [800/886] loss : 2.4882\n",
      "Average loss: 2.6060 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [8/50], Iter [100/886] loss : 2.2547\n",
      "Epoch [8/50], Iter [200/886] loss : 2.3774\n",
      "Epoch [8/50], Iter [300/886] loss : 2.7109\n",
      "Epoch [8/50], Iter [400/886] loss : 2.5091\n",
      "Epoch [8/50], Iter [500/886] loss : 2.4345\n",
      "Epoch [8/50], Iter [600/886] loss : 3.1985\n",
      "Epoch [8/50], Iter [700/886] loss : 3.4269\n",
      "Epoch [8/50], Iter [800/886] loss : 2.9481\n",
      "Average loss: 2.9137 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [9/50], Iter [100/886] loss : 2.2145\n",
      "Epoch [9/50], Iter [200/886] loss : 2.7515\n",
      "Epoch [9/50], Iter [300/886] loss : 3.4920\n",
      "Epoch [9/50], Iter [400/886] loss : 2.6600\n",
      "Epoch [9/50], Iter [500/886] loss : 2.2258\n",
      "Epoch [9/50], Iter [600/886] loss : 1.9913\n",
      "Epoch [9/50], Iter [700/886] loss : 3.3766\n",
      "Epoch [9/50], Iter [800/886] loss : 4.4351\n",
      "Average loss: 2.5248 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [10/50], Iter [100/886] loss : 3.1910\n",
      "Epoch [10/50], Iter [200/886] loss : 2.9575\n",
      "Epoch [10/50], Iter [300/886] loss : 3.0060\n",
      "Epoch [10/50], Iter [400/886] loss : 3.6588\n",
      "Epoch [10/50], Iter [500/886] loss : 1.9002\n",
      "Epoch [10/50], Iter [600/886] loss : 3.5088\n",
      "Epoch [10/50], Iter [700/886] loss : 3.7523\n",
      "Epoch [10/50], Iter [800/886] loss : 3.7400\n",
      "Average loss: 2.7161 \n",
      "\n",
      "Epoch     9: reducing learning rate of group 0 to 2.0000e-02.\n",
      "Learning rate : 0.02\n",
      "Epoch [11/50], Iter [100/886] loss : 2.5467\n",
      "Epoch [11/50], Iter [200/886] loss : 2.1752\n",
      "Epoch [11/50], Iter [300/886] loss : 2.3311\n",
      "Epoch [11/50], Iter [400/886] loss : 2.6719\n",
      "Epoch [11/50], Iter [500/886] loss : 2.5004\n",
      "Epoch [11/50], Iter [600/886] loss : 2.2371\n",
      "Epoch [11/50], Iter [700/886] loss : 2.1677\n",
      "Epoch [11/50], Iter [800/886] loss : 2.5808\n",
      "Average loss: 2.3471 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [12/50], Iter [100/886] loss : 2.3039\n",
      "Epoch [12/50], Iter [200/886] loss : 2.3332\n",
      "Epoch [12/50], Iter [300/886] loss : 2.1753\n",
      "Epoch [12/50], Iter [400/886] loss : 2.4972\n",
      "Epoch [12/50], Iter [500/886] loss : 2.3362\n",
      "Epoch [12/50], Iter [600/886] loss : 2.2149\n",
      "Epoch [12/50], Iter [700/886] loss : 2.4040\n",
      "Epoch [12/50], Iter [800/886] loss : 2.3012\n",
      "Average loss: 2.3263 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [13/50], Iter [100/886] loss : 2.5639\n",
      "Epoch [13/50], Iter [200/886] loss : 2.3974\n",
      "Epoch [13/50], Iter [300/886] loss : 2.3976\n",
      "Epoch [13/50], Iter [400/886] loss : 2.2691\n",
      "Epoch [13/50], Iter [500/886] loss : 2.2026\n",
      "Epoch [13/50], Iter [600/886] loss : 2.4031\n",
      "Epoch [13/50], Iter [700/886] loss : 2.2075\n",
      "Epoch [13/50], Iter [800/886] loss : 2.1431\n",
      "Average loss: 2.3369 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [14/50], Iter [100/886] loss : 2.2409\n",
      "Epoch [14/50], Iter [200/886] loss : 2.2855\n",
      "Epoch [14/50], Iter [300/886] loss : 2.1215\n",
      "Epoch [14/50], Iter [400/886] loss : 2.2337\n",
      "Epoch [14/50], Iter [500/886] loss : 2.3120\n",
      "Epoch [14/50], Iter [600/886] loss : 2.3918\n",
      "Epoch [14/50], Iter [700/886] loss : 2.3541\n",
      "Epoch [14/50], Iter [800/886] loss : 2.7802\n",
      "Average loss: 2.3245 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [15/50], Iter [100/886] loss : 2.4749\n",
      "Epoch [15/50], Iter [200/886] loss : 2.3596\n",
      "Epoch [15/50], Iter [300/886] loss : 2.5922\n",
      "Epoch [15/50], Iter [400/886] loss : 2.1917\n",
      "Epoch [15/50], Iter [500/886] loss : 2.5280\n",
      "Epoch [15/50], Iter [600/886] loss : 2.2285\n",
      "Epoch [15/50], Iter [700/886] loss : 2.4818\n",
      "Epoch [15/50], Iter [800/886] loss : 2.1467\n",
      "Average loss: 2.3185 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [16/50], Iter [100/886] loss : 2.3637\n",
      "Epoch [16/50], Iter [200/886] loss : 2.2965\n",
      "Epoch [16/50], Iter [300/886] loss : 2.3389\n",
      "Epoch [16/50], Iter [400/886] loss : 2.2782\n",
      "Epoch [16/50], Iter [500/886] loss : 2.2767\n",
      "Epoch [16/50], Iter [600/886] loss : 2.4559\n",
      "Epoch [16/50], Iter [700/886] loss : 2.6331\n",
      "Epoch [16/50], Iter [800/886] loss : 2.6032\n",
      "Average loss: 2.3435 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [17/50], Iter [100/886] loss : 2.2228\n",
      "Epoch [17/50], Iter [200/886] loss : 2.2186\n",
      "Epoch [17/50], Iter [300/886] loss : 2.3537\n",
      "Epoch [17/50], Iter [400/886] loss : 2.3007\n",
      "Epoch [17/50], Iter [500/886] loss : 2.4819\n",
      "Epoch [17/50], Iter [600/886] loss : 2.3985\n",
      "Epoch [17/50], Iter [700/886] loss : 2.3270\n",
      "Epoch [17/50], Iter [800/886] loss : 2.3485\n",
      "Average loss: 2.3354 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [18/50], Iter [100/886] loss : 2.3804\n",
      "Epoch [18/50], Iter [200/886] loss : 2.4579\n",
      "Epoch [18/50], Iter [300/886] loss : 2.4248\n",
      "Epoch [18/50], Iter [400/886] loss : 2.3360\n",
      "Epoch [18/50], Iter [500/886] loss : 2.4809\n",
      "Epoch [18/50], Iter [600/886] loss : 2.1391\n",
      "Epoch [18/50], Iter [700/886] loss : 2.5895\n",
      "Epoch [18/50], Iter [800/886] loss : 2.4202\n",
      "Average loss: 2.3221 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [19/50], Iter [100/886] loss : 2.3084\n",
      "Epoch [19/50], Iter [200/886] loss : 2.0569\n",
      "Epoch [19/50], Iter [300/886] loss : 2.5622\n",
      "Epoch [19/50], Iter [400/886] loss : 2.2750\n",
      "Epoch [19/50], Iter [500/886] loss : 2.6457\n",
      "Epoch [19/50], Iter [600/886] loss : 2.2276\n",
      "Epoch [19/50], Iter [700/886] loss : 2.0401\n",
      "Epoch [19/50], Iter [800/886] loss : 2.2909\n",
      "Average loss: 2.3688 \n",
      "\n",
      "Epoch    18: reducing learning rate of group 0 to 4.0000e-03.\n",
      "Learning rate : 0.004\n",
      "Epoch [20/50], Iter [100/886] loss : 2.3779\n",
      "Epoch [20/50], Iter [200/886] loss : 2.3291\n",
      "Epoch [20/50], Iter [300/886] loss : 2.3882\n",
      "Epoch [20/50], Iter [400/886] loss : 2.2668\n",
      "Epoch [20/50], Iter [500/886] loss : 2.2677\n",
      "Epoch [20/50], Iter [600/886] loss : 2.2307\n",
      "Epoch [20/50], Iter [700/886] loss : 2.2952\n",
      "Epoch [20/50], Iter [800/886] loss : 2.4045\n",
      "Average loss: 2.3000 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [21/50], Iter [100/886] loss : 2.1585\n",
      "Epoch [21/50], Iter [200/886] loss : 2.2369\n",
      "Epoch [21/50], Iter [300/886] loss : 2.3130\n",
      "Epoch [21/50], Iter [400/886] loss : 2.3254\n",
      "Epoch [21/50], Iter [500/886] loss : 2.2674\n",
      "Epoch [21/50], Iter [600/886] loss : 2.2694\n",
      "Epoch [21/50], Iter [700/886] loss : 2.3005\n",
      "Epoch [21/50], Iter [800/886] loss : 2.2470\n",
      "Average loss: 2.3071 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [22/50], Iter [100/886] loss : 2.3308\n",
      "Epoch [22/50], Iter [200/886] loss : 2.2164\n",
      "Epoch [22/50], Iter [300/886] loss : 2.3014\n",
      "Epoch [22/50], Iter [400/886] loss : 2.2739\n",
      "Epoch [22/50], Iter [500/886] loss : 2.2368\n",
      "Epoch [22/50], Iter [600/886] loss : 2.3607\n",
      "Epoch [22/50], Iter [700/886] loss : 2.2735\n",
      "Epoch [22/50], Iter [800/886] loss : 2.3531\n",
      "Average loss: 2.3034 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [23/50], Iter [100/886] loss : 2.4467\n",
      "Epoch [23/50], Iter [200/886] loss : 2.3905\n",
      "Epoch [23/50], Iter [300/886] loss : 2.3119\n",
      "Epoch [23/50], Iter [400/886] loss : 2.3091\n",
      "Epoch [23/50], Iter [500/886] loss : 2.1653\n",
      "Epoch [23/50], Iter [600/886] loss : 2.0427\n",
      "Epoch [23/50], Iter [700/886] loss : 1.8347\n",
      "Epoch [23/50], Iter [800/886] loss : 1.9470\n",
      "Average loss: 2.1207 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [24/50], Iter [100/886] loss : 2.0532\n",
      "Epoch [24/50], Iter [200/886] loss : 1.8392\n",
      "Epoch [24/50], Iter [300/886] loss : 2.1796\n",
      "Epoch [24/50], Iter [400/886] loss : 1.7402\n",
      "Epoch [24/50], Iter [500/886] loss : 1.9751\n",
      "Epoch [24/50], Iter [600/886] loss : 1.9873\n",
      "Epoch [24/50], Iter [700/886] loss : 1.8901\n",
      "Epoch [24/50], Iter [800/886] loss : 1.9345\n",
      "Average loss: 2.0508 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [25/50], Iter [100/886] loss : 1.7059\n",
      "Epoch [25/50], Iter [200/886] loss : 1.6676\n",
      "Epoch [25/50], Iter [300/886] loss : 2.2481\n",
      "Epoch [25/50], Iter [400/886] loss : 1.6225\n",
      "Epoch [25/50], Iter [500/886] loss : 2.3020\n",
      "Epoch [25/50], Iter [600/886] loss : 2.1431\n",
      "Epoch [25/50], Iter [700/886] loss : 1.8356\n",
      "Epoch [25/50], Iter [800/886] loss : 1.8917\n",
      "Average loss: 2.0643 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [26/50], Iter [100/886] loss : 1.7202\n",
      "Epoch [26/50], Iter [200/886] loss : 2.1762\n",
      "Epoch [26/50], Iter [300/886] loss : 2.3899\n",
      "Epoch [26/50], Iter [400/886] loss : 2.2932\n",
      "Epoch [26/50], Iter [500/886] loss : 2.0758\n",
      "Epoch [26/50], Iter [600/886] loss : 1.9923\n",
      "Epoch [26/50], Iter [700/886] loss : 1.9916\n",
      "Epoch [26/50], Iter [800/886] loss : 2.9020\n",
      "Average loss: 2.1266 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [27/50], Iter [100/886] loss : 1.5956\n",
      "Epoch [27/50], Iter [200/886] loss : 2.0448\n",
      "Epoch [27/50], Iter [300/886] loss : 2.0796\n",
      "Epoch [27/50], Iter [400/886] loss : 1.9286\n",
      "Epoch [27/50], Iter [500/886] loss : 2.0099\n",
      "Epoch [27/50], Iter [600/886] loss : 1.5936\n",
      "Epoch [27/50], Iter [700/886] loss : 2.6581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/50], Iter [800/886] loss : 2.1625\n",
      "Average loss: 2.1953 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [28/50], Iter [100/886] loss : 1.8469\n",
      "Epoch [28/50], Iter [200/886] loss : 1.9520\n",
      "Epoch [28/50], Iter [300/886] loss : 1.9468\n",
      "Epoch [28/50], Iter [400/886] loss : 1.8569\n",
      "Epoch [28/50], Iter [500/886] loss : 1.7766\n",
      "Epoch [28/50], Iter [600/886] loss : 1.8787\n",
      "Epoch [28/50], Iter [700/886] loss : 1.6422\n",
      "Epoch [28/50], Iter [800/886] loss : 2.1342\n",
      "Average loss: 2.0425 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [29/50], Iter [100/886] loss : 3.0633\n",
      "Epoch [29/50], Iter [200/886] loss : 1.7315\n",
      "Epoch [29/50], Iter [300/886] loss : 1.6246\n",
      "Epoch [29/50], Iter [400/886] loss : 1.6255\n",
      "Epoch [29/50], Iter [500/886] loss : 1.1315\n",
      "Epoch [29/50], Iter [600/886] loss : 2.1024\n",
      "Epoch [29/50], Iter [700/886] loss : 2.1402\n",
      "Epoch [29/50], Iter [800/886] loss : 1.6759\n",
      "Average loss: 2.0515 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [30/50], Iter [100/886] loss : 1.8813\n",
      "Epoch [30/50], Iter [200/886] loss : 2.4971\n",
      "Epoch [30/50], Iter [300/886] loss : 2.2299\n",
      "Epoch [30/50], Iter [400/886] loss : 1.9256\n",
      "Epoch [30/50], Iter [500/886] loss : 2.6020\n",
      "Epoch [30/50], Iter [600/886] loss : 2.6938\n",
      "Epoch [30/50], Iter [700/886] loss : 1.3137\n",
      "Epoch [30/50], Iter [800/886] loss : 2.4039\n",
      "Average loss: 2.0298 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [31/50], Iter [100/886] loss : 1.8284\n",
      "Epoch [31/50], Iter [200/886] loss : 2.3394\n",
      "Epoch [31/50], Iter [300/886] loss : 1.5191\n",
      "Epoch [31/50], Iter [400/886] loss : 1.6865\n",
      "Epoch [31/50], Iter [500/886] loss : 1.5933\n",
      "Epoch [31/50], Iter [600/886] loss : 1.8644\n",
      "Epoch [31/50], Iter [700/886] loss : 2.0117\n",
      "Epoch [31/50], Iter [800/886] loss : 1.6119\n",
      "Average loss: 1.9095 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [32/50], Iter [100/886] loss : 1.2457\n",
      "Epoch [32/50], Iter [200/886] loss : 2.0027\n",
      "Epoch [32/50], Iter [300/886] loss : 1.7139\n",
      "Epoch [32/50], Iter [400/886] loss : 1.9845\n",
      "Epoch [32/50], Iter [500/886] loss : 1.6544\n",
      "Epoch [32/50], Iter [600/886] loss : 1.6459\n",
      "Epoch [32/50], Iter [700/886] loss : 1.6956\n",
      "Epoch [32/50], Iter [800/886] loss : 1.9319\n",
      "Average loss: 1.9079 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [33/50], Iter [100/886] loss : 1.8866\n",
      "Epoch [33/50], Iter [200/886] loss : 1.2407\n",
      "Epoch [33/50], Iter [300/886] loss : 1.9699\n",
      "Epoch [33/50], Iter [400/886] loss : 0.9344\n",
      "Epoch [33/50], Iter [500/886] loss : 1.9490\n",
      "Epoch [33/50], Iter [600/886] loss : 2.2422\n",
      "Epoch [33/50], Iter [700/886] loss : 1.7510\n",
      "Epoch [33/50], Iter [800/886] loss : 1.3076\n",
      "Average loss: 1.9627 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [34/50], Iter [100/886] loss : 1.7698\n",
      "Epoch [34/50], Iter [200/886] loss : 1.0699\n",
      "Epoch [34/50], Iter [300/886] loss : 1.7950\n",
      "Epoch [34/50], Iter [400/886] loss : 1.1170\n",
      "Epoch [34/50], Iter [500/886] loss : 1.3814\n",
      "Epoch [34/50], Iter [600/886] loss : 3.3895\n",
      "Epoch [34/50], Iter [700/886] loss : 1.1256\n",
      "Epoch [34/50], Iter [800/886] loss : 1.9805\n",
      "Average loss: 1.8271 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [35/50], Iter [100/886] loss : 1.2078\n",
      "Epoch [35/50], Iter [200/886] loss : 1.8985\n",
      "Epoch [35/50], Iter [300/886] loss : 1.5245\n",
      "Epoch [35/50], Iter [400/886] loss : 2.1654\n",
      "Epoch [35/50], Iter [500/886] loss : 2.0200\n",
      "Epoch [35/50], Iter [600/886] loss : 0.8531\n",
      "Epoch [35/50], Iter [700/886] loss : 1.7456\n",
      "Epoch [35/50], Iter [800/886] loss : 1.4840\n",
      "Average loss: 1.9174 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [36/50], Iter [100/886] loss : 2.3620\n",
      "Epoch [36/50], Iter [200/886] loss : 1.3864\n",
      "Epoch [36/50], Iter [300/886] loss : 1.5321\n",
      "Epoch [36/50], Iter [400/886] loss : 1.4523\n",
      "Epoch [36/50], Iter [500/886] loss : 1.7293\n",
      "Epoch [36/50], Iter [600/886] loss : 1.7700\n",
      "Epoch [36/50], Iter [700/886] loss : 1.4562\n",
      "Epoch [36/50], Iter [800/886] loss : 1.3008\n",
      "Average loss: 1.8879 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [37/50], Iter [100/886] loss : 1.2491\n",
      "Epoch [37/50], Iter [200/886] loss : 1.0789\n",
      "Epoch [37/50], Iter [300/886] loss : 0.7125\n",
      "Epoch [37/50], Iter [400/886] loss : 1.5220\n",
      "Epoch [37/50], Iter [500/886] loss : 1.3369\n",
      "Epoch [37/50], Iter [600/886] loss : 1.4049\n",
      "Epoch [37/50], Iter [700/886] loss : 1.5944\n",
      "Epoch [37/50], Iter [800/886] loss : 2.0778\n",
      "Average loss: 2.0545 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [38/50], Iter [100/886] loss : 1.5278\n",
      "Epoch [38/50], Iter [200/886] loss : 1.4322\n",
      "Epoch [38/50], Iter [300/886] loss : 1.4018\n",
      "Epoch [38/50], Iter [400/886] loss : 1.1277\n",
      "Epoch [38/50], Iter [500/886] loss : 1.0281\n",
      "Epoch [38/50], Iter [600/886] loss : 0.9665\n",
      "Epoch [38/50], Iter [700/886] loss : 1.3835\n",
      "Epoch [38/50], Iter [800/886] loss : 1.6608\n",
      "Average loss: 1.9535 \n",
      "\n",
      "Epoch    37: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Learning rate : 0.0008\n",
      "Epoch [39/50], Iter [100/886] loss : 1.5761\n",
      "Epoch [39/50], Iter [200/886] loss : 1.3279\n",
      "Epoch [39/50], Iter [300/886] loss : 0.7848\n",
      "Epoch [39/50], Iter [400/886] loss : 1.5344\n",
      "Epoch [39/50], Iter [500/886] loss : 1.5325\n",
      "Epoch [39/50], Iter [600/886] loss : 1.6284\n",
      "Epoch [39/50], Iter [700/886] loss : 1.8158\n",
      "Epoch [39/50], Iter [800/886] loss : 1.8340\n",
      "Average loss: 1.9591 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [40/50], Iter [100/886] loss : 1.3860\n",
      "Epoch [40/50], Iter [200/886] loss : 1.1563\n",
      "Epoch [40/50], Iter [300/886] loss : 2.2235\n",
      "Epoch [40/50], Iter [400/886] loss : 1.2815\n",
      "Epoch [40/50], Iter [500/886] loss : 2.1573\n",
      "Epoch [40/50], Iter [600/886] loss : 2.0525\n",
      "Epoch [40/50], Iter [700/886] loss : 1.2855\n",
      "Epoch [40/50], Iter [800/886] loss : 1.1267\n",
      "Average loss: 1.9983 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [41/50], Iter [100/886] loss : 1.1676\n",
      "Epoch [41/50], Iter [200/886] loss : 1.1817\n",
      "Epoch [41/50], Iter [300/886] loss : 1.0644\n",
      "Epoch [41/50], Iter [400/886] loss : 1.5618\n",
      "Epoch [41/50], Iter [500/886] loss : 1.1690\n",
      "Epoch [41/50], Iter [600/886] loss : 1.1742\n",
      "Epoch [41/50], Iter [700/886] loss : 1.5382\n",
      "Epoch [41/50], Iter [800/886] loss : 1.2196\n",
      "Average loss: 1.9269 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [42/50], Iter [100/886] loss : 1.3691\n",
      "Epoch [42/50], Iter [200/886] loss : 1.1055\n",
      "Epoch [42/50], Iter [300/886] loss : 1.7745\n",
      "Epoch [42/50], Iter [400/886] loss : 1.3678\n",
      "Epoch [42/50], Iter [500/886] loss : 1.2494\n",
      "Epoch [42/50], Iter [600/886] loss : 1.4377\n",
      "Epoch [42/50], Iter [700/886] loss : 1.0597\n",
      "Epoch [42/50], Iter [800/886] loss : 0.9970\n",
      "Average loss: 1.9571 \n",
      "\n",
      "Epoch    41: reducing learning rate of group 0 to 1.6000e-04.\n",
      "Learning rate : 0.00016\n",
      "Epoch [43/50], Iter [100/886] loss : 1.9387\n",
      "Epoch [43/50], Iter [200/886] loss : 1.3079\n",
      "Epoch [43/50], Iter [300/886] loss : 1.4895\n",
      "Epoch [43/50], Iter [400/886] loss : 0.9827\n",
      "Epoch [43/50], Iter [500/886] loss : 1.3262\n",
      "Epoch [43/50], Iter [600/886] loss : 1.6808\n",
      "Epoch [43/50], Iter [700/886] loss : 1.2236\n",
      "Epoch [43/50], Iter [800/886] loss : 1.8619\n",
      "Average loss: 2.0112 \n",
      "\n",
      "Learning rate : 0.00016\n",
      "Epoch [44/50], Iter [100/886] loss : 0.8531\n",
      "Epoch [44/50], Iter [200/886] loss : 1.8083\n",
      "Epoch [44/50], Iter [300/886] loss : 2.0502\n",
      "Epoch [44/50], Iter [400/886] loss : 1.9143\n",
      "Epoch [44/50], Iter [500/886] loss : 1.2392\n",
      "Epoch [44/50], Iter [600/886] loss : 1.4645\n",
      "Epoch [44/50], Iter [700/886] loss : 1.9609\n",
      "Epoch [44/50], Iter [800/886] loss : 1.1224\n",
      "Average loss: 1.9266 \n",
      "\n",
      "Learning rate : 0.00016\n",
      "Epoch [45/50], Iter [100/886] loss : 2.7753\n",
      "Epoch [45/50], Iter [200/886] loss : 1.5346\n",
      "Epoch [45/50], Iter [300/886] loss : 1.8795\n",
      "Epoch [45/50], Iter [400/886] loss : 0.8286\n",
      "Epoch [45/50], Iter [500/886] loss : 1.4017\n",
      "Epoch [45/50], Iter [600/886] loss : 1.3372\n",
      "Epoch [45/50], Iter [700/886] loss : 1.2600\n",
      "Epoch [45/50], Iter [800/886] loss : 1.6285\n",
      "Average loss: 1.9414 \n",
      "\n",
      "Learning rate : 0.00016\n",
      "Epoch [46/50], Iter [100/886] loss : 1.2857\n",
      "Epoch [46/50], Iter [200/886] loss : 1.5238\n",
      "Epoch [46/50], Iter [300/886] loss : 1.8472\n",
      "Epoch [46/50], Iter [400/886] loss : 1.7529\n",
      "Epoch [46/50], Iter [500/886] loss : 1.6118\n",
      "Epoch [46/50], Iter [600/886] loss : 1.7913\n",
      "Epoch [46/50], Iter [700/886] loss : 2.7526\n",
      "Epoch [46/50], Iter [800/886] loss : 1.4838\n",
      "Average loss: 2.0153 \n",
      "\n",
      "Epoch    45: reducing learning rate of group 0 to 3.2000e-05.\n",
      "Learning rate : 3.2e-05\n",
      "Epoch [47/50], Iter [100/886] loss : 1.3339\n",
      "Epoch [47/50], Iter [200/886] loss : 1.0271\n",
      "Epoch [47/50], Iter [300/886] loss : 1.2407\n",
      "Epoch [47/50], Iter [400/886] loss : 0.9437\n",
      "Epoch [47/50], Iter [500/886] loss : 1.6258\n",
      "Epoch [47/50], Iter [600/886] loss : 1.4001\n",
      "Epoch [47/50], Iter [700/886] loss : 1.3062\n",
      "Epoch [47/50], Iter [800/886] loss : 1.1287\n",
      "Average loss: 1.9467 \n",
      "\n",
      "Learning rate : 3.2e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/50], Iter [100/886] loss : 1.0970\n",
      "Epoch [48/50], Iter [200/886] loss : 1.0720\n",
      "Epoch [48/50], Iter [300/886] loss : 1.6612\n",
      "Epoch [48/50], Iter [400/886] loss : 1.8261\n",
      "Epoch [48/50], Iter [500/886] loss : 1.1806\n",
      "Epoch [48/50], Iter [600/886] loss : 1.3995\n",
      "Epoch [48/50], Iter [700/886] loss : 2.0432\n",
      "Epoch [48/50], Iter [800/886] loss : 1.7389\n",
      "Average loss: 1.9101 \n",
      "\n",
      "Learning rate : 3.2e-05\n",
      "Epoch [49/50], Iter [100/886] loss : 1.6213\n",
      "Epoch [49/50], Iter [200/886] loss : 1.8095\n",
      "Epoch [49/50], Iter [300/886] loss : 2.2248\n",
      "Epoch [49/50], Iter [400/886] loss : 1.5407\n",
      "Epoch [49/50], Iter [500/886] loss : 1.3801\n",
      "Epoch [49/50], Iter [600/886] loss : 1.2554\n",
      "Epoch [49/50], Iter [700/886] loss : 1.0164\n",
      "Epoch [49/50], Iter [800/886] loss : 1.9255\n",
      "Average loss: 1.9831 \n",
      "\n",
      "Learning rate : 3.2e-05\n",
      "Epoch [50/50], Iter [100/886] loss : 1.5139\n",
      "Epoch [50/50], Iter [200/886] loss : 1.6085\n",
      "Epoch [50/50], Iter [300/886] loss : 1.4703\n",
      "Epoch [50/50], Iter [400/886] loss : 1.0483\n",
      "Epoch [50/50], Iter [500/886] loss : 2.2601\n",
      "Epoch [50/50], Iter [600/886] loss : 1.1890\n",
      "Epoch [50/50], Iter [700/886] loss : 1.4081\n",
      "Epoch [50/50], Iter [800/886] loss : 1.0307\n",
      "Average loss: 1.9450 \n",
      "\n",
      "Epoch    49: reducing learning rate of group 0 to 6.4000e-06.\n",
      "Learning rate : 6.4e-06\n",
      "Early stopping\n",
      "\n",
      "\n",
      "WD:   0.0001  DO:   0.6  LR:   0.1  -> VAL_loss: tensor(1.9450, device='cuda:0')\n",
      "Epoch [1/50], Iter [100/886] loss : 3.3870\n",
      "Epoch [1/50], Iter [200/886] loss : 2.6564\n",
      "Epoch [1/50], Iter [300/886] loss : 2.9733\n",
      "Epoch [1/50], Iter [400/886] loss : 2.5519\n",
      "Epoch [1/50], Iter [500/886] loss : 3.1731\n",
      "Epoch [1/50], Iter [600/886] loss : 2.4911\n",
      "Epoch [1/50], Iter [700/886] loss : 2.6862\n",
      "Epoch [1/50], Iter [800/886] loss : 2.7690\n",
      "Average loss: 2.3562 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [2/50], Iter [100/886] loss : 2.2691\n",
      "Epoch [2/50], Iter [200/886] loss : 2.8835\n",
      "Epoch [2/50], Iter [300/886] loss : 2.5356\n",
      "Epoch [2/50], Iter [400/886] loss : 2.8346\n",
      "Epoch [2/50], Iter [500/886] loss : 3.6484\n",
      "Epoch [2/50], Iter [600/886] loss : 3.3656\n",
      "Epoch [2/50], Iter [700/886] loss : 1.8908\n",
      "Epoch [2/50], Iter [800/886] loss : 2.4535\n",
      "Average loss: 2.8059 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [3/50], Iter [100/886] loss : 3.0263\n",
      "Epoch [3/50], Iter [200/886] loss : 2.4329\n",
      "Epoch [3/50], Iter [300/886] loss : 2.8419\n",
      "Epoch [3/50], Iter [400/886] loss : 2.8391\n",
      "Epoch [3/50], Iter [500/886] loss : 2.7381\n",
      "Epoch [3/50], Iter [600/886] loss : 3.0766\n",
      "Epoch [3/50], Iter [700/886] loss : 2.6722\n",
      "Epoch [3/50], Iter [800/886] loss : 2.5470\n",
      "Average loss: 2.7431 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [4/50], Iter [100/886] loss : 2.4384\n",
      "Epoch [4/50], Iter [200/886] loss : 3.8618\n",
      "Epoch [4/50], Iter [300/886] loss : 2.7444\n",
      "Epoch [4/50], Iter [400/886] loss : 3.8628\n",
      "Epoch [4/50], Iter [500/886] loss : 2.8415\n",
      "Epoch [4/50], Iter [600/886] loss : 2.0519\n",
      "Epoch [4/50], Iter [700/886] loss : 1.9203\n",
      "Epoch [4/50], Iter [800/886] loss : 3.3267\n",
      "Average loss: 2.8343 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [5/50], Iter [100/886] loss : 3.1926\n",
      "Epoch [5/50], Iter [200/886] loss : 2.8951\n",
      "Epoch [5/50], Iter [300/886] loss : 2.8918\n",
      "Epoch [5/50], Iter [400/886] loss : 2.9693\n",
      "Epoch [5/50], Iter [500/886] loss : 2.5607\n",
      "Epoch [5/50], Iter [600/886] loss : 2.8862\n",
      "Epoch [5/50], Iter [700/886] loss : 2.7018\n",
      "Epoch [5/50], Iter [800/886] loss : 2.3659\n",
      "Average loss: 2.6925 \n",
      "\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-02.\n",
      "Learning rate : 0.02\n",
      "Epoch [6/50], Iter [100/886] loss : 2.4487\n",
      "Epoch [6/50], Iter [200/886] loss : 2.0996\n",
      "Epoch [6/50], Iter [300/886] loss : 2.5619\n",
      "Epoch [6/50], Iter [400/886] loss : 2.4121\n",
      "Epoch [6/50], Iter [500/886] loss : 2.2381\n",
      "Epoch [6/50], Iter [600/886] loss : 2.4204\n",
      "Epoch [6/50], Iter [700/886] loss : 2.2278\n",
      "Epoch [6/50], Iter [800/886] loss : 2.2237\n",
      "Average loss: 2.3356 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [7/50], Iter [100/886] loss : 2.3000\n",
      "Epoch [7/50], Iter [200/886] loss : 2.3752\n",
      "Epoch [7/50], Iter [300/886] loss : 2.3247\n",
      "Epoch [7/50], Iter [400/886] loss : 2.2268\n",
      "Epoch [7/50], Iter [500/886] loss : 2.3076\n",
      "Epoch [7/50], Iter [600/886] loss : 2.2833\n",
      "Epoch [7/50], Iter [700/886] loss : 2.2659\n",
      "Epoch [7/50], Iter [800/886] loss : 2.3030\n",
      "Average loss: 2.3208 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [8/50], Iter [100/886] loss : 2.4763\n",
      "Epoch [8/50], Iter [200/886] loss : 2.4597\n",
      "Epoch [8/50], Iter [300/886] loss : 2.3225\n",
      "Epoch [8/50], Iter [400/886] loss : 2.3762\n",
      "Epoch [8/50], Iter [500/886] loss : 2.1870\n",
      "Epoch [8/50], Iter [600/886] loss : 2.4029\n",
      "Epoch [8/50], Iter [700/886] loss : 2.3686\n",
      "Epoch [8/50], Iter [800/886] loss : 2.1111\n",
      "Average loss: 2.3512 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [9/50], Iter [100/886] loss : 2.3543\n",
      "Epoch [9/50], Iter [200/886] loss : 2.3773\n",
      "Epoch [9/50], Iter [300/886] loss : 2.5104\n",
      "Epoch [9/50], Iter [400/886] loss : 2.3752\n",
      "Epoch [9/50], Iter [500/886] loss : 2.4095\n",
      "Epoch [9/50], Iter [600/886] loss : 2.5520\n",
      "Epoch [9/50], Iter [700/886] loss : 2.4352\n",
      "Epoch [9/50], Iter [800/886] loss : 2.2732\n",
      "Average loss: 2.3236 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [10/50], Iter [100/886] loss : 2.5209\n",
      "Epoch [10/50], Iter [200/886] loss : 2.3043\n",
      "Epoch [10/50], Iter [300/886] loss : 2.5412\n",
      "Epoch [10/50], Iter [400/886] loss : 2.4337\n",
      "Epoch [10/50], Iter [500/886] loss : 2.2939\n",
      "Epoch [10/50], Iter [600/886] loss : 2.2957\n",
      "Epoch [10/50], Iter [700/886] loss : 2.3307\n",
      "Epoch [10/50], Iter [800/886] loss : 2.6290\n",
      "Average loss: 2.3652 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [11/50], Iter [100/886] loss : 2.0938\n",
      "Epoch [11/50], Iter [200/886] loss : 1.8545\n",
      "Epoch [11/50], Iter [300/886] loss : 2.3990\n",
      "Epoch [11/50], Iter [400/886] loss : 2.4788\n",
      "Epoch [11/50], Iter [500/886] loss : 2.2266\n",
      "Epoch [11/50], Iter [600/886] loss : 2.2489\n",
      "Epoch [11/50], Iter [700/886] loss : 2.1964\n",
      "Epoch [11/50], Iter [800/886] loss : 2.2434\n",
      "Average loss: 2.3298 \n",
      "\n",
      "Epoch    10: reducing learning rate of group 0 to 4.0000e-03.\n",
      "Learning rate : 0.004\n",
      "Epoch [12/50], Iter [100/886] loss : 2.2900\n",
      "Epoch [12/50], Iter [200/886] loss : 2.2413\n",
      "Epoch [12/50], Iter [300/886] loss : 2.4258\n",
      "Epoch [12/50], Iter [400/886] loss : 2.3181\n",
      "Epoch [12/50], Iter [500/886] loss : 2.2378\n",
      "Epoch [12/50], Iter [600/886] loss : 2.2507\n",
      "Epoch [12/50], Iter [700/886] loss : 2.2609\n",
      "Epoch [12/50], Iter [800/886] loss : 2.2818\n",
      "Average loss: 2.3081 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [13/50], Iter [100/886] loss : 2.2601\n",
      "Epoch [13/50], Iter [200/886] loss : 2.3019\n",
      "Epoch [13/50], Iter [300/886] loss : 2.2860\n",
      "Epoch [13/50], Iter [400/886] loss : 2.2276\n",
      "Epoch [13/50], Iter [500/886] loss : 2.3720\n",
      "Epoch [13/50], Iter [600/886] loss : 2.2455\n",
      "Epoch [13/50], Iter [700/886] loss : 2.3651\n",
      "Epoch [13/50], Iter [800/886] loss : 2.2927\n",
      "Average loss: 2.3028 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [14/50], Iter [100/886] loss : 2.2213\n",
      "Epoch [14/50], Iter [200/886] loss : 2.3353\n",
      "Epoch [14/50], Iter [300/886] loss : 2.2803\n",
      "Epoch [14/50], Iter [400/886] loss : 2.3466\n",
      "Epoch [14/50], Iter [500/886] loss : 2.2729\n",
      "Epoch [14/50], Iter [600/886] loss : 2.3134\n",
      "Epoch [14/50], Iter [700/886] loss : 2.3432\n",
      "Epoch [14/50], Iter [800/886] loss : 2.3287\n",
      "Average loss: 2.3049 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [15/50], Iter [100/886] loss : 2.2674\n",
      "Epoch [15/50], Iter [200/886] loss : 2.3301\n",
      "Epoch [15/50], Iter [300/886] loss : 2.3472\n",
      "Epoch [15/50], Iter [400/886] loss : 2.2638\n",
      "Epoch [15/50], Iter [500/886] loss : 2.3194\n",
      "Epoch [15/50], Iter [600/886] loss : 2.3127\n",
      "Epoch [15/50], Iter [700/886] loss : 2.2919\n",
      "Epoch [15/50], Iter [800/886] loss : 2.2352\n",
      "Average loss: 2.3042 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [16/50], Iter [100/886] loss : 2.2612\n",
      "Epoch [16/50], Iter [200/886] loss : 2.2817\n",
      "Epoch [16/50], Iter [300/886] loss : 2.3475\n",
      "Epoch [16/50], Iter [400/886] loss : 2.2828\n",
      "Epoch [16/50], Iter [500/886] loss : 2.3953\n",
      "Epoch [16/50], Iter [600/886] loss : 2.3075\n",
      "Epoch [16/50], Iter [700/886] loss : 2.3088\n",
      "Epoch [16/50], Iter [800/886] loss : 2.2994\n",
      "Average loss: 2.2982 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [17/50], Iter [100/886] loss : 2.1860\n",
      "Epoch [17/50], Iter [200/886] loss : 2.2467\n",
      "Epoch [17/50], Iter [300/886] loss : 1.9148\n",
      "Epoch [17/50], Iter [400/886] loss : 2.0674\n",
      "Epoch [17/50], Iter [500/886] loss : 1.9419\n",
      "Epoch [17/50], Iter [600/886] loss : 1.8436\n",
      "Epoch [17/50], Iter [700/886] loss : 2.9757\n",
      "Epoch [17/50], Iter [800/886] loss : 2.3798\n",
      "Average loss: 2.1495 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [18/50], Iter [100/886] loss : 1.6815\n",
      "Epoch [18/50], Iter [200/886] loss : 2.0392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/50], Iter [300/886] loss : 2.1898\n",
      "Epoch [18/50], Iter [400/886] loss : 2.1967\n",
      "Epoch [18/50], Iter [500/886] loss : 1.7498\n",
      "Epoch [18/50], Iter [600/886] loss : 2.3308\n",
      "Epoch [18/50], Iter [700/886] loss : 1.7665\n",
      "Epoch [18/50], Iter [800/886] loss : 2.4338\n",
      "Average loss: 2.0821 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [19/50], Iter [100/886] loss : 1.7053\n",
      "Epoch [19/50], Iter [200/886] loss : 1.4686\n",
      "Epoch [19/50], Iter [300/886] loss : 1.8971\n",
      "Epoch [19/50], Iter [400/886] loss : 1.9250\n",
      "Epoch [19/50], Iter [500/886] loss : 1.5691\n",
      "Epoch [19/50], Iter [600/886] loss : 2.6485\n",
      "Epoch [19/50], Iter [700/886] loss : 1.4500\n",
      "Epoch [19/50], Iter [800/886] loss : 1.7793\n",
      "Average loss: 1.8951 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [20/50], Iter [100/886] loss : 1.9865\n",
      "Epoch [20/50], Iter [200/886] loss : 1.9100\n",
      "Epoch [20/50], Iter [300/886] loss : 1.7475\n",
      "Epoch [20/50], Iter [400/886] loss : 2.0370\n",
      "Epoch [20/50], Iter [500/886] loss : 1.5239\n",
      "Epoch [20/50], Iter [600/886] loss : 2.2351\n",
      "Epoch [20/50], Iter [700/886] loss : 1.1322\n",
      "Epoch [20/50], Iter [800/886] loss : 1.3209\n",
      "Average loss: 1.9237 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [21/50], Iter [100/886] loss : 1.4617\n",
      "Epoch [21/50], Iter [200/886] loss : 2.1269\n",
      "Epoch [21/50], Iter [300/886] loss : 2.0075\n",
      "Epoch [21/50], Iter [400/886] loss : 2.1802\n",
      "Epoch [21/50], Iter [500/886] loss : 1.2839\n",
      "Epoch [21/50], Iter [600/886] loss : 2.3131\n",
      "Epoch [21/50], Iter [700/886] loss : 2.2439\n",
      "Epoch [21/50], Iter [800/886] loss : 1.4993\n",
      "Average loss: 1.8432 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [22/50], Iter [100/886] loss : 1.7622\n",
      "Epoch [22/50], Iter [200/886] loss : 1.5544\n",
      "Epoch [22/50], Iter [300/886] loss : 1.5858\n",
      "Epoch [22/50], Iter [400/886] loss : 2.2584\n",
      "Epoch [22/50], Iter [500/886] loss : 3.2394\n",
      "Epoch [22/50], Iter [600/886] loss : 1.6503\n",
      "Epoch [22/50], Iter [700/886] loss : 2.1807\n",
      "Epoch [22/50], Iter [800/886] loss : 1.4761\n",
      "Average loss: 1.8505 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [23/50], Iter [100/886] loss : 1.6946\n",
      "Epoch [23/50], Iter [200/886] loss : 1.9412\n",
      "Epoch [23/50], Iter [300/886] loss : 1.8543\n",
      "Epoch [23/50], Iter [400/886] loss : 1.9519\n",
      "Epoch [23/50], Iter [500/886] loss : 1.7629\n",
      "Epoch [23/50], Iter [600/886] loss : 1.9813\n",
      "Epoch [23/50], Iter [700/886] loss : 1.4438\n",
      "Epoch [23/50], Iter [800/886] loss : 2.1751\n",
      "Average loss: 1.7603 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [24/50], Iter [100/886] loss : 1.4912\n",
      "Epoch [24/50], Iter [200/886] loss : 2.2712\n",
      "Epoch [24/50], Iter [300/886] loss : 2.9814\n",
      "Epoch [24/50], Iter [400/886] loss : 1.5126\n",
      "Epoch [24/50], Iter [500/886] loss : 1.4327\n",
      "Epoch [24/50], Iter [600/886] loss : 1.2263\n",
      "Epoch [24/50], Iter [700/886] loss : 1.2647\n",
      "Epoch [24/50], Iter [800/886] loss : 1.6685\n",
      "Average loss: 1.8311 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [25/50], Iter [100/886] loss : 3.3197\n",
      "Epoch [25/50], Iter [200/886] loss : 2.0681\n",
      "Epoch [25/50], Iter [300/886] loss : 1.9136\n",
      "Epoch [25/50], Iter [400/886] loss : 1.7177\n",
      "Epoch [25/50], Iter [500/886] loss : 2.3294\n",
      "Epoch [25/50], Iter [600/886] loss : 1.0912\n",
      "Epoch [25/50], Iter [700/886] loss : 1.2135\n",
      "Epoch [25/50], Iter [800/886] loss : 1.3898\n",
      "Average loss: 1.7550 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [26/50], Iter [100/886] loss : 1.7490\n",
      "Epoch [26/50], Iter [200/886] loss : 1.6963\n",
      "Epoch [26/50], Iter [300/886] loss : 0.9142\n",
      "Epoch [26/50], Iter [400/886] loss : 1.5378\n",
      "Epoch [26/50], Iter [500/886] loss : 1.1416\n",
      "Epoch [26/50], Iter [600/886] loss : 1.2040\n",
      "Epoch [26/50], Iter [700/886] loss : 1.7787\n",
      "Epoch [26/50], Iter [800/886] loss : 0.8793\n",
      "Average loss: 1.7624 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [27/50], Iter [100/886] loss : 1.4089\n",
      "Epoch [27/50], Iter [200/886] loss : 0.6816\n",
      "Epoch [27/50], Iter [300/886] loss : 1.5112\n",
      "Epoch [27/50], Iter [400/886] loss : 1.3658\n",
      "Epoch [27/50], Iter [500/886] loss : 0.7987\n",
      "Epoch [27/50], Iter [600/886] loss : 1.1056\n",
      "Epoch [27/50], Iter [700/886] loss : 1.7994\n",
      "Epoch [27/50], Iter [800/886] loss : 1.6909\n",
      "Average loss: 1.7497 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [28/50], Iter [100/886] loss : 0.7509\n",
      "Epoch [28/50], Iter [200/886] loss : 1.7991\n",
      "Epoch [28/50], Iter [300/886] loss : 2.2891\n",
      "Epoch [28/50], Iter [400/886] loss : 1.7004\n",
      "Epoch [28/50], Iter [500/886] loss : 1.2882\n",
      "Epoch [28/50], Iter [600/886] loss : 2.1032\n",
      "Epoch [28/50], Iter [700/886] loss : 0.8816\n",
      "Epoch [28/50], Iter [800/886] loss : 1.5469\n",
      "Average loss: 1.7768 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [29/50], Iter [100/886] loss : 1.0790\n",
      "Epoch [29/50], Iter [200/886] loss : 1.7370\n",
      "Epoch [29/50], Iter [300/886] loss : 1.3400\n",
      "Epoch [29/50], Iter [400/886] loss : 1.7835\n",
      "Epoch [29/50], Iter [500/886] loss : 1.4642\n",
      "Epoch [29/50], Iter [600/886] loss : 2.0003\n",
      "Epoch [29/50], Iter [700/886] loss : 0.7453\n",
      "Epoch [29/50], Iter [800/886] loss : 1.7863\n",
      "Average loss: 1.7476 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [30/50], Iter [100/886] loss : 2.1316\n",
      "Epoch [30/50], Iter [200/886] loss : 1.2819\n",
      "Epoch [30/50], Iter [300/886] loss : 1.7418\n",
      "Epoch [30/50], Iter [400/886] loss : 1.2811\n",
      "Epoch [30/50], Iter [500/886] loss : 0.8639\n",
      "Epoch [30/50], Iter [600/886] loss : 1.5655\n",
      "Epoch [30/50], Iter [700/886] loss : 1.1550\n",
      "Epoch [30/50], Iter [800/886] loss : 1.7720\n",
      "Average loss: 1.7589 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [31/50], Iter [100/886] loss : 2.6076\n",
      "Epoch [31/50], Iter [200/886] loss : 2.4416\n",
      "Epoch [31/50], Iter [300/886] loss : 1.0379\n",
      "Epoch [31/50], Iter [400/886] loss : 1.3525\n",
      "Epoch [31/50], Iter [500/886] loss : 1.7053\n",
      "Epoch [31/50], Iter [600/886] loss : 0.7871\n",
      "Epoch [31/50], Iter [700/886] loss : 1.0910\n",
      "Epoch [31/50], Iter [800/886] loss : 1.0894\n",
      "Average loss: 1.8463 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [32/50], Iter [100/886] loss : 1.7550\n",
      "Epoch [32/50], Iter [200/886] loss : 1.1298\n",
      "Epoch [32/50], Iter [300/886] loss : 0.7683\n",
      "Epoch [32/50], Iter [400/886] loss : 0.7489\n",
      "Epoch [32/50], Iter [500/886] loss : 1.2321\n",
      "Epoch [32/50], Iter [600/886] loss : 1.5815\n",
      "Epoch [32/50], Iter [700/886] loss : 1.6529\n",
      "Epoch [32/50], Iter [800/886] loss : 1.0297\n",
      "Average loss: 1.8768 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [33/50], Iter [100/886] loss : 1.3800\n",
      "Epoch [33/50], Iter [200/886] loss : 1.2493\n",
      "Epoch [33/50], Iter [300/886] loss : 0.6243\n",
      "Epoch [33/50], Iter [400/886] loss : 2.2320\n",
      "Epoch [33/50], Iter [500/886] loss : 1.8632\n",
      "Epoch [33/50], Iter [600/886] loss : 1.7380\n",
      "Epoch [33/50], Iter [700/886] loss : 1.0952\n",
      "Epoch [33/50], Iter [800/886] loss : 1.1914\n",
      "Average loss: 1.7321 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [34/50], Iter [100/886] loss : 0.2967\n",
      "Epoch [34/50], Iter [200/886] loss : 1.8393\n",
      "Epoch [34/50], Iter [300/886] loss : 1.6599\n",
      "Epoch [34/50], Iter [400/886] loss : 2.3050\n",
      "Epoch [34/50], Iter [500/886] loss : 0.9633\n",
      "Epoch [34/50], Iter [600/886] loss : 1.4725\n",
      "Epoch [34/50], Iter [700/886] loss : 1.8884\n",
      "Epoch [34/50], Iter [800/886] loss : 1.8299\n",
      "Average loss: 1.7143 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [35/50], Iter [100/886] loss : 1.0589\n",
      "Epoch [35/50], Iter [200/886] loss : 0.9795\n",
      "Epoch [35/50], Iter [300/886] loss : 1.2820\n",
      "Epoch [35/50], Iter [400/886] loss : 1.1234\n",
      "Epoch [35/50], Iter [500/886] loss : 0.9808\n",
      "Epoch [35/50], Iter [600/886] loss : 2.2760\n",
      "Epoch [35/50], Iter [700/886] loss : 1.4181\n",
      "Epoch [35/50], Iter [800/886] loss : 1.1288\n",
      "Average loss: 1.7107 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [36/50], Iter [100/886] loss : 1.4125\n",
      "Epoch [36/50], Iter [200/886] loss : 0.7190\n",
      "Epoch [36/50], Iter [300/886] loss : 1.2392\n",
      "Epoch [36/50], Iter [400/886] loss : 0.7788\n",
      "Epoch [36/50], Iter [500/886] loss : 1.0060\n",
      "Epoch [36/50], Iter [600/886] loss : 0.8279\n",
      "Epoch [36/50], Iter [700/886] loss : 0.7856\n",
      "Epoch [36/50], Iter [800/886] loss : 1.4795\n",
      "Average loss: 1.6884 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [37/50], Iter [100/886] loss : 1.1570\n",
      "Epoch [37/50], Iter [200/886] loss : 1.0457\n",
      "Epoch [37/50], Iter [300/886] loss : 1.3149\n",
      "Epoch [37/50], Iter [400/886] loss : 1.8210\n",
      "Epoch [37/50], Iter [500/886] loss : 0.9755\n",
      "Epoch [37/50], Iter [600/886] loss : 0.8854\n",
      "Epoch [37/50], Iter [700/886] loss : 1.1023\n",
      "Epoch [37/50], Iter [800/886] loss : 0.7965\n",
      "Average loss: 1.7690 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [38/50], Iter [100/886] loss : 0.7764\n",
      "Epoch [38/50], Iter [200/886] loss : 1.1031\n",
      "Epoch [38/50], Iter [300/886] loss : 1.7436\n",
      "Epoch [38/50], Iter [400/886] loss : 1.4751\n",
      "Epoch [38/50], Iter [500/886] loss : 0.9152\n",
      "Epoch [38/50], Iter [600/886] loss : 1.2893\n",
      "Epoch [38/50], Iter [700/886] loss : 0.9275\n",
      "Epoch [38/50], Iter [800/886] loss : 1.0782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 1.7877 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [39/50], Iter [100/886] loss : 1.0319\n",
      "Epoch [39/50], Iter [200/886] loss : 1.3849\n",
      "Epoch [39/50], Iter [300/886] loss : 0.8223\n",
      "Epoch [39/50], Iter [400/886] loss : 1.1526\n",
      "Epoch [39/50], Iter [500/886] loss : 2.4083\n",
      "Epoch [39/50], Iter [600/886] loss : 0.8474\n",
      "Epoch [39/50], Iter [700/886] loss : 1.6017\n",
      "Epoch [39/50], Iter [800/886] loss : 0.6166\n",
      "Average loss: 1.6487 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [40/50], Iter [100/886] loss : 1.1062\n",
      "Epoch [40/50], Iter [200/886] loss : 1.5392\n",
      "Epoch [40/50], Iter [300/886] loss : 1.9828\n",
      "Epoch [40/50], Iter [400/886] loss : 1.1377\n",
      "Epoch [40/50], Iter [500/886] loss : 1.7188\n",
      "Epoch [40/50], Iter [600/886] loss : 0.4043\n",
      "Epoch [40/50], Iter [700/886] loss : 1.9393\n",
      "Epoch [40/50], Iter [800/886] loss : 0.5384\n",
      "Average loss: 1.7724 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [41/50], Iter [100/886] loss : 1.4672\n",
      "Epoch [41/50], Iter [200/886] loss : 1.8176\n",
      "Epoch [41/50], Iter [300/886] loss : 1.3714\n",
      "Epoch [41/50], Iter [400/886] loss : 0.3334\n",
      "Epoch [41/50], Iter [500/886] loss : 0.6108\n",
      "Epoch [41/50], Iter [600/886] loss : 0.4699\n",
      "Epoch [41/50], Iter [700/886] loss : 0.8791\n",
      "Epoch [41/50], Iter [800/886] loss : 1.0804\n",
      "Average loss: 1.8487 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [42/50], Iter [100/886] loss : 0.6620\n",
      "Epoch [42/50], Iter [200/886] loss : 0.6160\n",
      "Epoch [42/50], Iter [300/886] loss : 1.5865\n",
      "Epoch [42/50], Iter [400/886] loss : 1.3265\n",
      "Epoch [42/50], Iter [500/886] loss : 1.1440\n",
      "Epoch [42/50], Iter [600/886] loss : 0.8439\n",
      "Epoch [42/50], Iter [700/886] loss : 1.3385\n",
      "Epoch [42/50], Iter [800/886] loss : 0.1342\n",
      "Average loss: 1.7118 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [43/50], Iter [100/886] loss : 1.8495\n",
      "Epoch [43/50], Iter [200/886] loss : 2.0488\n",
      "Epoch [43/50], Iter [300/886] loss : 1.8638\n",
      "Epoch [43/50], Iter [400/886] loss : 0.3877\n",
      "Epoch [43/50], Iter [500/886] loss : 1.2204\n",
      "Epoch [43/50], Iter [600/886] loss : 2.0567\n",
      "Epoch [43/50], Iter [700/886] loss : 0.2998\n",
      "Epoch [43/50], Iter [800/886] loss : 1.3518\n",
      "Average loss: 1.7378 \n",
      "\n",
      "Epoch    42: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Learning rate : 0.0008\n",
      "Epoch [44/50], Iter [100/886] loss : 0.4703\n",
      "Epoch [44/50], Iter [200/886] loss : 1.7373\n",
      "Epoch [44/50], Iter [300/886] loss : 0.6023\n",
      "Epoch [44/50], Iter [400/886] loss : 0.7141\n",
      "Epoch [44/50], Iter [500/886] loss : 0.4295\n",
      "Epoch [44/50], Iter [600/886] loss : 0.4918\n",
      "Epoch [44/50], Iter [700/886] loss : 0.9654\n",
      "Epoch [44/50], Iter [800/886] loss : 0.3484\n",
      "Average loss: 1.7985 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [45/50], Iter [100/886] loss : 1.2180\n",
      "Epoch [45/50], Iter [200/886] loss : 0.4510\n",
      "Epoch [45/50], Iter [300/886] loss : 0.5298\n",
      "Epoch [45/50], Iter [400/886] loss : 0.4241\n",
      "Epoch [45/50], Iter [500/886] loss : 0.7269\n",
      "Epoch [45/50], Iter [600/886] loss : 0.5126\n",
      "Epoch [45/50], Iter [700/886] loss : 0.2763\n",
      "Epoch [45/50], Iter [800/886] loss : 0.8877\n",
      "Average loss: 1.8329 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [46/50], Iter [100/886] loss : 0.5870\n",
      "Epoch [46/50], Iter [200/886] loss : 0.5578\n",
      "Epoch [46/50], Iter [300/886] loss : 1.2386\n",
      "Epoch [46/50], Iter [400/886] loss : 1.1804\n",
      "Epoch [46/50], Iter [500/886] loss : 1.0335\n",
      "Epoch [46/50], Iter [600/886] loss : 0.6371\n",
      "Epoch [46/50], Iter [700/886] loss : 2.0036\n",
      "Epoch [46/50], Iter [800/886] loss : 0.3675\n",
      "Average loss: 1.7995 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [47/50], Iter [100/886] loss : 0.9085\n",
      "Epoch [47/50], Iter [200/886] loss : 0.4719\n",
      "Epoch [47/50], Iter [300/886] loss : 0.3733\n",
      "Epoch [47/50], Iter [400/886] loss : 0.5468\n",
      "Epoch [47/50], Iter [500/886] loss : 1.0887\n",
      "Epoch [47/50], Iter [600/886] loss : 0.4206\n",
      "Epoch [47/50], Iter [700/886] loss : 0.3454\n",
      "Epoch [47/50], Iter [800/886] loss : 1.7001\n",
      "Average loss: 1.8680 \n",
      "\n",
      "Epoch    46: reducing learning rate of group 0 to 1.6000e-04.\n",
      "Learning rate : 0.00016\n",
      "Epoch [48/50], Iter [100/886] loss : 0.2853\n",
      "Epoch [48/50], Iter [200/886] loss : 1.2457\n",
      "Epoch [48/50], Iter [300/886] loss : 0.6562\n",
      "Epoch [48/50], Iter [400/886] loss : 0.6072\n",
      "Epoch [48/50], Iter [500/886] loss : 0.5819\n",
      "Epoch [48/50], Iter [600/886] loss : 0.3691\n",
      "Epoch [48/50], Iter [700/886] loss : 0.3878\n",
      "Epoch [48/50], Iter [800/886] loss : 0.5966\n",
      "Average loss: 1.8683 \n",
      "\n",
      "Learning rate : 0.00016\n",
      "Epoch [49/50], Iter [100/886] loss : 0.2649\n",
      "Epoch [49/50], Iter [200/886] loss : 0.3481\n",
      "Epoch [49/50], Iter [300/886] loss : 1.3808\n",
      "Epoch [49/50], Iter [400/886] loss : 1.2781\n",
      "Epoch [49/50], Iter [500/886] loss : 0.6671\n",
      "Epoch [49/50], Iter [600/886] loss : 0.3209\n",
      "Epoch [49/50], Iter [700/886] loss : 0.5997\n",
      "Epoch [49/50], Iter [800/886] loss : 1.4479\n",
      "Average loss: 1.8673 \n",
      "\n",
      "Learning rate : 0.00016\n",
      "Epoch [50/50], Iter [100/886] loss : 0.7735\n",
      "Epoch [50/50], Iter [200/886] loss : 1.4797\n",
      "Epoch [50/50], Iter [300/886] loss : 0.6411\n",
      "Epoch [50/50], Iter [400/886] loss : 1.2340\n",
      "Epoch [50/50], Iter [500/886] loss : 1.4076\n",
      "Epoch [50/50], Iter [600/886] loss : 1.3258\n",
      "Epoch [50/50], Iter [700/886] loss : 0.2984\n",
      "Epoch [50/50], Iter [800/886] loss : 0.7539\n",
      "Average loss: 1.9434 \n",
      "\n",
      "Learning rate : 0.00016\n",
      "WD:   1e-05  DO:   0.5  LR:   0.1  -> VAL_loss: tensor(1.9434, device='cuda:0')\n",
      "Epoch [1/50], Iter [100/886] loss : 1.9879\n",
      "Epoch [1/50], Iter [200/886] loss : 2.9935\n",
      "Epoch [1/50], Iter [300/886] loss : 2.5693\n",
      "Epoch [1/50], Iter [400/886] loss : 3.0953\n",
      "Epoch [1/50], Iter [500/886] loss : 2.5253\n",
      "Epoch [1/50], Iter [600/886] loss : 3.4875\n",
      "Epoch [1/50], Iter [700/886] loss : 3.1549\n",
      "Epoch [1/50], Iter [800/886] loss : 3.3652\n",
      "Average loss: 2.4350 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [2/50], Iter [100/886] loss : 3.8258\n",
      "Epoch [2/50], Iter [200/886] loss : 3.1576\n",
      "Epoch [2/50], Iter [300/886] loss : 2.3037\n",
      "Epoch [2/50], Iter [400/886] loss : 2.8612\n",
      "Epoch [2/50], Iter [500/886] loss : 2.3001\n",
      "Epoch [2/50], Iter [600/886] loss : 2.7463\n",
      "Epoch [2/50], Iter [700/886] loss : 2.5081\n",
      "Epoch [2/50], Iter [800/886] loss : 2.2961\n",
      "Average loss: 2.6448 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [3/50], Iter [100/886] loss : 2.0812\n",
      "Epoch [3/50], Iter [200/886] loss : 2.7595\n",
      "Epoch [3/50], Iter [300/886] loss : 3.6947\n",
      "Epoch [3/50], Iter [400/886] loss : 2.5128\n",
      "Epoch [3/50], Iter [500/886] loss : 2.6388\n",
      "Epoch [3/50], Iter [600/886] loss : 2.3841\n",
      "Epoch [3/50], Iter [700/886] loss : 3.3764\n",
      "Epoch [3/50], Iter [800/886] loss : 3.6872\n",
      "Average loss: 2.6553 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [4/50], Iter [100/886] loss : 3.0456\n",
      "Epoch [4/50], Iter [200/886] loss : 2.2959\n",
      "Epoch [4/50], Iter [300/886] loss : 2.7716\n",
      "Epoch [4/50], Iter [400/886] loss : 3.0384\n",
      "Epoch [4/50], Iter [500/886] loss : 3.1239\n",
      "Epoch [4/50], Iter [600/886] loss : 3.4573\n",
      "Epoch [4/50], Iter [700/886] loss : 3.1392\n",
      "Epoch [4/50], Iter [800/886] loss : 2.6664\n",
      "Average loss: 2.6804 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [5/50], Iter [100/886] loss : 2.9083\n",
      "Epoch [5/50], Iter [200/886] loss : 4.2920\n",
      "Epoch [5/50], Iter [300/886] loss : 3.0593\n",
      "Epoch [5/50], Iter [400/886] loss : 2.5049\n",
      "Epoch [5/50], Iter [500/886] loss : 3.5050\n",
      "Epoch [5/50], Iter [600/886] loss : 3.0841\n",
      "Epoch [5/50], Iter [700/886] loss : 2.4505\n",
      "Epoch [5/50], Iter [800/886] loss : 2.6448\n",
      "Average loss: 2.6395 \n",
      "\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-02.\n",
      "Learning rate : 0.02\n",
      "Epoch [6/50], Iter [100/886] loss : 2.3144\n",
      "Epoch [6/50], Iter [200/886] loss : 2.5443\n",
      "Epoch [6/50], Iter [300/886] loss : 2.6695\n",
      "Epoch [6/50], Iter [400/886] loss : 2.6698\n",
      "Epoch [6/50], Iter [500/886] loss : 2.4751\n",
      "Epoch [6/50], Iter [600/886] loss : 2.1847\n",
      "Epoch [6/50], Iter [700/886] loss : 2.1385\n",
      "Epoch [6/50], Iter [800/886] loss : 2.1792\n",
      "Average loss: 2.3232 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [7/50], Iter [100/886] loss : 2.1980\n",
      "Epoch [7/50], Iter [200/886] loss : 2.1827\n",
      "Epoch [7/50], Iter [300/886] loss : 2.2969\n",
      "Epoch [7/50], Iter [400/886] loss : 2.3392\n",
      "Epoch [7/50], Iter [500/886] loss : 2.5171\n",
      "Epoch [7/50], Iter [600/886] loss : 2.2212\n",
      "Epoch [7/50], Iter [700/886] loss : 2.4978\n",
      "Epoch [7/50], Iter [800/886] loss : 2.2788\n",
      "Average loss: 2.3155 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [8/50], Iter [100/886] loss : 2.2899\n",
      "Epoch [8/50], Iter [200/886] loss : 2.1643\n",
      "Epoch [8/50], Iter [300/886] loss : 2.3563\n",
      "Epoch [8/50], Iter [400/886] loss : 2.3245\n",
      "Epoch [8/50], Iter [500/886] loss : 2.3191\n",
      "Epoch [8/50], Iter [600/886] loss : 2.5029\n",
      "Epoch [8/50], Iter [700/886] loss : 2.4509\n",
      "Epoch [8/50], Iter [800/886] loss : 2.2427\n",
      "Average loss: 2.3192 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [9/50], Iter [100/886] loss : 2.3351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Iter [200/886] loss : 2.1726\n",
      "Epoch [9/50], Iter [300/886] loss : 2.4751\n",
      "Epoch [9/50], Iter [400/886] loss : 2.4032\n",
      "Epoch [9/50], Iter [500/886] loss : 2.3761\n",
      "Epoch [9/50], Iter [600/886] loss : 2.2711\n",
      "Epoch [9/50], Iter [700/886] loss : 2.2401\n",
      "Epoch [9/50], Iter [800/886] loss : 2.3106\n",
      "Average loss: 2.3240 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [10/50], Iter [100/886] loss : 2.4203\n",
      "Epoch [10/50], Iter [200/886] loss : 2.3198\n",
      "Epoch [10/50], Iter [300/886] loss : 2.3737\n",
      "Epoch [10/50], Iter [400/886] loss : 2.4897\n",
      "Epoch [10/50], Iter [500/886] loss : 2.0405\n",
      "Epoch [10/50], Iter [600/886] loss : 2.3765\n",
      "Epoch [10/50], Iter [700/886] loss : 2.1169\n",
      "Epoch [10/50], Iter [800/886] loss : 2.4281\n",
      "Average loss: 2.3538 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [11/50], Iter [100/886] loss : 2.3227\n",
      "Epoch [11/50], Iter [200/886] loss : 2.3183\n",
      "Epoch [11/50], Iter [300/886] loss : 2.2821\n",
      "Epoch [11/50], Iter [400/886] loss : 2.2715\n",
      "Epoch [11/50], Iter [500/886] loss : 2.5102\n",
      "Epoch [11/50], Iter [600/886] loss : 2.1102\n",
      "Epoch [11/50], Iter [700/886] loss : 2.1828\n",
      "Epoch [11/50], Iter [800/886] loss : 2.5752\n",
      "Average loss: 2.3382 \n",
      "\n",
      "Epoch    10: reducing learning rate of group 0 to 4.0000e-03.\n",
      "Learning rate : 0.004\n",
      "Epoch [12/50], Iter [100/886] loss : 2.2664\n",
      "Epoch [12/50], Iter [200/886] loss : 2.3966\n",
      "Epoch [12/50], Iter [300/886] loss : 2.2936\n",
      "Epoch [12/50], Iter [400/886] loss : 2.3342\n",
      "Epoch [12/50], Iter [500/886] loss : 2.4395\n",
      "Epoch [12/50], Iter [600/886] loss : 2.3490\n",
      "Epoch [12/50], Iter [700/886] loss : 2.2637\n",
      "Epoch [12/50], Iter [800/886] loss : 2.2594\n",
      "Average loss: 2.3027 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [13/50], Iter [100/886] loss : 2.2554\n",
      "Epoch [13/50], Iter [200/886] loss : 2.3947\n",
      "Epoch [13/50], Iter [300/886] loss : 2.3537\n",
      "Epoch [13/50], Iter [400/886] loss : 2.2760\n",
      "Epoch [13/50], Iter [500/886] loss : 2.2727\n",
      "Epoch [13/50], Iter [600/886] loss : 2.2875\n",
      "Epoch [13/50], Iter [700/886] loss : 2.3586\n",
      "Epoch [13/50], Iter [800/886] loss : 2.3166\n",
      "Average loss: 2.3032 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [14/50], Iter [100/886] loss : 2.4136\n",
      "Epoch [14/50], Iter [200/886] loss : 2.2314\n",
      "Epoch [14/50], Iter [300/886] loss : 2.2790\n",
      "Epoch [14/50], Iter [400/886] loss : 2.3241\n",
      "Epoch [14/50], Iter [500/886] loss : 2.2922\n",
      "Epoch [14/50], Iter [600/886] loss : 2.3212\n",
      "Epoch [14/50], Iter [700/886] loss : 2.3960\n",
      "Epoch [14/50], Iter [800/886] loss : 2.2695\n",
      "Average loss: 2.3064 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [15/50], Iter [100/886] loss : 2.3020\n",
      "Epoch [15/50], Iter [200/886] loss : 2.2904\n",
      "Epoch [15/50], Iter [300/886] loss : 2.2855\n",
      "Epoch [15/50], Iter [400/886] loss : 2.3523\n",
      "Epoch [15/50], Iter [500/886] loss : 2.2930\n",
      "Epoch [15/50], Iter [600/886] loss : 2.3894\n",
      "Epoch [15/50], Iter [700/886] loss : 2.3084\n",
      "Epoch [15/50], Iter [800/886] loss : 2.3389\n",
      "Average loss: 2.3029 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [16/50], Iter [100/886] loss : 2.3427\n",
      "Epoch [16/50], Iter [200/886] loss : 2.2796\n",
      "Epoch [16/50], Iter [300/886] loss : 2.2328\n",
      "Epoch [16/50], Iter [400/886] loss : 2.3991\n",
      "Epoch [16/50], Iter [500/886] loss : 2.3050\n",
      "Epoch [16/50], Iter [600/886] loss : 2.2719\n",
      "Epoch [16/50], Iter [700/886] loss : 2.3297\n",
      "Epoch [16/50], Iter [800/886] loss : 2.4310\n",
      "Average loss: 2.3019 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [17/50], Iter [100/886] loss : 2.3815\n",
      "Epoch [17/50], Iter [200/886] loss : 2.2711\n",
      "Epoch [17/50], Iter [300/886] loss : 2.2717\n",
      "Epoch [17/50], Iter [400/886] loss : 2.3048\n",
      "Epoch [17/50], Iter [500/886] loss : 2.2110\n",
      "Epoch [17/50], Iter [600/886] loss : 2.2942\n",
      "Epoch [17/50], Iter [700/886] loss : 2.3474\n",
      "Epoch [17/50], Iter [800/886] loss : 2.4398\n",
      "Average loss: 2.3072 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [18/50], Iter [100/886] loss : 2.3697\n",
      "Epoch [18/50], Iter [200/886] loss : 2.3334\n",
      "Epoch [18/50], Iter [300/886] loss : 2.4281\n",
      "Epoch [18/50], Iter [400/886] loss : 2.3122\n",
      "Epoch [18/50], Iter [500/886] loss : 2.2997\n",
      "Epoch [18/50], Iter [600/886] loss : 2.2740\n",
      "Epoch [18/50], Iter [700/886] loss : 2.3528\n",
      "Epoch [18/50], Iter [800/886] loss : 2.4236\n",
      "Average loss: 2.3050 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [19/50], Iter [100/886] loss : 2.3614\n",
      "Epoch [19/50], Iter [200/886] loss : 2.3099\n",
      "Epoch [19/50], Iter [300/886] loss : 2.3401\n",
      "Epoch [19/50], Iter [400/886] loss : 2.2819\n",
      "Epoch [19/50], Iter [500/886] loss : 2.2119\n",
      "Epoch [19/50], Iter [600/886] loss : 2.6181\n",
      "Epoch [19/50], Iter [700/886] loss : 2.2008\n",
      "Epoch [19/50], Iter [800/886] loss : 2.0942\n",
      "Average loss: 2.3990 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [20/50], Iter [100/886] loss : 2.0163\n",
      "Epoch [20/50], Iter [200/886] loss : 2.3121\n",
      "Epoch [20/50], Iter [300/886] loss : 2.1855\n",
      "Epoch [20/50], Iter [400/886] loss : 2.3154\n",
      "Epoch [20/50], Iter [500/886] loss : 2.0715\n",
      "Epoch [20/50], Iter [600/886] loss : 1.9696\n",
      "Epoch [20/50], Iter [700/886] loss : 2.0748\n",
      "Epoch [20/50], Iter [800/886] loss : 1.9346\n",
      "Average loss: 2.1701 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [21/50], Iter [100/886] loss : 1.8728\n",
      "Epoch [21/50], Iter [200/886] loss : 3.0836\n",
      "Epoch [21/50], Iter [300/886] loss : 2.4904\n",
      "Epoch [21/50], Iter [400/886] loss : 2.3987\n",
      "Epoch [21/50], Iter [500/886] loss : 2.0389\n",
      "Epoch [21/50], Iter [600/886] loss : 2.1298\n",
      "Epoch [21/50], Iter [700/886] loss : 2.4607\n",
      "Epoch [21/50], Iter [800/886] loss : 2.1166\n",
      "Average loss: 2.1729 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [22/50], Iter [100/886] loss : 2.1507\n",
      "Epoch [22/50], Iter [200/886] loss : 2.3510\n",
      "Epoch [22/50], Iter [300/886] loss : 2.0688\n",
      "Epoch [22/50], Iter [400/886] loss : 1.9739\n",
      "Epoch [22/50], Iter [500/886] loss : 2.0199\n",
      "Epoch [22/50], Iter [600/886] loss : 1.8291\n",
      "Epoch [22/50], Iter [700/886] loss : 1.8103\n",
      "Epoch [22/50], Iter [800/886] loss : 2.0577\n",
      "Average loss: 2.0926 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [23/50], Iter [100/886] loss : 1.8172\n",
      "Epoch [23/50], Iter [200/886] loss : 1.6454\n",
      "Epoch [23/50], Iter [300/886] loss : 1.7740\n",
      "Epoch [23/50], Iter [400/886] loss : 2.1549\n",
      "Epoch [23/50], Iter [500/886] loss : 1.6150\n",
      "Epoch [23/50], Iter [600/886] loss : 2.6022\n",
      "Epoch [23/50], Iter [700/886] loss : 2.4159\n",
      "Epoch [23/50], Iter [800/886] loss : 2.0468\n",
      "Average loss: 2.0263 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [24/50], Iter [100/886] loss : 1.5000\n",
      "Epoch [24/50], Iter [200/886] loss : 2.2294\n",
      "Epoch [24/50], Iter [300/886] loss : 2.1629\n",
      "Epoch [24/50], Iter [400/886] loss : 1.6652\n",
      "Epoch [24/50], Iter [500/886] loss : 1.8399\n",
      "Epoch [24/50], Iter [600/886] loss : 1.8835\n",
      "Epoch [24/50], Iter [700/886] loss : 1.4462\n",
      "Epoch [24/50], Iter [800/886] loss : 2.3154\n",
      "Average loss: 1.9287 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [25/50], Iter [100/886] loss : 1.4556\n",
      "Epoch [25/50], Iter [200/886] loss : 2.0580\n",
      "Epoch [25/50], Iter [300/886] loss : 2.4143\n",
      "Epoch [25/50], Iter [400/886] loss : 1.7289\n",
      "Epoch [25/50], Iter [500/886] loss : 1.8870\n",
      "Epoch [25/50], Iter [600/886] loss : 1.3691\n",
      "Epoch [25/50], Iter [700/886] loss : 1.9631\n",
      "Epoch [25/50], Iter [800/886] loss : 2.3886\n",
      "Average loss: 2.0005 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [26/50], Iter [100/886] loss : 2.0928\n",
      "Epoch [26/50], Iter [200/886] loss : 1.5521\n",
      "Epoch [26/50], Iter [300/886] loss : 1.6628\n",
      "Epoch [26/50], Iter [400/886] loss : 1.5084\n",
      "Epoch [26/50], Iter [500/886] loss : 1.6047\n",
      "Epoch [26/50], Iter [600/886] loss : 1.2557\n",
      "Epoch [26/50], Iter [700/886] loss : 2.0155\n",
      "Epoch [26/50], Iter [800/886] loss : 1.3701\n",
      "Average loss: 1.8408 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [27/50], Iter [100/886] loss : 1.3716\n",
      "Epoch [27/50], Iter [200/886] loss : 1.6932\n",
      "Epoch [27/50], Iter [300/886] loss : 0.8595\n",
      "Epoch [27/50], Iter [400/886] loss : 1.6439\n",
      "Epoch [27/50], Iter [500/886] loss : 2.2173\n",
      "Epoch [27/50], Iter [600/886] loss : 1.9783\n",
      "Epoch [27/50], Iter [700/886] loss : 1.5760\n",
      "Epoch [27/50], Iter [800/886] loss : 1.6358\n",
      "Average loss: 1.8264 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [28/50], Iter [100/886] loss : 1.1711\n",
      "Epoch [28/50], Iter [200/886] loss : 1.6899\n",
      "Epoch [28/50], Iter [300/886] loss : 1.7150\n",
      "Epoch [28/50], Iter [400/886] loss : 0.8881\n",
      "Epoch [28/50], Iter [500/886] loss : 2.0108\n",
      "Epoch [28/50], Iter [600/886] loss : 1.2093\n",
      "Epoch [28/50], Iter [700/886] loss : 2.4524\n",
      "Epoch [28/50], Iter [800/886] loss : 2.2314\n",
      "Average loss: 1.7736 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [29/50], Iter [100/886] loss : 2.0409\n",
      "Epoch [29/50], Iter [200/886] loss : 1.4380\n",
      "Epoch [29/50], Iter [300/886] loss : 1.3494\n",
      "Epoch [29/50], Iter [400/886] loss : 1.1276\n",
      "Epoch [29/50], Iter [500/886] loss : 2.0535\n",
      "Epoch [29/50], Iter [600/886] loss : 1.6771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/50], Iter [700/886] loss : 1.3588\n",
      "Epoch [29/50], Iter [800/886] loss : 1.3134\n",
      "Average loss: 1.7810 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [30/50], Iter [100/886] loss : 1.1369\n",
      "Epoch [30/50], Iter [200/886] loss : 1.8040\n",
      "Epoch [30/50], Iter [300/886] loss : 1.2591\n",
      "Epoch [30/50], Iter [400/886] loss : 0.9351\n",
      "Epoch [30/50], Iter [500/886] loss : 0.9006\n",
      "Epoch [30/50], Iter [600/886] loss : 1.2802\n",
      "Epoch [30/50], Iter [700/886] loss : 1.5140\n",
      "Epoch [30/50], Iter [800/886] loss : 2.2927\n",
      "Average loss: 1.8519 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [31/50], Iter [100/886] loss : 1.2444\n",
      "Epoch [31/50], Iter [200/886] loss : 2.1414\n",
      "Epoch [31/50], Iter [300/886] loss : 1.1790\n",
      "Epoch [31/50], Iter [400/886] loss : 1.3648\n",
      "Epoch [31/50], Iter [500/886] loss : 2.2029\n",
      "Epoch [31/50], Iter [600/886] loss : 2.0385\n",
      "Epoch [31/50], Iter [700/886] loss : 0.8900\n",
      "Epoch [31/50], Iter [800/886] loss : 1.3596\n",
      "Average loss: 1.8228 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [32/50], Iter [100/886] loss : 1.0839\n",
      "Epoch [32/50], Iter [200/886] loss : 0.8073\n",
      "Epoch [32/50], Iter [300/886] loss : 1.9820\n",
      "Epoch [32/50], Iter [400/886] loss : 0.9660\n",
      "Epoch [32/50], Iter [500/886] loss : 1.6094\n",
      "Epoch [32/50], Iter [600/886] loss : 1.5993\n",
      "Epoch [32/50], Iter [700/886] loss : 1.1825\n",
      "Epoch [32/50], Iter [800/886] loss : 1.4101\n",
      "Average loss: 1.8116 \n",
      "\n",
      "Epoch    31: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Learning rate : 0.0008\n",
      "Epoch [33/50], Iter [100/886] loss : 1.4745\n",
      "Epoch [33/50], Iter [200/886] loss : 1.1144\n",
      "Epoch [33/50], Iter [300/886] loss : 1.6397\n",
      "Epoch [33/50], Iter [400/886] loss : 1.0807\n",
      "Epoch [33/50], Iter [500/886] loss : 1.8084\n",
      "Epoch [33/50], Iter [600/886] loss : 1.1099\n",
      "Epoch [33/50], Iter [700/886] loss : 1.3731\n",
      "Epoch [33/50], Iter [800/886] loss : 0.8488\n",
      "Average loss: 1.7630 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [34/50], Iter [100/886] loss : 0.8754\n",
      "Epoch [34/50], Iter [200/886] loss : 1.3775\n",
      "Epoch [34/50], Iter [300/886] loss : 0.9225\n",
      "Epoch [34/50], Iter [400/886] loss : 1.1899\n",
      "Epoch [34/50], Iter [500/886] loss : 1.7817\n",
      "Epoch [34/50], Iter [600/886] loss : 1.9724\n",
      "Epoch [34/50], Iter [700/886] loss : 1.2533\n",
      "Epoch [34/50], Iter [800/886] loss : 0.9717\n",
      "Average loss: 1.7761 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [35/50], Iter [100/886] loss : 1.3172\n",
      "Epoch [35/50], Iter [200/886] loss : 0.7439\n",
      "Epoch [35/50], Iter [300/886] loss : 1.5376\n",
      "Epoch [35/50], Iter [400/886] loss : 1.5355\n",
      "Epoch [35/50], Iter [500/886] loss : 1.0673\n",
      "Epoch [35/50], Iter [600/886] loss : 1.2201\n",
      "Epoch [35/50], Iter [700/886] loss : 1.2511\n",
      "Epoch [35/50], Iter [800/886] loss : 1.0603\n",
      "Average loss: 1.7590 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [36/50], Iter [100/886] loss : 0.8387\n",
      "Epoch [36/50], Iter [200/886] loss : 1.1196\n",
      "Epoch [36/50], Iter [300/886] loss : 1.7635\n",
      "Epoch [36/50], Iter [400/886] loss : 1.7510\n",
      "Epoch [36/50], Iter [500/886] loss : 0.9857\n",
      "Epoch [36/50], Iter [600/886] loss : 1.2870\n",
      "Epoch [36/50], Iter [700/886] loss : 0.9675\n",
      "Epoch [36/50], Iter [800/886] loss : 1.2863\n",
      "Average loss: 1.8373 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [37/50], Iter [100/886] loss : 1.8912\n",
      "Epoch [37/50], Iter [200/886] loss : 2.0821\n",
      "Epoch [37/50], Iter [300/886] loss : 0.9057\n",
      "Epoch [37/50], Iter [400/886] loss : 1.3450\n",
      "Epoch [37/50], Iter [500/886] loss : 1.8362\n",
      "Epoch [37/50], Iter [600/886] loss : 0.8123\n",
      "Epoch [37/50], Iter [700/886] loss : 0.8329\n",
      "Epoch [37/50], Iter [800/886] loss : 1.3282\n",
      "Average loss: 1.7546 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [38/50], Iter [100/886] loss : 1.1928\n",
      "Epoch [38/50], Iter [200/886] loss : 0.4010\n",
      "Epoch [38/50], Iter [300/886] loss : 1.0643\n",
      "Epoch [38/50], Iter [400/886] loss : 1.5285\n",
      "Epoch [38/50], Iter [500/886] loss : 2.2411\n",
      "Epoch [38/50], Iter [600/886] loss : 1.5276\n",
      "Epoch [38/50], Iter [700/886] loss : 0.6023\n",
      "Epoch [38/50], Iter [800/886] loss : 0.7724\n",
      "Average loss: 1.8276 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [39/50], Iter [100/886] loss : 1.1945\n",
      "Epoch [39/50], Iter [200/886] loss : 0.8793\n",
      "Epoch [39/50], Iter [300/886] loss : 0.8958\n",
      "Epoch [39/50], Iter [400/886] loss : 0.9885\n",
      "Epoch [39/50], Iter [500/886] loss : 1.5859\n",
      "Epoch [39/50], Iter [600/886] loss : 1.3409\n",
      "Epoch [39/50], Iter [700/886] loss : 1.3062\n",
      "Epoch [39/50], Iter [800/886] loss : 1.2024\n",
      "Average loss: 1.8307 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [40/50], Iter [100/886] loss : 0.7247\n",
      "Epoch [40/50], Iter [200/886] loss : 0.7111\n",
      "Epoch [40/50], Iter [300/886] loss : 1.4338\n",
      "Epoch [40/50], Iter [400/886] loss : 1.6093\n",
      "Epoch [40/50], Iter [500/886] loss : 1.3868\n",
      "Epoch [40/50], Iter [600/886] loss : 0.8368\n",
      "Epoch [40/50], Iter [700/886] loss : 1.1520\n",
      "Epoch [40/50], Iter [800/886] loss : 1.2637\n",
      "Average loss: 1.7879 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [41/50], Iter [100/886] loss : 0.7206\n",
      "Epoch [41/50], Iter [200/886] loss : 1.5471\n",
      "Epoch [41/50], Iter [300/886] loss : 1.1517\n",
      "Epoch [41/50], Iter [400/886] loss : 1.0455\n",
      "Epoch [41/50], Iter [500/886] loss : 1.7114\n",
      "Epoch [41/50], Iter [600/886] loss : 1.9460\n",
      "Epoch [41/50], Iter [700/886] loss : 0.6506\n",
      "Epoch [41/50], Iter [800/886] loss : 0.5295\n",
      "Average loss: 1.7940 \n",
      "\n",
      "Epoch    40: reducing learning rate of group 0 to 1.6000e-04.\n",
      "Learning rate : 0.00016\n",
      "Epoch [42/50], Iter [100/886] loss : 1.7596\n",
      "Epoch [42/50], Iter [200/886] loss : 0.5853\n",
      "Epoch [42/50], Iter [300/886] loss : 1.7244\n",
      "Epoch [42/50], Iter [400/886] loss : 1.5047\n",
      "Epoch [42/50], Iter [500/886] loss : 1.0666\n",
      "Epoch [42/50], Iter [600/886] loss : 1.2967\n",
      "Epoch [42/50], Iter [700/886] loss : 1.0600\n",
      "Epoch [42/50], Iter [800/886] loss : 1.7268\n",
      "Average loss: 1.8626 \n",
      "\n",
      "Learning rate : 0.00016\n",
      "Epoch [43/50], Iter [100/886] loss : 1.0429\n",
      "Epoch [43/50], Iter [200/886] loss : 1.0848\n",
      "Epoch [43/50], Iter [300/886] loss : 2.1421\n",
      "Epoch [43/50], Iter [400/886] loss : 0.8751\n",
      "Epoch [43/50], Iter [500/886] loss : 0.8734\n",
      "Epoch [43/50], Iter [600/886] loss : 1.2108\n",
      "Epoch [43/50], Iter [700/886] loss : 1.7645\n",
      "Epoch [43/50], Iter [800/886] loss : 0.4331\n",
      "Average loss: 1.8232 \n",
      "\n",
      "Learning rate : 0.00016\n",
      "Epoch [44/50], Iter [100/886] loss : 1.5086\n",
      "Epoch [44/50], Iter [200/886] loss : 0.9492\n",
      "Epoch [44/50], Iter [300/886] loss : 1.2693\n",
      "Epoch [44/50], Iter [400/886] loss : 1.3900\n",
      "Epoch [44/50], Iter [500/886] loss : 0.8306\n",
      "Epoch [44/50], Iter [600/886] loss : 1.1381\n",
      "Epoch [44/50], Iter [700/886] loss : 1.0356\n",
      "Epoch [44/50], Iter [800/886] loss : 1.1418\n",
      "Average loss: 1.8729 \n",
      "\n",
      "Learning rate : 0.00016\n",
      "Epoch [45/50], Iter [100/886] loss : 0.6643\n",
      "Epoch [45/50], Iter [200/886] loss : 1.1145\n",
      "Epoch [45/50], Iter [300/886] loss : 2.8161\n",
      "Epoch [45/50], Iter [400/886] loss : 0.9585\n",
      "Epoch [45/50], Iter [500/886] loss : 1.0803\n",
      "Epoch [45/50], Iter [600/886] loss : 1.0595\n",
      "Epoch [45/50], Iter [700/886] loss : 1.0372\n",
      "Epoch [45/50], Iter [800/886] loss : 1.1487\n",
      "Average loss: 1.8084 \n",
      "\n",
      "Epoch    44: reducing learning rate of group 0 to 3.2000e-05.\n",
      "Learning rate : 3.2e-05\n",
      "Epoch [46/50], Iter [100/886] loss : 1.1928\n",
      "Epoch [46/50], Iter [200/886] loss : 0.6140\n",
      "Epoch [46/50], Iter [300/886] loss : 1.0591\n",
      "Epoch [46/50], Iter [400/886] loss : 1.4685\n",
      "Epoch [46/50], Iter [500/886] loss : 0.9785\n",
      "Epoch [46/50], Iter [600/886] loss : 1.0587\n",
      "Epoch [46/50], Iter [700/886] loss : 1.1356\n",
      "Epoch [46/50], Iter [800/886] loss : 0.5924\n",
      "Average loss: 1.7981 \n",
      "\n",
      "Learning rate : 3.2e-05\n",
      "Epoch [47/50], Iter [100/886] loss : 1.4721\n",
      "Epoch [47/50], Iter [200/886] loss : 1.2293\n",
      "Epoch [47/50], Iter [300/886] loss : 1.7257\n",
      "Epoch [47/50], Iter [400/886] loss : 1.5417\n",
      "Epoch [47/50], Iter [500/886] loss : 1.8270\n",
      "Epoch [47/50], Iter [600/886] loss : 1.1030\n",
      "Epoch [47/50], Iter [700/886] loss : 1.1326\n",
      "Epoch [47/50], Iter [800/886] loss : 1.1848\n",
      "Average loss: 1.8609 \n",
      "\n",
      "Learning rate : 3.2e-05\n",
      "Epoch [48/50], Iter [100/886] loss : 1.0144\n",
      "Epoch [48/50], Iter [200/886] loss : 1.4967\n",
      "Epoch [48/50], Iter [300/886] loss : 1.7642\n",
      "Epoch [48/50], Iter [400/886] loss : 1.1740\n",
      "Epoch [48/50], Iter [500/886] loss : 0.5872\n",
      "Epoch [48/50], Iter [600/886] loss : 0.7640\n",
      "Epoch [48/50], Iter [700/886] loss : 2.8018\n",
      "Epoch [48/50], Iter [800/886] loss : 1.2965\n",
      "Average loss: 1.8132 \n",
      "\n",
      "Learning rate : 3.2e-05\n",
      "Epoch [49/50], Iter [100/886] loss : 0.9102\n",
      "Epoch [49/50], Iter [200/886] loss : 0.7024\n",
      "Epoch [49/50], Iter [300/886] loss : 0.8683\n",
      "Epoch [49/50], Iter [400/886] loss : 1.1065\n",
      "Epoch [49/50], Iter [500/886] loss : 0.9478\n",
      "Epoch [49/50], Iter [600/886] loss : 1.5099\n",
      "Epoch [49/50], Iter [700/886] loss : 0.7973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/50], Iter [800/886] loss : 1.1457\n",
      "Average loss: 1.8331 \n",
      "\n",
      "Epoch    48: reducing learning rate of group 0 to 6.4000e-06.\n",
      "Learning rate : 6.4e-06\n",
      "Early stopping\n",
      "\n",
      "\n",
      "WD:   1e-05  DO:   0.6  LR:   0.1  -> VAL_loss: tensor(1.8331, device='cuda:0')\n",
      "Epoch [1/50], Iter [100/886] loss : 3.5073\n",
      "Epoch [1/50], Iter [200/886] loss : 2.3575\n",
      "Epoch [1/50], Iter [300/886] loss : 2.6724\n",
      "Epoch [1/50], Iter [400/886] loss : 2.3508\n",
      "Epoch [1/50], Iter [500/886] loss : 2.7938\n",
      "Epoch [1/50], Iter [600/886] loss : 2.7312\n",
      "Epoch [1/50], Iter [700/886] loss : 3.5334\n",
      "Epoch [1/50], Iter [800/886] loss : 2.2423\n",
      "Average loss: 2.5924 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [2/50], Iter [100/886] loss : 2.7107\n",
      "Epoch [2/50], Iter [200/886] loss : 3.2960\n",
      "Epoch [2/50], Iter [300/886] loss : 2.2442\n",
      "Epoch [2/50], Iter [400/886] loss : 2.6921\n",
      "Epoch [2/50], Iter [500/886] loss : 2.9189\n",
      "Epoch [2/50], Iter [600/886] loss : 2.4021\n",
      "Epoch [2/50], Iter [700/886] loss : 3.0498\n",
      "Epoch [2/50], Iter [800/886] loss : 3.1310\n",
      "Average loss: 2.4537 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [3/50], Iter [100/886] loss : 2.5118\n",
      "Epoch [3/50], Iter [200/886] loss : 2.4920\n",
      "Epoch [3/50], Iter [300/886] loss : 2.4159\n",
      "Epoch [3/50], Iter [400/886] loss : 2.7168\n",
      "Epoch [3/50], Iter [500/886] loss : 2.4777\n",
      "Epoch [3/50], Iter [600/886] loss : 3.2890\n",
      "Epoch [3/50], Iter [700/886] loss : 2.9910\n",
      "Epoch [3/50], Iter [800/886] loss : 3.2151\n",
      "Average loss: 2.7372 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [4/50], Iter [100/886] loss : 2.4668\n",
      "Epoch [4/50], Iter [200/886] loss : 2.1615\n",
      "Epoch [4/50], Iter [300/886] loss : 2.1929\n",
      "Epoch [4/50], Iter [400/886] loss : 2.9234\n",
      "Epoch [4/50], Iter [500/886] loss : 2.1628\n",
      "Epoch [4/50], Iter [600/886] loss : 1.9981\n",
      "Epoch [4/50], Iter [700/886] loss : 2.9407\n",
      "Epoch [4/50], Iter [800/886] loss : 2.8527\n",
      "Average loss: 2.8271 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [5/50], Iter [100/886] loss : 2.6548\n",
      "Epoch [5/50], Iter [200/886] loss : 3.0133\n",
      "Epoch [5/50], Iter [300/886] loss : 2.7143\n",
      "Epoch [5/50], Iter [400/886] loss : 2.6047\n",
      "Epoch [5/50], Iter [500/886] loss : 3.1760\n",
      "Epoch [5/50], Iter [600/886] loss : 2.4220\n",
      "Epoch [5/50], Iter [700/886] loss : 2.8647\n",
      "Epoch [5/50], Iter [800/886] loss : 2.4902\n",
      "Average loss: 2.5689 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [6/50], Iter [100/886] loss : 2.7211\n",
      "Epoch [6/50], Iter [200/886] loss : 2.8253\n",
      "Epoch [6/50], Iter [300/886] loss : 2.5142\n",
      "Epoch [6/50], Iter [400/886] loss : 3.1807\n",
      "Epoch [6/50], Iter [500/886] loss : 2.4697\n",
      "Epoch [6/50], Iter [600/886] loss : 3.0232\n",
      "Epoch [6/50], Iter [700/886] loss : 2.8920\n",
      "Epoch [6/50], Iter [800/886] loss : 2.2873\n",
      "Average loss: 2.5762 \n",
      "\n",
      "Epoch     5: reducing learning rate of group 0 to 2.0000e-02.\n",
      "Learning rate : 0.02\n",
      "Epoch [7/50], Iter [100/886] loss : 2.7044\n",
      "Epoch [7/50], Iter [200/886] loss : 2.4811\n",
      "Epoch [7/50], Iter [300/886] loss : 2.4275\n",
      "Epoch [7/50], Iter [400/886] loss : 2.2895\n",
      "Epoch [7/50], Iter [500/886] loss : 2.3126\n",
      "Epoch [7/50], Iter [600/886] loss : 2.3668\n",
      "Epoch [7/50], Iter [700/886] loss : 2.1922\n",
      "Epoch [7/50], Iter [800/886] loss : 2.5170\n",
      "Average loss: 2.3233 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [8/50], Iter [100/886] loss : 2.3431\n",
      "Epoch [8/50], Iter [200/886] loss : 2.4689\n",
      "Epoch [8/50], Iter [300/886] loss : 2.2846\n",
      "Epoch [8/50], Iter [400/886] loss : 2.1931\n",
      "Epoch [8/50], Iter [500/886] loss : 2.3163\n",
      "Epoch [8/50], Iter [600/886] loss : 2.1686\n",
      "Epoch [8/50], Iter [700/886] loss : 2.2799\n",
      "Epoch [8/50], Iter [800/886] loss : 2.1389\n",
      "Average loss: 2.3144 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [9/50], Iter [100/886] loss : 2.4211\n",
      "Epoch [9/50], Iter [200/886] loss : 2.2725\n",
      "Epoch [9/50], Iter [300/886] loss : 2.5240\n",
      "Epoch [9/50], Iter [400/886] loss : 2.4335\n",
      "Epoch [9/50], Iter [500/886] loss : 2.3649\n",
      "Epoch [9/50], Iter [600/886] loss : 2.1699\n",
      "Epoch [9/50], Iter [700/886] loss : 2.2501\n",
      "Epoch [9/50], Iter [800/886] loss : 2.3836\n",
      "Average loss: 2.3112 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [10/50], Iter [100/886] loss : 2.3764\n",
      "Epoch [10/50], Iter [200/886] loss : 2.3491\n",
      "Epoch [10/50], Iter [300/886] loss : 2.2258\n",
      "Epoch [10/50], Iter [400/886] loss : 2.3658\n",
      "Epoch [10/50], Iter [500/886] loss : 2.4371\n",
      "Epoch [10/50], Iter [600/886] loss : 2.1887\n",
      "Epoch [10/50], Iter [700/886] loss : 2.3508\n",
      "Epoch [10/50], Iter [800/886] loss : 2.4389\n",
      "Average loss: 2.3229 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [11/50], Iter [100/886] loss : 2.4535\n",
      "Epoch [11/50], Iter [200/886] loss : 2.3680\n",
      "Epoch [11/50], Iter [300/886] loss : 2.2915\n",
      "Epoch [11/50], Iter [400/886] loss : 2.3556\n",
      "Epoch [11/50], Iter [500/886] loss : 2.3505\n",
      "Epoch [11/50], Iter [600/886] loss : 2.3247\n",
      "Epoch [11/50], Iter [700/886] loss : 2.3942\n",
      "Epoch [11/50], Iter [800/886] loss : 2.2822\n",
      "Average loss: 2.3738 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [12/50], Iter [100/886] loss : 2.2860\n",
      "Epoch [12/50], Iter [200/886] loss : 2.4139\n",
      "Epoch [12/50], Iter [300/886] loss : 2.1201\n",
      "Epoch [12/50], Iter [400/886] loss : 2.4070\n",
      "Epoch [12/50], Iter [500/886] loss : 2.3644\n",
      "Epoch [12/50], Iter [600/886] loss : 2.2333\n",
      "Epoch [12/50], Iter [700/886] loss : 2.3146\n",
      "Epoch [12/50], Iter [800/886] loss : 2.4129\n",
      "Average loss: 2.3234 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [13/50], Iter [100/886] loss : 2.3526\n",
      "Epoch [13/50], Iter [200/886] loss : 2.3529\n",
      "Epoch [13/50], Iter [300/886] loss : 2.7553\n",
      "Epoch [13/50], Iter [400/886] loss : 2.5069\n",
      "Epoch [13/50], Iter [500/886] loss : 2.4914\n",
      "Epoch [13/50], Iter [600/886] loss : 2.2884\n",
      "Epoch [13/50], Iter [700/886] loss : 2.2916\n",
      "Epoch [13/50], Iter [800/886] loss : 2.3541\n",
      "Average loss: 2.4572 \n",
      "\n",
      "Epoch    12: reducing learning rate of group 0 to 4.0000e-03.\n",
      "Learning rate : 0.004\n",
      "Epoch [14/50], Iter [100/886] loss : 2.3178\n",
      "Epoch [14/50], Iter [200/886] loss : 2.3215\n",
      "Epoch [14/50], Iter [300/886] loss : 2.2325\n",
      "Epoch [14/50], Iter [400/886] loss : 2.3818\n",
      "Epoch [14/50], Iter [500/886] loss : 2.3441\n",
      "Epoch [14/50], Iter [600/886] loss : 2.2782\n",
      "Epoch [14/50], Iter [700/886] loss : 2.2310\n",
      "Epoch [14/50], Iter [800/886] loss : 2.3774\n",
      "Average loss: 2.2992 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [15/50], Iter [100/886] loss : 2.3518\n",
      "Epoch [15/50], Iter [200/886] loss : 2.2199\n",
      "Epoch [15/50], Iter [300/886] loss : 2.2327\n",
      "Epoch [15/50], Iter [400/886] loss : 2.3841\n",
      "Epoch [15/50], Iter [500/886] loss : 2.3404\n",
      "Epoch [15/50], Iter [600/886] loss : 2.2404\n",
      "Epoch [15/50], Iter [700/886] loss : 2.2661\n",
      "Epoch [15/50], Iter [800/886] loss : 2.2283\n",
      "Average loss: 2.3070 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [16/50], Iter [100/886] loss : 2.2193\n",
      "Epoch [16/50], Iter [200/886] loss : 2.3161\n",
      "Epoch [16/50], Iter [300/886] loss : 2.3376\n",
      "Epoch [16/50], Iter [400/886] loss : 2.2602\n",
      "Epoch [16/50], Iter [500/886] loss : 2.1856\n",
      "Epoch [16/50], Iter [600/886] loss : 2.2809\n",
      "Epoch [16/50], Iter [700/886] loss : 2.3159\n",
      "Epoch [16/50], Iter [800/886] loss : 2.4431\n",
      "Average loss: 2.3149 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [17/50], Iter [100/886] loss : 2.2041\n",
      "Epoch [17/50], Iter [200/886] loss : 2.3856\n",
      "Epoch [17/50], Iter [300/886] loss : 2.3138\n",
      "Epoch [17/50], Iter [400/886] loss : 2.3168\n",
      "Epoch [17/50], Iter [500/886] loss : 2.2480\n",
      "Epoch [17/50], Iter [600/886] loss : 2.2865\n",
      "Epoch [17/50], Iter [700/886] loss : 2.2892\n",
      "Epoch [17/50], Iter [800/886] loss : 2.3692\n",
      "Average loss: 2.3146 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [18/50], Iter [100/886] loss : 2.2542\n",
      "Epoch [18/50], Iter [200/886] loss : 2.3021\n",
      "Epoch [18/50], Iter [300/886] loss : 2.3511\n",
      "Epoch [18/50], Iter [400/886] loss : 2.2738\n",
      "Epoch [18/50], Iter [500/886] loss : 2.3155\n",
      "Epoch [18/50], Iter [600/886] loss : 2.3373\n",
      "Epoch [18/50], Iter [700/886] loss : 2.2866\n",
      "Epoch [18/50], Iter [800/886] loss : 2.3248\n",
      "Average loss: 2.3016 \n",
      "\n",
      "Epoch    17: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Learning rate : 0.0008\n",
      "Epoch [19/50], Iter [100/886] loss : 2.2756\n",
      "Epoch [19/50], Iter [200/886] loss : 2.2799\n",
      "Epoch [19/50], Iter [300/886] loss : 2.2889\n",
      "Epoch [19/50], Iter [400/886] loss : 2.2981\n",
      "Epoch [19/50], Iter [500/886] loss : 2.2746\n",
      "Epoch [19/50], Iter [600/886] loss : 2.3347\n",
      "Epoch [19/50], Iter [700/886] loss : 2.3658\n",
      "Epoch [19/50], Iter [800/886] loss : 2.3282\n",
      "Average loss: 2.1387 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [20/50], Iter [100/886] loss : 2.2271\n",
      "Epoch [20/50], Iter [200/886] loss : 1.9604\n",
      "Epoch [20/50], Iter [300/886] loss : 1.9870\n",
      "Epoch [20/50], Iter [400/886] loss : 2.1204\n",
      "Epoch [20/50], Iter [500/886] loss : 2.4170\n",
      "Epoch [20/50], Iter [600/886] loss : 2.6536\n",
      "Epoch [20/50], Iter [700/886] loss : 1.7119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/50], Iter [800/886] loss : 2.6007\n",
      "Average loss: 2.1139 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [21/50], Iter [100/886] loss : 1.9630\n",
      "Epoch [21/50], Iter [200/886] loss : 1.7612\n",
      "Epoch [21/50], Iter [300/886] loss : 2.0015\n",
      "Epoch [21/50], Iter [400/886] loss : 2.0848\n",
      "Epoch [21/50], Iter [500/886] loss : 1.9651\n",
      "Epoch [21/50], Iter [600/886] loss : 1.7268\n",
      "Epoch [21/50], Iter [700/886] loss : 2.0390\n",
      "Epoch [21/50], Iter [800/886] loss : 2.0421\n",
      "Average loss: 2.0464 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [22/50], Iter [100/886] loss : 1.9406\n",
      "Epoch [22/50], Iter [200/886] loss : 1.6770\n",
      "Epoch [22/50], Iter [300/886] loss : 1.7099\n",
      "Epoch [22/50], Iter [400/886] loss : 2.3020\n",
      "Epoch [22/50], Iter [500/886] loss : 2.0431\n",
      "Epoch [22/50], Iter [600/886] loss : 1.6090\n",
      "Epoch [22/50], Iter [700/886] loss : 1.9339\n",
      "Epoch [22/50], Iter [800/886] loss : 1.8391\n",
      "Average loss: 1.9863 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [23/50], Iter [100/886] loss : 1.8754\n",
      "Epoch [23/50], Iter [200/886] loss : 2.4991\n",
      "Epoch [23/50], Iter [300/886] loss : 2.1575\n",
      "Epoch [23/50], Iter [400/886] loss : 1.7770\n",
      "Epoch [23/50], Iter [500/886] loss : 1.9351\n",
      "Epoch [23/50], Iter [600/886] loss : 1.7312\n",
      "Epoch [23/50], Iter [700/886] loss : 2.0524\n",
      "Epoch [23/50], Iter [800/886] loss : 1.9552\n",
      "Average loss: 1.9844 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [24/50], Iter [100/886] loss : 1.7424\n",
      "Epoch [24/50], Iter [200/886] loss : 1.5502\n",
      "Epoch [24/50], Iter [300/886] loss : 1.9853\n",
      "Epoch [24/50], Iter [400/886] loss : 1.4977\n",
      "Epoch [24/50], Iter [500/886] loss : 1.7082\n",
      "Epoch [24/50], Iter [600/886] loss : 2.7778\n",
      "Epoch [24/50], Iter [700/886] loss : 1.9562\n",
      "Epoch [24/50], Iter [800/886] loss : 2.1189\n",
      "Average loss: 1.9546 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [25/50], Iter [100/886] loss : 1.4051\n",
      "Epoch [25/50], Iter [200/886] loss : 1.8338\n",
      "Epoch [25/50], Iter [300/886] loss : 1.8782\n",
      "Epoch [25/50], Iter [400/886] loss : 2.5937\n",
      "Epoch [25/50], Iter [500/886] loss : 2.7595\n",
      "Epoch [25/50], Iter [600/886] loss : 1.7109\n",
      "Epoch [25/50], Iter [700/886] loss : 1.7762\n",
      "Epoch [25/50], Iter [800/886] loss : 1.1380\n",
      "Average loss: 1.9308 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [26/50], Iter [100/886] loss : 2.5619\n",
      "Epoch [26/50], Iter [200/886] loss : 2.3103\n",
      "Epoch [26/50], Iter [300/886] loss : 2.3915\n",
      "Epoch [26/50], Iter [400/886] loss : 2.6598\n",
      "Epoch [26/50], Iter [500/886] loss : 1.5864\n",
      "Epoch [26/50], Iter [600/886] loss : 1.2294\n",
      "Epoch [26/50], Iter [700/886] loss : 1.9048\n",
      "Epoch [26/50], Iter [800/886] loss : 2.0401\n",
      "Average loss: 1.9305 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [27/50], Iter [100/886] loss : 1.9455\n",
      "Epoch [27/50], Iter [200/886] loss : 1.8363\n",
      "Epoch [27/50], Iter [300/886] loss : 1.7315\n",
      "Epoch [27/50], Iter [400/886] loss : 1.4195\n",
      "Epoch [27/50], Iter [500/886] loss : 2.2058\n",
      "Epoch [27/50], Iter [600/886] loss : 1.2217\n",
      "Epoch [27/50], Iter [700/886] loss : 2.1213\n",
      "Epoch [27/50], Iter [800/886] loss : 2.0511\n",
      "Average loss: 1.8345 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [28/50], Iter [100/886] loss : 2.3534\n",
      "Epoch [28/50], Iter [200/886] loss : 1.3410\n",
      "Epoch [28/50], Iter [300/886] loss : 1.5259\n",
      "Epoch [28/50], Iter [400/886] loss : 1.9414\n",
      "Epoch [28/50], Iter [500/886] loss : 1.6478\n",
      "Epoch [28/50], Iter [600/886] loss : 1.5908\n",
      "Epoch [28/50], Iter [700/886] loss : 1.9903\n",
      "Epoch [28/50], Iter [800/886] loss : 1.3218\n",
      "Average loss: 1.8624 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [29/50], Iter [100/886] loss : 1.7613\n",
      "Epoch [29/50], Iter [200/886] loss : 2.1727\n",
      "Epoch [29/50], Iter [300/886] loss : 1.5766\n",
      "Epoch [29/50], Iter [400/886] loss : 1.5410\n",
      "Epoch [29/50], Iter [500/886] loss : 1.8977\n",
      "Epoch [29/50], Iter [600/886] loss : 1.5169\n",
      "Epoch [29/50], Iter [700/886] loss : 1.6627\n",
      "Epoch [29/50], Iter [800/886] loss : 1.8939\n",
      "Average loss: 1.8417 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [30/50], Iter [100/886] loss : 1.7249\n",
      "Epoch [30/50], Iter [200/886] loss : 2.1066\n",
      "Epoch [30/50], Iter [300/886] loss : 1.2719\n",
      "Epoch [30/50], Iter [400/886] loss : 1.1615\n",
      "Epoch [30/50], Iter [500/886] loss : 3.0852\n",
      "Epoch [30/50], Iter [600/886] loss : 1.6327\n",
      "Epoch [30/50], Iter [700/886] loss : 1.7178\n",
      "Epoch [30/50], Iter [800/886] loss : 1.2925\n",
      "Average loss: 1.8127 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [31/50], Iter [100/886] loss : 1.6948\n",
      "Epoch [31/50], Iter [200/886] loss : 1.8413\n",
      "Epoch [31/50], Iter [300/886] loss : 2.3421\n",
      "Epoch [31/50], Iter [400/886] loss : 1.0615\n",
      "Epoch [31/50], Iter [500/886] loss : 2.3402\n",
      "Epoch [31/50], Iter [600/886] loss : 1.1994\n",
      "Epoch [31/50], Iter [700/886] loss : 1.7498\n",
      "Epoch [31/50], Iter [800/886] loss : 1.9948\n",
      "Average loss: 1.8505 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [32/50], Iter [100/886] loss : 2.5403\n",
      "Epoch [32/50], Iter [200/886] loss : 2.0165\n",
      "Epoch [32/50], Iter [300/886] loss : 1.8025\n",
      "Epoch [32/50], Iter [400/886] loss : 1.3274\n",
      "Epoch [32/50], Iter [500/886] loss : 1.5887\n",
      "Epoch [32/50], Iter [600/886] loss : 1.3942\n",
      "Epoch [32/50], Iter [700/886] loss : 2.1099\n",
      "Epoch [32/50], Iter [800/886] loss : 1.7485\n",
      "Average loss: 1.7910 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [33/50], Iter [100/886] loss : 1.3680\n",
      "Epoch [33/50], Iter [200/886] loss : 1.7790\n",
      "Epoch [33/50], Iter [300/886] loss : 2.1084\n",
      "Epoch [33/50], Iter [400/886] loss : 1.5634\n",
      "Epoch [33/50], Iter [500/886] loss : 0.9844\n",
      "Epoch [33/50], Iter [600/886] loss : 1.3230\n",
      "Epoch [33/50], Iter [700/886] loss : 1.6173\n",
      "Epoch [33/50], Iter [800/886] loss : 1.2643\n",
      "Average loss: 1.8393 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [34/50], Iter [100/886] loss : 1.4175\n",
      "Epoch [34/50], Iter [200/886] loss : 1.9870\n",
      "Epoch [34/50], Iter [300/886] loss : 2.6585\n",
      "Epoch [34/50], Iter [400/886] loss : 1.6137\n",
      "Epoch [34/50], Iter [500/886] loss : 2.0370\n",
      "Epoch [34/50], Iter [600/886] loss : 1.4874\n",
      "Epoch [34/50], Iter [700/886] loss : 1.2534\n",
      "Epoch [34/50], Iter [800/886] loss : 1.4098\n",
      "Average loss: 1.7135 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [35/50], Iter [100/886] loss : 1.7713\n",
      "Epoch [35/50], Iter [200/886] loss : 1.3146\n",
      "Epoch [35/50], Iter [300/886] loss : 2.6701\n",
      "Epoch [35/50], Iter [400/886] loss : 1.4678\n",
      "Epoch [35/50], Iter [500/886] loss : 1.5319\n",
      "Epoch [35/50], Iter [600/886] loss : 2.1206\n",
      "Epoch [35/50], Iter [700/886] loss : 1.6977\n",
      "Epoch [35/50], Iter [800/886] loss : 1.7069\n",
      "Average loss: 1.6863 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [36/50], Iter [100/886] loss : 1.5794\n",
      "Epoch [36/50], Iter [200/886] loss : 1.5490\n",
      "Epoch [36/50], Iter [300/886] loss : 1.2603\n",
      "Epoch [36/50], Iter [400/886] loss : 0.7770\n",
      "Epoch [36/50], Iter [500/886] loss : 1.2545\n",
      "Epoch [36/50], Iter [600/886] loss : 1.9821\n",
      "Epoch [36/50], Iter [700/886] loss : 1.2922\n",
      "Epoch [36/50], Iter [800/886] loss : 2.9066\n",
      "Average loss: 1.7497 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [37/50], Iter [100/886] loss : 1.4645\n",
      "Epoch [37/50], Iter [200/886] loss : 0.9620\n",
      "Epoch [37/50], Iter [300/886] loss : 1.6416\n",
      "Epoch [37/50], Iter [400/886] loss : 1.2773\n",
      "Epoch [37/50], Iter [500/886] loss : 1.4626\n",
      "Epoch [37/50], Iter [600/886] loss : 1.8944\n",
      "Epoch [37/50], Iter [700/886] loss : 1.1082\n",
      "Epoch [37/50], Iter [800/886] loss : 0.9190\n",
      "Average loss: 1.8280 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [38/50], Iter [100/886] loss : 1.9185\n",
      "Epoch [38/50], Iter [200/886] loss : 1.7264\n",
      "Epoch [38/50], Iter [300/886] loss : 1.1295\n",
      "Epoch [38/50], Iter [400/886] loss : 1.0910\n",
      "Epoch [38/50], Iter [500/886] loss : 0.9236\n",
      "Epoch [38/50], Iter [600/886] loss : 1.0847\n",
      "Epoch [38/50], Iter [700/886] loss : 1.5523\n",
      "Epoch [38/50], Iter [800/886] loss : 1.4694\n",
      "Average loss: 1.7719 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [39/50], Iter [100/886] loss : 1.4286\n",
      "Epoch [39/50], Iter [200/886] loss : 1.9677\n",
      "Epoch [39/50], Iter [300/886] loss : 2.7832\n",
      "Epoch [39/50], Iter [400/886] loss : 1.4911\n",
      "Epoch [39/50], Iter [500/886] loss : 1.2393\n",
      "Epoch [39/50], Iter [600/886] loss : 1.1601\n",
      "Epoch [39/50], Iter [700/886] loss : 1.5472\n",
      "Epoch [39/50], Iter [800/886] loss : 1.8721\n",
      "Average loss: 1.7834 \n",
      "\n",
      "Epoch    38: reducing learning rate of group 0 to 1.6000e-04.\n",
      "Learning rate : 0.00016\n",
      "Epoch [40/50], Iter [100/886] loss : 1.2859\n",
      "Epoch [40/50], Iter [200/886] loss : 0.8307\n",
      "Epoch [40/50], Iter [300/886] loss : 1.4068\n",
      "Epoch [40/50], Iter [400/886] loss : 1.4868\n",
      "Epoch [40/50], Iter [500/886] loss : 1.7034\n",
      "Epoch [40/50], Iter [600/886] loss : 1.5128\n",
      "Epoch [40/50], Iter [700/886] loss : 1.2901\n",
      "Epoch [40/50], Iter [800/886] loss : 1.1793\n",
      "Average loss: 1.7533 \n",
      "\n",
      "Learning rate : 0.00016\n",
      "Epoch [41/50], Iter [100/886] loss : 1.2146\n",
      "Epoch [41/50], Iter [200/886] loss : 0.8047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/50], Iter [300/886] loss : 3.5071\n",
      "Epoch [41/50], Iter [400/886] loss : 1.5349\n",
      "Epoch [41/50], Iter [500/886] loss : 0.9026\n",
      "Epoch [41/50], Iter [600/886] loss : 0.8706\n",
      "Epoch [41/50], Iter [700/886] loss : 1.2224\n",
      "Epoch [41/50], Iter [800/886] loss : 1.0833\n",
      "Average loss: 1.7160 \n",
      "\n",
      "Learning rate : 0.00016\n",
      "Epoch [42/50], Iter [100/886] loss : 1.2283\n",
      "Epoch [42/50], Iter [200/886] loss : 1.2128\n",
      "Epoch [42/50], Iter [300/886] loss : 1.5214\n",
      "Epoch [42/50], Iter [400/886] loss : 0.9014\n",
      "Epoch [42/50], Iter [500/886] loss : 2.3372\n",
      "Epoch [42/50], Iter [600/886] loss : 1.5303\n",
      "Epoch [42/50], Iter [700/886] loss : 1.1157\n",
      "Epoch [42/50], Iter [800/886] loss : 1.4502\n",
      "Average loss: 1.6903 \n",
      "\n",
      "Learning rate : 0.00016\n",
      "Epoch [43/50], Iter [100/886] loss : 1.7312\n",
      "Epoch [43/50], Iter [200/886] loss : 1.0312\n",
      "Epoch [43/50], Iter [300/886] loss : 1.4477\n",
      "Epoch [43/50], Iter [400/886] loss : 2.0679\n",
      "Epoch [43/50], Iter [500/886] loss : 1.1038\n",
      "Epoch [43/50], Iter [600/886] loss : 1.7203\n",
      "Epoch [43/50], Iter [700/886] loss : 1.5750\n",
      "Epoch [43/50], Iter [800/886] loss : 1.4268\n",
      "Average loss: 1.6969 \n",
      "\n",
      "Epoch    42: reducing learning rate of group 0 to 3.2000e-05.\n",
      "Learning rate : 3.2e-05\n",
      "Epoch [44/50], Iter [100/886] loss : 2.6800\n",
      "Epoch [44/50], Iter [200/886] loss : 1.0177\n",
      "Epoch [44/50], Iter [300/886] loss : 0.7678\n",
      "Epoch [44/50], Iter [400/886] loss : 1.0248\n",
      "Epoch [44/50], Iter [500/886] loss : 1.4431\n",
      "Epoch [44/50], Iter [600/886] loss : 1.8213\n",
      "Epoch [44/50], Iter [700/886] loss : 1.8032\n",
      "Epoch [44/50], Iter [800/886] loss : 1.6408\n",
      "Average loss: 1.7706 \n",
      "\n",
      "Learning rate : 3.2e-05\n",
      "Epoch [45/50], Iter [100/886] loss : 1.1126\n",
      "Epoch [45/50], Iter [200/886] loss : 1.6301\n",
      "Epoch [45/50], Iter [300/886] loss : 1.3728\n",
      "Epoch [45/50], Iter [400/886] loss : 2.1038\n",
      "Epoch [45/50], Iter [500/886] loss : 1.2743\n",
      "Epoch [45/50], Iter [600/886] loss : 0.9851\n",
      "Epoch [45/50], Iter [700/886] loss : 1.2884\n",
      "Epoch [45/50], Iter [800/886] loss : 2.6281\n",
      "Average loss: 1.6704 \n",
      "\n",
      "Learning rate : 3.2e-05\n",
      "Epoch [46/50], Iter [100/886] loss : 0.9706\n",
      "Epoch [46/50], Iter [200/886] loss : 1.0829\n",
      "Epoch [46/50], Iter [300/886] loss : 1.3821\n",
      "Epoch [46/50], Iter [400/886] loss : 1.5870\n",
      "Epoch [46/50], Iter [500/886] loss : 0.8376\n",
      "Epoch [46/50], Iter [600/886] loss : 1.2356\n",
      "Epoch [46/50], Iter [700/886] loss : 1.7688\n",
      "Epoch [46/50], Iter [800/886] loss : 1.3432\n",
      "Average loss: 1.6953 \n",
      "\n",
      "Learning rate : 3.2e-05\n",
      "Epoch [47/50], Iter [100/886] loss : 1.8955\n",
      "Epoch [47/50], Iter [200/886] loss : 0.8877\n",
      "Epoch [47/50], Iter [300/886] loss : 0.7370\n",
      "Epoch [47/50], Iter [400/886] loss : 1.0899\n",
      "Epoch [47/50], Iter [500/886] loss : 1.1132\n",
      "Epoch [47/50], Iter [600/886] loss : 1.3894\n",
      "Epoch [47/50], Iter [700/886] loss : 2.3106\n",
      "Epoch [47/50], Iter [800/886] loss : 2.1796\n",
      "Average loss: 1.6842 \n",
      "\n",
      "Learning rate : 3.2e-05\n",
      "Epoch [48/50], Iter [100/886] loss : 1.1867\n",
      "Epoch [48/50], Iter [200/886] loss : 1.6460\n",
      "Epoch [48/50], Iter [300/886] loss : 1.6753\n",
      "Epoch [48/50], Iter [400/886] loss : 1.1447\n",
      "Epoch [48/50], Iter [500/886] loss : 2.1967\n",
      "Epoch [48/50], Iter [600/886] loss : 1.3356\n",
      "Epoch [48/50], Iter [700/886] loss : 0.9912\n",
      "Epoch [48/50], Iter [800/886] loss : 0.8445\n",
      "Average loss: 1.7197 \n",
      "\n",
      "Learning rate : 3.2e-05\n",
      "Epoch [49/50], Iter [100/886] loss : 1.8667\n",
      "Epoch [49/50], Iter [200/886] loss : 1.1748\n",
      "Epoch [49/50], Iter [300/886] loss : 0.7953\n",
      "Epoch [49/50], Iter [400/886] loss : 1.3427\n",
      "Epoch [49/50], Iter [500/886] loss : 1.4088\n",
      "Epoch [49/50], Iter [600/886] loss : 1.7581\n",
      "Epoch [49/50], Iter [700/886] loss : 2.5006\n",
      "Epoch [49/50], Iter [800/886] loss : 1.4729\n",
      "Average loss: 1.6570 \n",
      "\n",
      "Learning rate : 3.2e-05\n",
      "Epoch [50/50], Iter [100/886] loss : 1.4053\n",
      "Epoch [50/50], Iter [200/886] loss : 0.8454\n",
      "Epoch [50/50], Iter [300/886] loss : 1.5182\n",
      "Epoch [50/50], Iter [400/886] loss : 1.1976\n",
      "Epoch [50/50], Iter [500/886] loss : 1.9473\n",
      "Epoch [50/50], Iter [600/886] loss : 1.1218\n",
      "Epoch [50/50], Iter [700/886] loss : 1.0476\n",
      "Epoch [50/50], Iter [800/886] loss : 1.3168\n",
      "Average loss: 1.6943 \n",
      "\n",
      "Learning rate : 3.2e-05\n",
      "WD:   1e-06  DO:   0.5  LR:   0.1  -> VAL_loss: tensor(1.6943, device='cuda:0')\n",
      "Epoch [1/50], Iter [100/886] loss : 2.6830\n",
      "Epoch [1/50], Iter [200/886] loss : 2.7289\n",
      "Epoch [1/50], Iter [300/886] loss : 3.0800\n",
      "Epoch [1/50], Iter [400/886] loss : 3.0890\n",
      "Epoch [1/50], Iter [500/886] loss : 2.8687\n",
      "Epoch [1/50], Iter [600/886] loss : 2.7460\n",
      "Epoch [1/50], Iter [700/886] loss : 2.1561\n",
      "Epoch [1/50], Iter [800/886] loss : 3.4986\n",
      "Average loss: 2.7036 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [2/50], Iter [100/886] loss : 2.6744\n",
      "Epoch [2/50], Iter [200/886] loss : 2.8749\n",
      "Epoch [2/50], Iter [300/886] loss : 2.6664\n",
      "Epoch [2/50], Iter [400/886] loss : 2.7534\n",
      "Epoch [2/50], Iter [500/886] loss : 3.4457\n",
      "Epoch [2/50], Iter [600/886] loss : 2.8973\n",
      "Epoch [2/50], Iter [700/886] loss : 3.2857\n",
      "Epoch [2/50], Iter [800/886] loss : 2.3408\n",
      "Average loss: 2.4896 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [3/50], Iter [100/886] loss : 2.8690\n",
      "Epoch [3/50], Iter [200/886] loss : 2.4131\n",
      "Epoch [3/50], Iter [300/886] loss : 2.9860\n",
      "Epoch [3/50], Iter [400/886] loss : 4.2993\n",
      "Epoch [3/50], Iter [500/886] loss : 4.0439\n",
      "Epoch [3/50], Iter [600/886] loss : 3.8214\n",
      "Epoch [3/50], Iter [700/886] loss : 3.5987\n",
      "Epoch [3/50], Iter [800/886] loss : 2.7516\n",
      "Average loss: 2.5474 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [4/50], Iter [100/886] loss : 3.2923\n",
      "Epoch [4/50], Iter [200/886] loss : 2.8559\n",
      "Epoch [4/50], Iter [300/886] loss : 2.9673\n",
      "Epoch [4/50], Iter [400/886] loss : 2.6308\n",
      "Epoch [4/50], Iter [500/886] loss : 2.5183\n",
      "Epoch [4/50], Iter [600/886] loss : 2.4119\n",
      "Epoch [4/50], Iter [700/886] loss : 1.7905\n",
      "Epoch [4/50], Iter [800/886] loss : 2.9839\n",
      "Average loss: 2.6152 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [5/50], Iter [100/886] loss : 2.9604\n",
      "Epoch [5/50], Iter [200/886] loss : 3.0537\n",
      "Epoch [5/50], Iter [300/886] loss : 1.6336\n",
      "Epoch [5/50], Iter [400/886] loss : 3.1892\n",
      "Epoch [5/50], Iter [500/886] loss : 2.0696\n",
      "Epoch [5/50], Iter [600/886] loss : 3.5321\n",
      "Epoch [5/50], Iter [700/886] loss : 2.6635\n",
      "Epoch [5/50], Iter [800/886] loss : 2.8576\n",
      "Average loss: 2.5337 \n",
      "\n",
      "Learning rate : 0.1\n",
      "Epoch [6/50], Iter [100/886] loss : 3.0410\n",
      "Epoch [6/50], Iter [200/886] loss : 3.4638\n",
      "Epoch [6/50], Iter [300/886] loss : 3.8337\n",
      "Epoch [6/50], Iter [400/886] loss : 2.7376\n",
      "Epoch [6/50], Iter [500/886] loss : 1.8573\n",
      "Epoch [6/50], Iter [600/886] loss : 2.7378\n",
      "Epoch [6/50], Iter [700/886] loss : 3.0630\n",
      "Epoch [6/50], Iter [800/886] loss : 2.6338\n",
      "Average loss: 2.6056 \n",
      "\n",
      "Epoch     5: reducing learning rate of group 0 to 2.0000e-02.\n",
      "Learning rate : 0.02\n",
      "Epoch [7/50], Iter [100/886] loss : 2.4675\n",
      "Epoch [7/50], Iter [200/886] loss : 2.5523\n",
      "Epoch [7/50], Iter [300/886] loss : 2.4657\n",
      "Epoch [7/50], Iter [400/886] loss : 2.7226\n",
      "Epoch [7/50], Iter [500/886] loss : 2.4525\n",
      "Epoch [7/50], Iter [600/886] loss : 2.0995\n",
      "Epoch [7/50], Iter [700/886] loss : 2.5697\n",
      "Epoch [7/50], Iter [800/886] loss : 2.2829\n",
      "Average loss: 2.3018 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [8/50], Iter [100/886] loss : 2.3442\n",
      "Epoch [8/50], Iter [200/886] loss : 2.2880\n",
      "Epoch [8/50], Iter [300/886] loss : 2.1900\n",
      "Epoch [8/50], Iter [400/886] loss : 2.2831\n",
      "Epoch [8/50], Iter [500/886] loss : 2.3300\n",
      "Epoch [8/50], Iter [600/886] loss : 2.1519\n",
      "Epoch [8/50], Iter [700/886] loss : 2.2292\n",
      "Epoch [8/50], Iter [800/886] loss : 2.3181\n",
      "Average loss: 2.3260 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [9/50], Iter [100/886] loss : 2.4340\n",
      "Epoch [9/50], Iter [200/886] loss : 2.4743\n",
      "Epoch [9/50], Iter [300/886] loss : 2.1093\n",
      "Epoch [9/50], Iter [400/886] loss : 2.4316\n",
      "Epoch [9/50], Iter [500/886] loss : 2.5216\n",
      "Epoch [9/50], Iter [600/886] loss : 2.2342\n",
      "Epoch [9/50], Iter [700/886] loss : 2.3304\n",
      "Epoch [9/50], Iter [800/886] loss : 2.4861\n",
      "Average loss: 2.3118 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [10/50], Iter [100/886] loss : 2.3157\n",
      "Epoch [10/50], Iter [200/886] loss : 2.4042\n",
      "Epoch [10/50], Iter [300/886] loss : 2.2382\n",
      "Epoch [10/50], Iter [400/886] loss : 2.3884\n",
      "Epoch [10/50], Iter [500/886] loss : 2.4232\n",
      "Epoch [10/50], Iter [600/886] loss : 2.2236\n",
      "Epoch [10/50], Iter [700/886] loss : 2.4780\n",
      "Epoch [10/50], Iter [800/886] loss : 2.3094\n",
      "Average loss: 2.3238 \n",
      "\n",
      "Learning rate : 0.02\n",
      "Epoch [11/50], Iter [100/886] loss : 2.2644\n",
      "Epoch [11/50], Iter [200/886] loss : 2.0870\n",
      "Epoch [11/50], Iter [300/886] loss : 2.3419\n",
      "Epoch [11/50], Iter [400/886] loss : 2.2897\n",
      "Epoch [11/50], Iter [500/886] loss : 2.3400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Iter [600/886] loss : 2.3830\n",
      "Epoch [11/50], Iter [700/886] loss : 2.3017\n",
      "Epoch [11/50], Iter [800/886] loss : 2.1971\n",
      "Average loss: 2.3923 \n",
      "\n",
      "Epoch    10: reducing learning rate of group 0 to 4.0000e-03.\n",
      "Learning rate : 0.004\n",
      "Epoch [12/50], Iter [100/886] loss : 2.3225\n",
      "Epoch [12/50], Iter [200/886] loss : 2.4268\n",
      "Epoch [12/50], Iter [300/886] loss : 2.4043\n",
      "Epoch [12/50], Iter [400/886] loss : 2.2707\n",
      "Epoch [12/50], Iter [500/886] loss : 2.2938\n",
      "Epoch [12/50], Iter [600/886] loss : 2.2881\n",
      "Epoch [12/50], Iter [700/886] loss : 2.3127\n",
      "Epoch [12/50], Iter [800/886] loss : 2.3690\n",
      "Average loss: 2.3173 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [13/50], Iter [100/886] loss : 2.3679\n",
      "Epoch [13/50], Iter [200/886] loss : 2.3664\n",
      "Epoch [13/50], Iter [300/886] loss : 2.3039\n",
      "Epoch [13/50], Iter [400/886] loss : 2.2727\n",
      "Epoch [13/50], Iter [500/886] loss : 2.2889\n",
      "Epoch [13/50], Iter [600/886] loss : 2.3040\n",
      "Epoch [13/50], Iter [700/886] loss : 2.2994\n",
      "Epoch [13/50], Iter [800/886] loss : 2.2416\n",
      "Average loss: 2.3047 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [14/50], Iter [100/886] loss : 2.3364\n",
      "Epoch [14/50], Iter [200/886] loss : 2.3070\n",
      "Epoch [14/50], Iter [300/886] loss : 2.2992\n",
      "Epoch [14/50], Iter [400/886] loss : 2.2982\n",
      "Epoch [14/50], Iter [500/886] loss : 2.2538\n",
      "Epoch [14/50], Iter [600/886] loss : 2.3212\n",
      "Epoch [14/50], Iter [700/886] loss : 2.2102\n",
      "Epoch [14/50], Iter [800/886] loss : 2.2615\n",
      "Average loss: 2.3010 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [15/50], Iter [100/886] loss : 2.2426\n",
      "Epoch [15/50], Iter [200/886] loss : 2.3113\n",
      "Epoch [15/50], Iter [300/886] loss : 2.3794\n",
      "Epoch [15/50], Iter [400/886] loss : 2.3916\n",
      "Epoch [15/50], Iter [500/886] loss : 2.3489\n",
      "Epoch [15/50], Iter [600/886] loss : 2.3629\n",
      "Epoch [15/50], Iter [700/886] loss : 2.3095\n",
      "Epoch [15/50], Iter [800/886] loss : 2.3026\n",
      "Average loss: 2.2978 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [16/50], Iter [100/886] loss : 2.2884\n",
      "Epoch [16/50], Iter [200/886] loss : 2.4218\n",
      "Epoch [16/50], Iter [300/886] loss : 2.3696\n",
      "Epoch [16/50], Iter [400/886] loss : 2.3395\n",
      "Epoch [16/50], Iter [500/886] loss : 2.2931\n",
      "Epoch [16/50], Iter [600/886] loss : 2.5007\n",
      "Epoch [16/50], Iter [700/886] loss : 2.3012\n",
      "Epoch [16/50], Iter [800/886] loss : 2.3689\n",
      "Average loss: 2.3071 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [17/50], Iter [100/886] loss : 2.2302\n",
      "Epoch [17/50], Iter [200/886] loss : 2.4013\n",
      "Epoch [17/50], Iter [300/886] loss : 2.2527\n",
      "Epoch [17/50], Iter [400/886] loss : 2.3429\n",
      "Epoch [17/50], Iter [500/886] loss : 2.3775\n",
      "Epoch [17/50], Iter [600/886] loss : 2.2982\n",
      "Epoch [17/50], Iter [700/886] loss : 2.2686\n",
      "Epoch [17/50], Iter [800/886] loss : 2.3681\n",
      "Average loss: 2.2970 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [18/50], Iter [100/886] loss : 2.3008\n",
      "Epoch [18/50], Iter [200/886] loss : 2.3048\n",
      "Epoch [18/50], Iter [300/886] loss : 2.2872\n",
      "Epoch [18/50], Iter [400/886] loss : 2.2643\n",
      "Epoch [18/50], Iter [500/886] loss : 2.3968\n",
      "Epoch [18/50], Iter [600/886] loss : 2.3617\n",
      "Epoch [18/50], Iter [700/886] loss : 2.1816\n",
      "Epoch [18/50], Iter [800/886] loss : 2.2946\n",
      "Average loss: 2.3085 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [19/50], Iter [100/886] loss : 2.2993\n",
      "Epoch [19/50], Iter [200/886] loss : 2.0195\n",
      "Epoch [19/50], Iter [300/886] loss : 1.9075\n",
      "Epoch [19/50], Iter [400/886] loss : 2.4377\n",
      "Epoch [19/50], Iter [500/886] loss : 2.1686\n",
      "Epoch [19/50], Iter [600/886] loss : 2.3495\n",
      "Epoch [19/50], Iter [700/886] loss : 1.7583\n",
      "Epoch [19/50], Iter [800/886] loss : 1.9957\n",
      "Average loss: 2.0647 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [20/50], Iter [100/886] loss : 2.1133\n",
      "Epoch [20/50], Iter [200/886] loss : 1.7200\n",
      "Epoch [20/50], Iter [300/886] loss : 1.7925\n",
      "Epoch [20/50], Iter [400/886] loss : 1.7261\n",
      "Epoch [20/50], Iter [500/886] loss : 2.1972\n",
      "Epoch [20/50], Iter [600/886] loss : 2.1577\n",
      "Epoch [20/50], Iter [700/886] loss : 2.0946\n",
      "Epoch [20/50], Iter [800/886] loss : 1.9209\n",
      "Average loss: 2.0132 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [21/50], Iter [100/886] loss : 2.4040\n",
      "Epoch [21/50], Iter [200/886] loss : 2.5373\n",
      "Epoch [21/50], Iter [300/886] loss : 2.2442\n",
      "Epoch [21/50], Iter [400/886] loss : 2.1111\n",
      "Epoch [21/50], Iter [500/886] loss : 1.8759\n",
      "Epoch [21/50], Iter [600/886] loss : 2.4651\n",
      "Epoch [21/50], Iter [700/886] loss : 2.0043\n",
      "Epoch [21/50], Iter [800/886] loss : 3.2627\n",
      "Average loss: 2.0436 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [22/50], Iter [100/886] loss : 1.7981\n",
      "Epoch [22/50], Iter [200/886] loss : 2.2476\n",
      "Epoch [22/50], Iter [300/886] loss : 1.9405\n",
      "Epoch [22/50], Iter [400/886] loss : 1.7165\n",
      "Epoch [22/50], Iter [500/886] loss : 2.2593\n",
      "Epoch [22/50], Iter [600/886] loss : 2.1115\n",
      "Epoch [22/50], Iter [700/886] loss : 1.7971\n",
      "Epoch [22/50], Iter [800/886] loss : 1.9428\n",
      "Average loss: 2.0349 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [23/50], Iter [100/886] loss : 2.1556\n",
      "Epoch [23/50], Iter [200/886] loss : 1.9867\n",
      "Epoch [23/50], Iter [300/886] loss : 2.2136\n",
      "Epoch [23/50], Iter [400/886] loss : 1.9406\n",
      "Epoch [23/50], Iter [500/886] loss : 2.1599\n",
      "Epoch [23/50], Iter [600/886] loss : 2.2722\n",
      "Epoch [23/50], Iter [700/886] loss : 2.2006\n",
      "Epoch [23/50], Iter [800/886] loss : 2.2099\n",
      "Average loss: 2.0307 \n",
      "\n",
      "Learning rate : 0.004\n",
      "Epoch [24/50], Iter [100/886] loss : 1.7240\n",
      "Epoch [24/50], Iter [200/886] loss : 1.7810\n",
      "Epoch [24/50], Iter [300/886] loss : 1.9678\n",
      "Epoch [24/50], Iter [400/886] loss : 1.6558\n",
      "Epoch [24/50], Iter [500/886] loss : 2.5460\n",
      "Epoch [24/50], Iter [600/886] loss : 2.4972\n",
      "Epoch [24/50], Iter [700/886] loss : 1.4251\n",
      "Epoch [24/50], Iter [800/886] loss : 1.6836\n",
      "Average loss: 2.0532 \n",
      "\n",
      "Epoch    23: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Learning rate : 0.0008\n",
      "Epoch [25/50], Iter [100/886] loss : 1.7824\n",
      "Epoch [25/50], Iter [200/886] loss : 1.5854\n",
      "Epoch [25/50], Iter [300/886] loss : 1.7949\n",
      "Epoch [25/50], Iter [400/886] loss : 1.6993\n",
      "Epoch [25/50], Iter [500/886] loss : 1.5720\n",
      "Epoch [25/50], Iter [600/886] loss : 1.8817\n",
      "Epoch [25/50], Iter [700/886] loss : 2.6148\n",
      "Epoch [25/50], Iter [800/886] loss : 1.8947\n",
      "Average loss: 2.0116 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [26/50], Iter [100/886] loss : 2.2331\n",
      "Epoch [26/50], Iter [200/886] loss : 1.8880\n",
      "Epoch [26/50], Iter [300/886] loss : 1.8693\n",
      "Epoch [26/50], Iter [400/886] loss : 1.4408\n",
      "Epoch [26/50], Iter [500/886] loss : 1.8744\n",
      "Epoch [26/50], Iter [600/886] loss : 2.0181\n",
      "Epoch [26/50], Iter [700/886] loss : 1.7295\n",
      "Epoch [26/50], Iter [800/886] loss : 1.6886\n",
      "Average loss: 2.0132 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [27/50], Iter [100/886] loss : 1.7464\n",
      "Epoch [27/50], Iter [200/886] loss : 1.9643\n",
      "Epoch [27/50], Iter [300/886] loss : 1.7717\n",
      "Epoch [27/50], Iter [400/886] loss : 2.4137\n",
      "Epoch [27/50], Iter [500/886] loss : 1.5355\n",
      "Epoch [27/50], Iter [600/886] loss : 1.8080\n",
      "Epoch [27/50], Iter [700/886] loss : 1.7587\n",
      "Epoch [27/50], Iter [800/886] loss : 2.0519\n",
      "Average loss: 2.0303 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [28/50], Iter [100/886] loss : 1.6431\n",
      "Epoch [28/50], Iter [200/886] loss : 1.9719\n",
      "Epoch [28/50], Iter [300/886] loss : 1.6250\n",
      "Epoch [28/50], Iter [400/886] loss : 2.6790\n",
      "Epoch [28/50], Iter [500/886] loss : 2.3726\n",
      "Epoch [28/50], Iter [600/886] loss : 1.6417\n",
      "Epoch [28/50], Iter [700/886] loss : 1.8219\n",
      "Epoch [28/50], Iter [800/886] loss : 2.4380\n",
      "Average loss: 2.0000 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [29/50], Iter [100/886] loss : 1.3753\n",
      "Epoch [29/50], Iter [200/886] loss : 1.6753\n",
      "Epoch [29/50], Iter [300/886] loss : 1.9498\n",
      "Epoch [29/50], Iter [400/886] loss : 1.5754\n",
      "Epoch [29/50], Iter [500/886] loss : 1.7759\n",
      "Epoch [29/50], Iter [600/886] loss : 1.7830\n",
      "Epoch [29/50], Iter [700/886] loss : 1.4113\n",
      "Epoch [29/50], Iter [800/886] loss : 1.6198\n",
      "Average loss: 2.0252 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [30/50], Iter [100/886] loss : 1.7189\n",
      "Epoch [30/50], Iter [200/886] loss : 1.8508\n",
      "Epoch [30/50], Iter [300/886] loss : 1.6910\n",
      "Epoch [30/50], Iter [400/886] loss : 1.6213\n",
      "Epoch [30/50], Iter [500/886] loss : 2.1877\n",
      "Epoch [30/50], Iter [600/886] loss : 2.1457\n",
      "Epoch [30/50], Iter [700/886] loss : 2.3757\n",
      "Epoch [30/50], Iter [800/886] loss : 2.3540\n",
      "Average loss: 2.0295 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [31/50], Iter [100/886] loss : 2.0952\n",
      "Epoch [31/50], Iter [200/886] loss : 1.5759\n",
      "Epoch [31/50], Iter [300/886] loss : 1.7678\n",
      "Epoch [31/50], Iter [400/886] loss : 1.8054\n",
      "Epoch [31/50], Iter [500/886] loss : 2.0952\n",
      "Epoch [31/50], Iter [600/886] loss : 1.9445\n",
      "Epoch [31/50], Iter [700/886] loss : 2.0794\n",
      "Epoch [31/50], Iter [800/886] loss : 1.6611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 2.0176 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [32/50], Iter [100/886] loss : 2.1167\n",
      "Epoch [32/50], Iter [200/886] loss : 1.8825\n",
      "Epoch [32/50], Iter [300/886] loss : 2.3353\n",
      "Epoch [32/50], Iter [400/886] loss : 2.3339\n",
      "Epoch [32/50], Iter [500/886] loss : 2.4257\n",
      "Epoch [32/50], Iter [600/886] loss : 1.9982\n",
      "Epoch [32/50], Iter [700/886] loss : 1.8017\n",
      "Epoch [32/50], Iter [800/886] loss : 4.0027\n",
      "Average loss: 1.9990 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [33/50], Iter [100/886] loss : 4.2492\n",
      "Epoch [33/50], Iter [200/886] loss : 2.1113\n",
      "Epoch [33/50], Iter [300/886] loss : 1.6903\n",
      "Epoch [33/50], Iter [400/886] loss : 2.0941\n",
      "Epoch [33/50], Iter [500/886] loss : 1.3820\n",
      "Epoch [33/50], Iter [600/886] loss : 2.0730\n",
      "Epoch [33/50], Iter [700/886] loss : 1.7378\n",
      "Epoch [33/50], Iter [800/886] loss : 1.6242\n",
      "Average loss: 1.9231 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [34/50], Iter [100/886] loss : 1.9383\n",
      "Epoch [34/50], Iter [200/886] loss : 1.7597\n",
      "Epoch [34/50], Iter [300/886] loss : 1.7340\n",
      "Epoch [34/50], Iter [400/886] loss : 2.1491\n",
      "Epoch [34/50], Iter [500/886] loss : 1.9407\n",
      "Epoch [34/50], Iter [600/886] loss : 2.6157\n",
      "Epoch [34/50], Iter [700/886] loss : 1.8016\n",
      "Epoch [34/50], Iter [800/886] loss : 1.3801\n",
      "Average loss: 1.8923 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [35/50], Iter [100/886] loss : 1.3561\n",
      "Epoch [35/50], Iter [200/886] loss : 2.1134\n",
      "Epoch [35/50], Iter [300/886] loss : 1.3009\n",
      "Epoch [35/50], Iter [400/886] loss : 1.8465\n",
      "Epoch [35/50], Iter [500/886] loss : 1.5659\n",
      "Epoch [35/50], Iter [600/886] loss : 2.7272\n",
      "Epoch [35/50], Iter [700/886] loss : 1.3156\n",
      "Epoch [35/50], Iter [800/886] loss : 1.7164\n",
      "Average loss: 1.8618 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [36/50], Iter [100/886] loss : 1.6699\n",
      "Epoch [36/50], Iter [200/886] loss : 1.3536\n",
      "Epoch [36/50], Iter [300/886] loss : 1.8507\n",
      "Epoch [36/50], Iter [400/886] loss : 1.4929\n",
      "Epoch [36/50], Iter [500/886] loss : 1.3147\n",
      "Epoch [36/50], Iter [600/886] loss : 2.1630\n",
      "Epoch [36/50], Iter [700/886] loss : 0.9710\n",
      "Epoch [36/50], Iter [800/886] loss : 1.6177\n",
      "Average loss: 1.8064 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [37/50], Iter [100/886] loss : 1.2688\n",
      "Epoch [37/50], Iter [200/886] loss : 1.6225\n",
      "Epoch [37/50], Iter [300/886] loss : 1.2855\n",
      "Epoch [37/50], Iter [400/886] loss : 1.2667\n",
      "Epoch [37/50], Iter [500/886] loss : 1.7803\n",
      "Epoch [37/50], Iter [600/886] loss : 1.6188\n",
      "Epoch [37/50], Iter [700/886] loss : 1.4356\n",
      "Epoch [37/50], Iter [800/886] loss : 1.7686\n",
      "Average loss: 1.8178 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [38/50], Iter [100/886] loss : 1.9097\n",
      "Epoch [38/50], Iter [200/886] loss : 1.1470\n",
      "Epoch [38/50], Iter [300/886] loss : 1.8520\n",
      "Epoch [38/50], Iter [400/886] loss : 1.3238\n",
      "Epoch [38/50], Iter [500/886] loss : 1.9424\n",
      "Epoch [38/50], Iter [600/886] loss : 1.8248\n",
      "Epoch [38/50], Iter [700/886] loss : 1.4356\n",
      "Epoch [38/50], Iter [800/886] loss : 1.5344\n",
      "Average loss: 1.8383 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [39/50], Iter [100/886] loss : 1.4099\n",
      "Epoch [39/50], Iter [200/886] loss : 1.2179\n",
      "Epoch [39/50], Iter [300/886] loss : 1.2824\n",
      "Epoch [39/50], Iter [400/886] loss : 1.1444\n",
      "Epoch [39/50], Iter [500/886] loss : 1.5257\n",
      "Epoch [39/50], Iter [600/886] loss : 1.6600\n",
      "Epoch [39/50], Iter [700/886] loss : 1.1674\n",
      "Epoch [39/50], Iter [800/886] loss : 0.9396\n",
      "Average loss: 1.7565 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [40/50], Iter [100/886] loss : 1.1136\n",
      "Epoch [40/50], Iter [200/886] loss : 1.7621\n",
      "Epoch [40/50], Iter [300/886] loss : 1.4314\n",
      "Epoch [40/50], Iter [400/886] loss : 1.7475\n",
      "Epoch [40/50], Iter [500/886] loss : 0.7170\n",
      "Epoch [40/50], Iter [600/886] loss : 1.2466\n",
      "Epoch [40/50], Iter [700/886] loss : 1.0909\n",
      "Epoch [40/50], Iter [800/886] loss : 1.6138\n",
      "Average loss: 1.7424 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [41/50], Iter [100/886] loss : 1.3227\n",
      "Epoch [41/50], Iter [200/886] loss : 1.5524\n",
      "Epoch [41/50], Iter [300/886] loss : 1.4800\n",
      "Epoch [41/50], Iter [400/886] loss : 0.8421\n",
      "Epoch [41/50], Iter [500/886] loss : 1.1054\n",
      "Epoch [41/50], Iter [600/886] loss : 1.5419\n",
      "Epoch [41/50], Iter [700/886] loss : 2.6391\n",
      "Epoch [41/50], Iter [800/886] loss : 1.5241\n",
      "Average loss: 1.8203 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [42/50], Iter [100/886] loss : 0.9230\n",
      "Epoch [42/50], Iter [200/886] loss : 1.1338\n",
      "Epoch [42/50], Iter [300/886] loss : 1.3315\n",
      "Epoch [42/50], Iter [400/886] loss : 0.8691\n",
      "Epoch [42/50], Iter [500/886] loss : 1.3407\n",
      "Epoch [42/50], Iter [600/886] loss : 1.5554\n",
      "Epoch [42/50], Iter [700/886] loss : 1.0575\n",
      "Epoch [42/50], Iter [800/886] loss : 2.2731\n",
      "Average loss: 1.7768 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [43/50], Iter [100/886] loss : 1.7652\n",
      "Epoch [43/50], Iter [200/886] loss : 1.6379\n",
      "Epoch [43/50], Iter [300/886] loss : 1.0820\n",
      "Epoch [43/50], Iter [400/886] loss : 1.4901\n",
      "Epoch [43/50], Iter [500/886] loss : 1.1660\n",
      "Epoch [43/50], Iter [600/886] loss : 1.0098\n",
      "Epoch [43/50], Iter [700/886] loss : 0.9602\n",
      "Epoch [43/50], Iter [800/886] loss : 1.1408\n",
      "Average loss: 1.7294 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [44/50], Iter [100/886] loss : 1.0982\n",
      "Epoch [44/50], Iter [200/886] loss : 1.2248\n",
      "Epoch [44/50], Iter [300/886] loss : 2.0139\n",
      "Epoch [44/50], Iter [400/886] loss : 1.4105\n",
      "Epoch [44/50], Iter [500/886] loss : 1.2697\n",
      "Epoch [44/50], Iter [600/886] loss : 1.0292\n",
      "Epoch [44/50], Iter [700/886] loss : 1.0424\n",
      "Epoch [44/50], Iter [800/886] loss : 1.7263\n",
      "Average loss: 1.7415 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [45/50], Iter [100/886] loss : 0.9351\n",
      "Epoch [45/50], Iter [200/886] loss : 1.3733\n",
      "Epoch [45/50], Iter [300/886] loss : 2.7376\n",
      "Epoch [45/50], Iter [400/886] loss : 1.4761\n",
      "Epoch [45/50], Iter [500/886] loss : 1.8229\n",
      "Epoch [45/50], Iter [600/886] loss : 1.1385\n",
      "Epoch [45/50], Iter [700/886] loss : 1.0032\n",
      "Epoch [45/50], Iter [800/886] loss : 1.1575\n",
      "Average loss: 1.7776 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [46/50], Iter [100/886] loss : 1.1385\n",
      "Epoch [46/50], Iter [200/886] loss : 0.9009\n",
      "Epoch [46/50], Iter [300/886] loss : 1.6151\n",
      "Epoch [46/50], Iter [400/886] loss : 1.6779\n",
      "Epoch [46/50], Iter [500/886] loss : 1.2570\n",
      "Epoch [46/50], Iter [600/886] loss : 1.4826\n",
      "Epoch [46/50], Iter [700/886] loss : 2.1360\n",
      "Epoch [46/50], Iter [800/886] loss : 1.3837\n",
      "Average loss: 1.8398 \n",
      "\n",
      "Learning rate : 0.0008\n",
      "Epoch [47/50], Iter [100/886] loss : 2.0695\n",
      "Epoch [47/50], Iter [200/886] loss : 1.6303\n",
      "Epoch [47/50], Iter [300/886] loss : 1.7619\n",
      "Epoch [47/50], Iter [400/886] loss : 1.2561\n",
      "Epoch [47/50], Iter [500/886] loss : 1.4096\n",
      "Epoch [47/50], Iter [600/886] loss : 1.9512\n",
      "Epoch [47/50], Iter [700/886] loss : 1.1565\n",
      "Epoch [47/50], Iter [800/886] loss : 1.1823\n",
      "Average loss: 1.7954 \n",
      "\n",
      "Epoch    46: reducing learning rate of group 0 to 1.6000e-04.\n",
      "Learning rate : 0.00016\n",
      "Epoch [48/50], Iter [100/886] loss : 0.8692\n",
      "Epoch [48/50], Iter [200/886] loss : 1.4379\n",
      "Epoch [48/50], Iter [300/886] loss : 1.3460\n",
      "Epoch [48/50], Iter [400/886] loss : 1.7782\n",
      "Epoch [48/50], Iter [500/886] loss : 1.5212\n",
      "Epoch [48/50], Iter [600/886] loss : 1.1406\n",
      "Epoch [48/50], Iter [700/886] loss : 1.2282\n",
      "Epoch [48/50], Iter [800/886] loss : 1.0791\n",
      "Average loss: 1.7198 \n",
      "\n",
      "Learning rate : 0.00016\n",
      "Epoch [49/50], Iter [100/886] loss : 1.5139\n",
      "Epoch [49/50], Iter [200/886] loss : 1.2501\n",
      "Epoch [49/50], Iter [300/886] loss : 0.7337\n",
      "Epoch [49/50], Iter [400/886] loss : 1.1480\n",
      "Epoch [49/50], Iter [500/886] loss : 1.6587\n",
      "Epoch [49/50], Iter [600/886] loss : 2.6890\n",
      "Epoch [49/50], Iter [700/886] loss : 1.1011\n",
      "Epoch [49/50], Iter [800/886] loss : 1.4163\n",
      "Average loss: 1.7619 \n",
      "\n",
      "Learning rate : 0.00016\n",
      "Epoch [50/50], Iter [100/886] loss : 1.8267\n",
      "Epoch [50/50], Iter [200/886] loss : 3.6665\n",
      "Epoch [50/50], Iter [300/886] loss : 1.4493\n",
      "Epoch [50/50], Iter [400/886] loss : 1.9377\n",
      "Epoch [50/50], Iter [500/886] loss : 1.1559\n",
      "Epoch [50/50], Iter [600/886] loss : 0.8002\n",
      "Epoch [50/50], Iter [700/886] loss : 0.6285\n",
      "Epoch [50/50], Iter [800/886] loss : 1.0113\n",
      "Average loss: 1.7749 \n",
      "\n",
      "Learning rate : 0.00016\n",
      "WD:   1e-06  DO:   0.6  LR:   0.1  -> VAL_loss: tensor(1.7749, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = np.inf\n",
    "best_model = None\n",
    "last_model = None\n",
    "\n",
    "for wd in [0.0001, 0.00001, 0.000001]:\n",
    "    for do in [0.5, 0.6]:\n",
    "        for lr in [0.1]:#, 0.01, 0.001]:\n",
    "            \n",
    "            tmp_model = model_1DCNN_short(dropout=do).cuda(args.which_gpu)\n",
    "            criterion = nn.NLLLoss()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            tmp_avg_loss = fit(tmp_model,train_loader,valid_loader,criterion, num_epochs, args, learning_rate=lr, output=False, wd=wd)\n",
    "            #print(\"--- %s seconds spent ---\" % (time.time() - start_time))\n",
    "                                           \n",
    "            print(\"WD:  \", wd, \" DO:  \", do, \" LR:  \", lr, \" -> VAL_loss:\", tmp_avg_loss)\n",
    "            \n",
    "            if tmp_avg_loss < best_val_loss:\n",
    "                best_val_loss = tmp_avg_loss\n",
    "                best_model = tmp_model                \n",
    "                \n",
    "model = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 1.5621 \n",
      "\n",
      "Test Accuracy: 0.5931 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_loss, output_all, label_all = eval(model,test_loader,criterion,args)\n",
    "prediction = np.concatenate(output_all)\n",
    "prediction = prediction.reshape(len(y_test),10)\n",
    "y_label = np.concatenate(label_all)\n",
    "\n",
    "if frames == 128:\n",
    "    prediction = average_segments(prediction, 10) \n",
    "    y_label = y_label[0:y_label.size:10]\n",
    "        \n",
    "prediction = prediction.argmax(axis=1)\n",
    "\n",
    "comparison = prediction - y_label\n",
    "acc = float(len(y_label) - np.count_nonzero(comparison)) / len(y_label)\n",
    "print('Test Accuracy: {:.4f} \\n'. format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_label, prediction)\n",
    "genre_list = ['reggae', 'classical', 'country', 'jazz', 'metal', 'pop', 'disco', 'hiphop', 'rock', 'blues']\n",
    "plot_confusion_matrix(cm, genre_list, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime.fromtimestamp(start_time).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "save_name = save_path + 'model-' + dt + '.pt'\n",
    "\n",
    "if not os.path.exists(os.path.dirname(save_name)):\n",
    "        os.makedirs(os.path.dirname(save_name))\n",
    "torch.save(model, save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Test Accuracy: 0.6000 (averaging results, frame size 2.99, model model-2018-04-27-07-34-35.pt  \n",
    "Test Accuracy: 0.6552 (switched to proposed method (classum) for loss, same model, not repeatable unfortunately)  \n",
    "Test Accuracy: 0.6138 (used Adam as optimizer (orch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-6, amsgrad=False))  \n",
    "Test Accuracy: 0.6138  (batch_size=10 with SGD)  \n",
    "Test Accuracy: 0.6379 (dropout=0.5, SGD)  \n",
    "Test Accuracy: 0.5862 (dropout=0.5, Adam, lr=0.001)  \n",
    "Test Accuracy: 0.6034 (new model, no dropout)  \n",
    "Test Accuracy: 0.6138 (new model, dropout=0.5)\n",
    "Test Accuracy: 0.6379 (short, dropout=0.6, SGD)\n",
    "Test Accuracy: 0.6034 (WD:   1e-06  DO:   0.6  LR:   0.001  -> VAL_loss: tensor(1.5854, device='cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test Accuracy: 0.6034 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
